{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/fayssalelansari/arabic-tweets-sentiment-analysis?scriptVersionId=116163513\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","id":"689ff192","metadata":{"papermill":{"duration":0.013128,"end_time":"2023-01-12T09:52:33.738555","exception":false,"start_time":"2023-01-12T09:52:33.725427","status":"completed"},"tags":[]},"source":["# TODO LIST\n","* [X] PREPROCESS DATA\n","* [X] STEM ARABIC WORDS\n","* [x] ADD DEFINITIONS FOR COMMON KEYWORDS\n","* [x] DEFINE THE EXACT STEPS USED IN THE PIPELINE (MAKE A DIAGRAM)\n","* [ ] DEFINE FOR EACH STEP THE LIST OF POSSIBLE METHODS TO USE\n","* [ ] DEFINE FOR EACH METHOD A SMALL LIST OF POSSIBLE CONFIGURATION FOR THE PARAMETERS TO USE\n","* [ ] MAKE PIPELINE\n","* [ ] EMBEDDINGS\n","* [ ] CONFUSION MATRIX"]},{"cell_type":"markdown","id":"2a5ef760","metadata":{"papermill":{"duration":0.015951,"end_time":"2023-01-12T09:52:33.767018","exception":false,"start_time":"2023-01-12T09:52:33.751067","status":"completed"},"tags":[]},"source":["1. How to split data into `test` and `train` data-sets from one initial data-set?\n","2. Emojis convey emotions so instead of deleting them we should substitute each emoji with the emotion it represents\n","3. when applying different steps of the Pipeline, instead of putting new data in a new variable or data frame just add a `new column` with a representive tag."]},{"cell_type":"markdown","id":"599ab7fc","metadata":{"papermill":{"duration":0.013555,"end_time":"2023-01-12T09:52:33.793356","exception":false,"start_time":"2023-01-12T09:52:33.779801","status":"completed"},"tags":[]},"source":["# Ideas to implement\n","1. webapp:\n","    * make a front end connected to a trained model or give the user the ability to train his own model.\n","    * in the webpage there will be a way to fetch a predetermined number of tweets using a keyword or a hashtag (should be in arabic). \n","2. Since for the same step there could be different methods and for each method different methods add an easier way to change the method and the parameters, could be modified in the web page also.\n","3. show graphs for different stats of the chosen models and methods\n","4. a way to run all possible (or a lot) methods and parameters and order them by decreasing order of validity"]},{"cell_type":"markdown","id":"1050afc6","metadata":{"papermill":{"duration":0.012069,"end_time":"2023-01-12T09:52:33.818579","exception":false,"start_time":"2023-01-12T09:52:33.80651","status":"completed"},"tags":[]},"source":["# Definitions of Common Words\n","**Normalizing**: Also called text cleansing, is the process of preprocessing text data to be understood and used by Natural Language Processors and other text analytics software. It includes many other substeps such as `tokenizing`, `case conversion`, `correcting spelling`, `removing stop words`, `stemming` and `lemmatization`.\n","\n","**Tokenization**: It is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens. Usually we split a text document by word to obtain a token for each individual word in a list of tokens.\n","\n","**Stop words**: A list of the most common words in a language\n","\n","**Vectorization**: The process of writing tokens of text data in the form of vectors, since computer programs are only able to understand numbers it is important to vectorize text data in order to process it. Also called `Feature extraction`; There are many feature-extraction methods the most famous being `bag of words based frequency features`.\n","\n","**Feature Matrix**: In this matrix each row represents a document (here tweet) and each column represents a feature (processed token). It is used by ML and statistical models to provide predictions.\n","\n","**Stemming**: Consists of extracting only the smallest uncompressable part of a word not containing any suffixes or prefixes or any other inflections. Then converting it to the root or the source of the meaning of that word.\n","\n","**lemmatization**: Similar to `Stemming` is the same process of removing inflections from a feature (token) but without converting it to its origin of meaning. It is less representative for making prediction since more words can have the same stem than the the words having the same lemma.\n","\n","**Bag of Words Model**: One of the most popular yet simple Feature Extraction methods, for each document we build a set of words without any regard of the order of presence (hence the name bag of words). But keeping count for each word.\n"]},{"cell_type":"markdown","id":"501bea5b","metadata":{"papermill":{"duration":0.013032,"end_time":"2023-01-12T09:52:33.843673","exception":false,"start_time":"2023-01-12T09:52:33.830641","status":"completed"},"tags":[]},"source":["# References\n","...\n"]},{"cell_type":"markdown","id":"4fd684e2","metadata":{"papermill":{"duration":0.011085,"end_time":"2023-01-12T09:52:33.867098","exception":false,"start_time":"2023-01-12T09:52:33.856013","status":"completed"},"tags":[]},"source":["## Diagram of Steps \n","In this section we will build a diagram showcasing the different steps used to analyse a set of tweets. \n","* For each step of the process we will put all the possible methods in a cluster.\n","* Between each step there will be an arrow with a label describing how to reach the next step."]},{"cell_type":"code","execution_count":1,"id":"6d7558ff","metadata":{"execution":{"iopub.execute_input":"2023-01-12T09:52:33.892258Z","iopub.status.busy":"2023-01-12T09:52:33.891746Z","iopub.status.idle":"2023-01-12T09:52:47.874009Z","shell.execute_reply":"2023-01-12T09:52:47.871927Z"},"papermill":{"duration":13.998466,"end_time":"2023-01-12T09:52:47.877095","exception":false,"start_time":"2023-01-12T09:52:33.878629","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting nb-js-diagrammers\r\n","  Downloading nb_js_diagrammers-0.0.7-py3-none-any.whl (8.8 kB)\r\n","Collecting pyflowchart\r\n","  Downloading pyflowchart-0.2.3-py3-none-any.whl (18 kB)\r\n","Requirement already satisfied: chardet in /opt/conda/lib/python3.7/site-packages (from pyflowchart->nb-js-diagrammers) (5.0.0)\r\n","Requirement already satisfied: astunparse in /opt/conda/lib/python3.7/site-packages (from pyflowchart->nb-js-diagrammers) (1.6.3)\r\n","Requirement already satisfied: six<2.0,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from astunparse->pyflowchart->nb-js-diagrammers) (1.15.0)\r\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse->pyflowchart->nb-js-diagrammers) (0.37.1)\r\n","Installing collected packages: pyflowchart, nb-js-diagrammers\r\n","Successfully installed nb-js-diagrammers-0.0.7 pyflowchart-0.2.3\r\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install --upgrade nb-js-diagrammers\n","%load_ext nb_js_diagrammers"]},{"cell_type":"code","execution_count":2,"id":"a662fea9","metadata":{"execution":{"iopub.execute_input":"2023-01-12T09:52:47.904201Z","iopub.status.busy":"2023-01-12T09:52:47.903749Z","iopub.status.idle":"2023-01-12T09:52:47.916355Z","shell.execute_reply":"2023-01-12T09:52:47.91517Z"},"papermill":{"duration":0.028775,"end_time":"2023-01-12T09:52:47.919083","exception":false,"start_time":"2023-01-12T09:52:47.890308","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<iframe srcdoc=\"&lt;html&gt;\n","    &lt;body&gt;\n","        &lt;script src=&quot;https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js&quot;&gt;&lt;/script&gt;\n","        &lt;script&gt;\n","            mermaid.initialize({ startOnLoad: true });\n","        &lt;/script&gt;\n"," \n","        &lt;div class=&quot;mermaid&quot;&gt;\n","            \n","flowchart TB\n","    subgraph DATASET\n","        tweets[Load Tweets.tsv]\n","        fetch[fetch Tweets using API]\n","        tweets --- |or| fetch\n","    end\n","    split_decision{Data Already Split \\nInto Test And Train?}\n","    subgraph SPLIT_DATA[Split Data]\n","        train_test_split[sklearn&#x27;s train_test_split]\n","    end\n","    subgraph NORMALIZE\n","        tokenize[Tokenize]\n","        subgraph EXTRACT_ROOT[EXTRACT ROOT]\n","            lemmetize[Lemmetize]\n","            stem[Stem]\n","            lemmetize --- |or| stem\n","        end\n","        subgraph PRE_PROCESS[PRE-PROCESS]\n","            remove_stop_words[Remove Stop Words]\n","            remove_emojis[Remove Emojis]\n","            remove_ponctuation[Remove Ponctuation]\n","            remove_special_characters[Remove Special Characters]   \n","        end\n","        remove_stop_words --- |and/or| remove_emojis\n","        remove_emojis --- |and/or| remove_ponctuation\n","        remove_ponctuation --- |and/or| remove_special_characters\n","        \n","        tokenize --&gt; PRE_PROCESS\n","        PRE_PROCESS --&gt; EXTRACT_ROOT\n","    end\n","        \n","    subgraph VECTORIZE\n","        CountVectorizer --- |or| TfidfTransformer\n","    end\n","    \n","    subgraph TRAIN_CLASSIFIER[TRAIN A CLASSIFIER]\n","        MultinomialNB[Sklearn&#x27;s MultinomialNB naive bayes] \n","        SGDClassifier[Sklearn&#x27;s SGDClassifier]\n","        MultinomialNB --- |or| SGDClassifier\n","    end\n","                      \n","    subgraph PREDICT\n","        \n","    end\n","    \n","    subgraph ANALYSE\n","                      \n","    end\n","    \n","    DATASET --&gt; split_decision\n","    split_decision --&gt; |yes| NORMALIZE\n","    split_decision --&gt; |no| SPLIT_DATA\n","    SPLIT_DATA --&gt; NORMALIZE\n","    NORMALIZE --&gt; VECTORIZE\n","    VECTORIZE --&gt; TRAIN_CLASSIFIER;\n","    TRAIN_CLASSIFIER --&gt; PREDICT;\n","    PREDICT --&gt; ANALYSE\n","\n","        &lt;/div&gt;\n"," \n","    &lt;/body&gt;\n","&lt;/html&gt;\n","\" width=\"100%\" height=\"1750\"style=\"border:none !important;\" \"allowfullscreen\" \"webkitallowfullscreen\" \"mozallowfullscreen\"></iframe>"],"text/plain":["<nb_js_diagrammers.magics.JSDiagram at 0x7fc2d0fcf790>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["%%mermaid_magic -h 1750\n","\n","flowchart TB\n","    subgraph DATASET\n","        tweets[Load Tweets.tsv]\n","        fetch[fetch Tweets using API]\n","        tweets --- |or| fetch\n","    end\n","    split_decision{Data Already Split \\nInto Test And Train?}\n","    subgraph SPLIT_DATA[Split Data]\n","        train_test_split[sklearn's train_test_split]\n","    end\n","    subgraph NORMALIZE\n","        tokenize[Tokenize]\n","        subgraph EXTRACT_ROOT[EXTRACT ROOT]\n","            lemmetize[Lemmetize]\n","            stem[Stem]\n","            lemmetize --- |or| stem\n","        end\n","        subgraph PRE_PROCESS[PRE-PROCESS]\n","            remove_stop_words[Remove Stop Words]\n","            remove_emojis[Remove Emojis]\n","            remove_ponctuation[Remove Ponctuation]\n","            remove_special_characters[Remove Special Characters]   \n","        end\n","        remove_stop_words --- |and/or| remove_emojis\n","        remove_emojis --- |and/or| remove_ponctuation\n","        remove_ponctuation --- |and/or| remove_special_characters\n","        \n","        tokenize --> PRE_PROCESS\n","        PRE_PROCESS --> EXTRACT_ROOT\n","    end\n","        \n","    subgraph VECTORIZE\n","        CountVectorizer --- |or| TfidfTransformer\n","    end\n","    \n","    subgraph TRAIN_CLASSIFIER[TRAIN A CLASSIFIER]\n","        MultinomialNB[Sklearn's MultinomialNB naive bayes] \n","        SGDClassifier[Sklearn's SGDClassifier]\n","        MultinomialNB --- |or| SGDClassifier\n","    end\n","                      \n","    subgraph PREDICT\n","        \n","    end\n","    \n","    subgraph ANALYSE\n","                      \n","    end\n","    \n","    DATASET --> split_decision\n","    split_decision --> |yes| NORMALIZE\n","    split_decision --> |no| SPLIT_DATA\n","    SPLIT_DATA --> NORMALIZE\n","    NORMALIZE --> VECTORIZE\n","    VECTORIZE --> TRAIN_CLASSIFIER;\n","    TRAIN_CLASSIFIER --> PREDICT;\n","    PREDICT --> ANALYSE\n"]},{"cell_type":"code","execution_count":3,"id":"61ba1792","metadata":{"_cell_guid":"4c1ef890-63d4-444a-b58a-f41939d9e62f","_uuid":"87d30fb1-988f-4483-b632-02009c36d444","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T09:52:47.945018Z","iopub.status.busy":"2023-01-12T09:52:47.94455Z","iopub.status.idle":"2023-01-12T09:52:47.950908Z","shell.execute_reply":"2023-01-12T09:52:47.949551Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.022241,"end_time":"2023-01-12T09:52:47.953133","exception":false,"start_time":"2023-01-12T09:52:47.930892","status":"completed"},"tags":[]},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","id":"0ab2ae0a","metadata":{"_cell_guid":"0380ce88-a279-42cb-b40b-c7476b152bd0","_uuid":"ec3e36fc-4064-4b35-951d-0b595aa23b14","papermill":{"duration":0.011177,"end_time":"2023-01-12T09:52:47.976533","exception":false,"start_time":"2023-01-12T09:52:47.965356","status":"completed"},"tags":[]},"source":["### constants"]},{"cell_type":"code","execution_count":4,"id":"e0869889","metadata":{"_cell_guid":"2bd62210-dad9-4e66-b738-4660c96b0fc9","_uuid":"7bbff164-5a1e-4f63-bd19-aa61814a168e","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T09:52:48.002267Z","iopub.status.busy":"2023-01-12T09:52:48.001879Z","iopub.status.idle":"2023-01-12T09:52:48.00665Z","shell.execute_reply":"2023-01-12T09:52:48.005406Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.020641,"end_time":"2023-01-12T09:52:48.009246","exception":false,"start_time":"2023-01-12T09:52:47.988605","status":"completed"},"tags":[]},"outputs":[],"source":["enc = \"UTF-8\""]},{"cell_type":"markdown","id":"1a97aa2c","metadata":{"_cell_guid":"6ec183c9-8c80-45e5-9e8f-9c9681f60b13","_uuid":"d9cc1119-139a-4382-8c4e-8ac304c69b41","papermill":{"duration":0.0111,"end_time":"2023-01-12T09:52:48.03199","exception":false,"start_time":"2023-01-12T09:52:48.02089","status":"completed"},"tags":[]},"source":["### model files"]},{"cell_type":"code","execution_count":5,"id":"6ff8a579","metadata":{"_cell_guid":"9e978d47-5887-4d41-91fb-63ef6ffd63b0","_uuid":"45b66073-ec3a-4a74-9cf1-441edc87b64c","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T09:52:48.059085Z","iopub.status.busy":"2023-01-12T09:52:48.058268Z","iopub.status.idle":"2023-01-12T09:52:48.066248Z","shell.execute_reply":"2023-01-12T09:52:48.065162Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.024901,"end_time":"2023-01-12T09:52:48.068905","exception":false,"start_time":"2023-01-12T09:52:48.044004","status":"completed"},"tags":[]},"outputs":[],"source":["import enum\n","\n","class Tag(enum.Enum):\n","    Neutral = \"NEUTRAL\"\n","    Positive = \"POSITIVE\"\n","    Negative = \"NEGATIVE\"\n","    Not_defined = \"NOT_DEFINED\"\n","\n","class Tweet:\n","    def __init__(self, text, real_tag=Tag.Not_defined, given_tag=Tag.Not_defined):\n","        self.text = text\n","        self.real_tag = real_tag\n","        self.given_tag = given_tag\n","\n","    def __str__(self):\n","        txt = self.text\n","        return txt\n","\n","    def __repr__(self):\n","        txt = self.text \n","        return txt"]},{"cell_type":"markdown","id":"ad684c64","metadata":{"_cell_guid":"ca0e9aec-e672-4faa-9b5f-8d43ee952691","_uuid":"c2c6de7b-7687-4684-8b7e-a8083b91d10b","papermill":{"duration":0.011758,"end_time":"2023-01-12T09:52:48.092359","exception":false,"start_time":"2023-01-12T09:52:48.080601","status":"completed"},"tags":[]},"source":["## Loading data\n","### Using our own method and model files\n","    In this section we will use our own definition of a function that will load the files by traversing the folders containing the positive and the negative tweets\n","    We should compare this method to simply reading the `.tsv` files (which should be faster)."]},{"cell_type":"code","execution_count":6,"id":"d5bc2de2","metadata":{"_cell_guid":"ca64d88f-05f5-4080-a33f-5fbf92fc36e5","_uuid":"9b2d8e93-cc11-4c21-864b-f0f2a05339fa","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T09:52:48.119474Z","iopub.status.busy":"2023-01-12T09:52:48.118687Z","iopub.status.idle":"2023-01-12T09:52:48.124333Z","shell.execute_reply":"2023-01-12T09:52:48.123513Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.021805,"end_time":"2023-01-12T09:52:48.126755","exception":false,"start_time":"2023-01-12T09:52:48.10495","status":"completed"},"tags":[]},"outputs":[],"source":["import os\n","import pathlib\n","\n","# # current_path = pathlib.Path(__file__).parent.resolve()\n","# POS_COUNT = 29848\n","# NEG_COUNT = 28901\n","# tweets = []\n","\n","# def import_data():\n","#     for i in range(POS_COUNT):\n","#         text = \"\"\n","#         with open(\"/kaggle/input/arabic-sentiment-twitter-corpus/arabic_tweets/pos/\" + str(i) + '.txt', encoding=enc) as f:\n","#             for line in f:\n","#                 text += line\n","#         tweets.append(Tweet(text, Tag.Positive))\n","#     for i in range(NEG_COUNT):\n","#         text = \"\"\n","#         with open(\"/kaggle/input/arabic-sentiment-twitter-corpus/arabic_tweets/neg/\" + str(i) + '.txt', encoding=enc) as f:\n","#             for line in f:\n","#                 text += line\n","#         tweets.append(Tweet(text, Tag.Negative))\n","        \n","# import_data()"]},{"cell_type":"code","execution_count":7,"id":"4132ce2c","metadata":{"_cell_guid":"5a3260da-95d6-46c9-b0a8-228f471bca48","_uuid":"002d715e-41d8-4714-8737-a9909c57f4db","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T09:52:48.151889Z","iopub.status.busy":"2023-01-12T09:52:48.151226Z","iopub.status.idle":"2023-01-12T09:52:48.156119Z","shell.execute_reply":"2023-01-12T09:52:48.155051Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.020511,"end_time":"2023-01-12T09:52:48.15883","exception":false,"start_time":"2023-01-12T09:52:48.138319","status":"completed"},"tags":[]},"outputs":[],"source":["# print(tweets)"]},{"cell_type":"markdown","id":"c7c3b14c","metadata":{"_cell_guid":"5fab9c36-56cb-4c8f-8db0-e91c821be46d","_uuid":"b3302efc-f1e9-4003-b14b-7efe6dff8091","papermill":{"duration":0.011381,"end_time":"2023-01-12T09:52:48.181992","exception":false,"start_time":"2023-01-12T09:52:48.170611","status":"completed"},"tags":[]},"source":["### Using our own method and reading from `tsv` files directly\n","    same as the previous step we shall populate a list of tweets[Tweet] with our data.\n","    After deep thought it is better to use a matrix instead of classes. We shall use pandas to represent our dataset."]},{"cell_type":"code","execution_count":8,"id":"c05bfd2f","metadata":{"_cell_guid":"f4d75feb-2e8a-4357-b398-b76b762d188a","_uuid":"d1e8751f-ad65-4fd7-8025-9cc982ccd09b","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T09:52:48.207368Z","iopub.status.busy":"2023-01-12T09:52:48.206519Z","iopub.status.idle":"2023-01-12T09:52:48.59392Z","shell.execute_reply":"2023-01-12T09:52:48.59276Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.403103,"end_time":"2023-01-12T09:52:48.596653","exception":false,"start_time":"2023-01-12T09:52:48.19355","status":"completed"},"tags":[]},"outputs":[],"source":["column_names = [\"sentiment\", \"content\"]\n","train_tweets_positive = pd.read_table(\"/kaggle/input/arabic-sentiment-twitter-corpus/train_Arabic_tweets_positive_20190413.tsv\", names=column_names)\n","train_tweets_negative = pd.read_table(\"/kaggle/input/arabic-sentiment-twitter-corpus/train_Arabic_tweets_negative_20190413.tsv\", names=column_names)\n","test_tweets_positive = pd.read_table(\"/kaggle/input/arabic-sentiment-twitter-corpus/test_Arabic_tweets_positive_20190413.tsv\", names=column_names)\n","test_tweets_negative = pd.read_table(\"/kaggle/input/arabic-sentiment-twitter-corpus/test_Arabic_tweets_negative_20190413.tsv\", names=column_names)"]},{"cell_type":"code","execution_count":9,"id":"dedcbe25","metadata":{"_cell_guid":"4cb90bf1-f45d-4c2d-8e22-19dda25c3ef2","_uuid":"49a5e898-0cc0-40e7-aa9f-ca9624957948","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T09:52:48.622381Z","iopub.status.busy":"2023-01-12T09:52:48.621687Z","iopub.status.idle":"2023-01-12T09:52:48.648081Z","shell.execute_reply":"2023-01-12T09:52:48.647216Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.041909,"end_time":"2023-01-12T09:52:48.650396","exception":false,"start_time":"2023-01-12T09:52:48.608487","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentiment</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>pos</td>\n","      <td>Ù†Ø­Ù† Ø§Ù„Ø°ÙŠÙ† ÙŠØªØ­ÙˆÙ„ ÙƒÙ„ Ù…Ø§ Ù†ÙˆØ¯ Ø£Ù† Ù†Ù‚ÙˆÙ„Ù‡ Ø¥Ù„Ù‰ Ø¯Ø¹Ø§Ø¡ Ù„Ù„...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>pos</td>\n","      <td>ÙˆÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ© Ù„Ù† ÙŠØ¨Ù‚Ù‰Ù° Ù…Ø¹Ùƒ Ø¢Ø­Ø¯Ø¥Ù„Ø§ Ù…Ù† Ø±Ø£Ù‰Ù° Ø§Ù„Ø¬Ù…Ø§Ù„...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>pos</td>\n","      <td>Ù…Ù† Ø§Ù„Ø®ÙŠØ± Ù†ÙØ³Ù‡ ğŸ’›</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>pos</td>\n","      <td>#Ø²Ù„Ø²Ù„_Ø§Ù„Ù…Ù„Ø¹Ø¨_Ù†ØµØ±Ù†Ø§_Ø¨ÙŠÙ„Ø¹Ø¨ ÙƒÙ† Ø¹Ø§Ù„ÙŠ Ø§Ù„Ù‡Ù…Ù‡ ÙˆÙ„Ø§ ØªØ±Ø¶...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>pos</td>\n","      <td>Ø§Ù„Ø´ÙŠØ¡ Ø§Ù„ÙˆØ­ÙŠØ¯ Ø§Ù„Ø°ÙŠ ÙˆØµÙ„ÙˆØ§ ÙÙŠÙ‡ Ù„Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ù‡Ùˆ : Ø§Ù„Ù…Ø³...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>22509</th>\n","      <td>neg</td>\n","      <td>ÙƒÙŠÙ ØªØ±Ù‰ Ø£ÙˆØ±Ø§Ù†ÙˆØ³ Ù„Ùˆ ÙƒØ§Ù† ÙŠÙ‚Ø¹ Ù…ÙƒØ§Ù† Ø§Ù„Ù‚Ù…Ø± ØŸ ğŸ’™ğŸ’™ ÙƒÙˆÙƒ...</td>\n","    </tr>\n","    <tr>\n","      <th>22510</th>\n","      <td>neg</td>\n","      <td>Ø§Ø­Ø³Ø¯Ùƒ Ø¹Ù„Ù‰ Ø§Ù„Ø§ÙŠÙ… ğŸ’”</td>\n","    </tr>\n","    <tr>\n","      <th>22511</th>\n","      <td>neg</td>\n","      <td>Ù„Ø£ÙˆÙ„ Ù…Ø±Ø© Ù…Ø§ Ø¨Ù†ÙƒÙˆÙ† Ø³ÙˆØ§ ğŸ’”</td>\n","    </tr>\n","    <tr>\n","      <th>22512</th>\n","      <td>neg</td>\n","      <td>Ø¨Ù‚Ù„Ù‡ Ù„ÙŠØ´ ÙŠØ§ ÙˆØ§Ø·ÙŠ ğŸ¤”</td>\n","    </tr>\n","    <tr>\n","      <th>22513</th>\n","      <td>neg</td>\n","      <td>Ù‚Ø¯ Ø·Ø§Ù„ ØµØ¨Ø±ÙŠ ÙÙŠ Ø§Ù„Ù†ÙˆÙ‰ Ø¥Ø° ØªØ±ÙƒØªÙ†ÙŠ ÙƒØ¦ÙŠØ¨Ø§ Ø› ØºØ±ÙŠØ¨Ø§ Ø¨...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>45275 rows Ã— 2 columns</p>\n","</div>"],"text/plain":["      sentiment                                            content\n","0           pos  Ù†Ø­Ù† Ø§Ù„Ø°ÙŠÙ† ÙŠØªØ­ÙˆÙ„ ÙƒÙ„ Ù…Ø§ Ù†ÙˆØ¯ Ø£Ù† Ù†Ù‚ÙˆÙ„Ù‡ Ø¥Ù„Ù‰ Ø¯Ø¹Ø§Ø¡ Ù„Ù„...\n","1           pos  ÙˆÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ© Ù„Ù† ÙŠØ¨Ù‚Ù‰Ù° Ù…Ø¹Ùƒ Ø¢Ø­Ø¯Ø¥Ù„Ø§ Ù…Ù† Ø±Ø£Ù‰Ù° Ø§Ù„Ø¬Ù…Ø§Ù„...\n","2           pos                                    Ù…Ù† Ø§Ù„Ø®ÙŠØ± Ù†ÙØ³Ù‡ ğŸ’›\n","3           pos  #Ø²Ù„Ø²Ù„_Ø§Ù„Ù…Ù„Ø¹Ø¨_Ù†ØµØ±Ù†Ø§_Ø¨ÙŠÙ„Ø¹Ø¨ ÙƒÙ† Ø¹Ø§Ù„ÙŠ Ø§Ù„Ù‡Ù…Ù‡ ÙˆÙ„Ø§ ØªØ±Ø¶...\n","4           pos  Ø§Ù„Ø´ÙŠØ¡ Ø§Ù„ÙˆØ­ÙŠØ¯ Ø§Ù„Ø°ÙŠ ÙˆØµÙ„ÙˆØ§ ÙÙŠÙ‡ Ù„Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ù‡Ùˆ : Ø§Ù„Ù…Ø³...\n","...         ...                                                ...\n","22509       neg  ÙƒÙŠÙ ØªØ±Ù‰ Ø£ÙˆØ±Ø§Ù†ÙˆØ³ Ù„Ùˆ ÙƒØ§Ù† ÙŠÙ‚Ø¹ Ù…ÙƒØ§Ù† Ø§Ù„Ù‚Ù…Ø± ØŸ ğŸ’™ğŸ’™ ÙƒÙˆÙƒ...\n","22510       neg                                  Ø§Ø­Ø³Ø¯Ùƒ Ø¹Ù„Ù‰ Ø§Ù„Ø§ÙŠÙ… ğŸ’”\n","22511       neg                            Ù„Ø£ÙˆÙ„ Ù…Ø±Ø© Ù…Ø§ Ø¨Ù†ÙƒÙˆÙ† Ø³ÙˆØ§ ğŸ’”\n","22512       neg                                 Ø¨Ù‚Ù„Ù‡ Ù„ÙŠØ´ ÙŠØ§ ÙˆØ§Ø·ÙŠ ğŸ¤”\n","22513       neg  Ù‚Ø¯ Ø·Ø§Ù„ ØµØ¨Ø±ÙŠ ÙÙŠ Ø§Ù„Ù†ÙˆÙ‰ Ø¥Ø° ØªØ±ÙƒØªÙ†ÙŠ ÙƒØ¦ÙŠØ¨Ø§ Ø› ØºØ±ÙŠØ¨Ø§ Ø¨...\n","\n","[45275 rows x 2 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["X_train = pd.concat([train_tweets_positive,train_tweets_negative])\n","\n","from IPython.display import display, HTML\n","display(X_train)"]},{"cell_type":"markdown","id":"5e08b815","metadata":{"papermill":{"duration":0.011715,"end_time":"2023-01-12T09:52:48.674646","exception":false,"start_time":"2023-01-12T09:52:48.662931","status":"completed"},"tags":[]},"source":["# Preprocessing dataset\n","Now we need to remove special characters including emoticones since apparently there are many emoticones in arabic tweets, we also need to remove punctuation and `tashkil` which is special character above letters to determine how they are pronounced\n","\n","So all that needs to be removed is:\n","* special characters and emoticones\n","* stop words\n","* ponctuation"]},{"cell_type":"code","execution_count":10,"id":"72f93710","metadata":{"execution":{"iopub.execute_input":"2023-01-12T09:52:48.702426Z","iopub.status.busy":"2023-01-12T09:52:48.701456Z","iopub.status.idle":"2023-01-12T09:52:50.678061Z","shell.execute_reply":"2023-01-12T09:52:50.676755Z"},"papermill":{"duration":1.993444,"end_time":"2023-01-12T09:52:50.681049","exception":false,"start_time":"2023-01-12T09:52:48.687605","status":"completed"},"tags":[]},"outputs":[],"source":["from nltk.corpus import stopwords\n","\n","def remove_stop_words(text):\n","    stop_words = set(stopwords.words(\"arabic\"))\n","    text = text.split()\n","    return \" \".join([word for word in text if word not in stop_words])\n","\n","import re\n","\n","def remove_emojis(text):\n","    emoji_pattern = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                           \"]+\", flags=re.UNICODE)\n","    return emoji_pattern.sub(r'', text)\n","\n","def remove_punctuation(text):\n","    return re.sub(r'[^\\w\\s]','',text)"]},{"cell_type":"code","execution_count":11,"id":"1a9d4ccc","metadata":{"execution":{"iopub.execute_input":"2023-01-12T09:52:50.707358Z","iopub.status.busy":"2023-01-12T09:52:50.70694Z","iopub.status.idle":"2023-01-12T09:53:11.786561Z","shell.execute_reply":"2023-01-12T09:53:11.785035Z"},"papermill":{"duration":21.095928,"end_time":"2023-01-12T09:53:11.78921","exception":false,"start_time":"2023-01-12T09:52:50.693282","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentiment</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>pos</td>\n","      <td>ÙŠØªØ­ÙˆÙ„ Ù†ÙˆØ¯ Ù†Ù‚ÙˆÙ„Ù‡ Ø¯Ø¹Ø§Ø¡ Ù„Ù„Ù‡ ØªØ¨Ø­Ø«ÙˆØ§ ÙÙŠÙ†Ø§ Ù‚ÙˆØ© Ø¥Ù†Ù†Ø§ ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>pos</td>\n","      <td>ÙˆÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ© ÙŠØ¨Ù‚Ù‰ Ù…Ø¹Ùƒ Ø¢Ø­Ø¯Ø¥Ù„Ø§ Ø±Ø£Ù‰ Ø§Ù„Ø¬Ù…Ø§Ù„ Ø±ÙˆØ­Ùƒ Ø£Ù…...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>pos</td>\n","      <td>Ø§Ù„Ø®ÙŠØ± Ù†ÙØ³Ù‡</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>pos</td>\n","      <td>Ø²Ù„Ø²Ù„_Ø§Ù„Ù…Ù„Ø¹Ø¨_Ù†ØµØ±Ù†Ø§_Ø¨ÙŠÙ„Ø¹Ø¨ Ø¹Ø§Ù„ÙŠ Ø§Ù„Ù‡Ù…Ù‡ ØªØ±Ø¶Ù‰ Ø¨ØºÙŠØ± Ø§...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>pos</td>\n","      <td>Ø§Ù„Ø´ÙŠØ¡ Ø§Ù„ÙˆØ­ÙŠØ¯ ÙˆØµÙ„ÙˆØ§ Ù„Ù„Ø¹Ø§Ù„Ù…ÙŠØ©  Ø§Ù„Ù…Ø³ÙŠØ§Ø±   ØªØ±Ù‰ ÙƒØ§Ù†...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>22509</th>\n","      <td>neg</td>\n","      <td>ØªØ±Ù‰ Ø£ÙˆØ±Ø§Ù†ÙˆØ³ ÙŠÙ‚Ø¹ Ù…ÙƒØ§Ù† Ø§Ù„Ù‚Ù…Ø±   ÙƒÙˆÙƒØ¨ Ø§Ù„Ø´Ù…Ø³ ÙŠØ¨Ù„Øº Ù‚...</td>\n","    </tr>\n","    <tr>\n","      <th>22510</th>\n","      <td>neg</td>\n","      <td>Ø§Ø­Ø³Ø¯Ùƒ Ø§Ù„Ø§ÙŠÙ…</td>\n","    </tr>\n","    <tr>\n","      <th>22511</th>\n","      <td>neg</td>\n","      <td>Ù„Ø£ÙˆÙ„ Ù…Ø±Ø© Ø¨Ù†ÙƒÙˆÙ† Ø³ÙˆØ§</td>\n","    </tr>\n","    <tr>\n","      <th>22512</th>\n","      <td>neg</td>\n","      <td>Ø¨Ù‚Ù„Ù‡ Ù„ÙŠØ´ ÙˆØ§Ø·ÙŠ</td>\n","    </tr>\n","    <tr>\n","      <th>22513</th>\n","      <td>neg</td>\n","      <td>Ø·Ø§Ù„ ØµØ¨Ø±ÙŠ Ø§Ù„Ù†ÙˆÙ‰ ØªØ±ÙƒØªÙ†ÙŠ ÙƒØ¦ÙŠØ¨Ø§  ØºØ±ÙŠØ¨Ø§ Ø¨Ø§ÙƒÙŠØ§ Ù…ØªÙˆØ¬Ø¹...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>45275 rows Ã— 2 columns</p>\n","</div>"],"text/plain":["      sentiment                                            content\n","0           pos  ÙŠØªØ­ÙˆÙ„ Ù†ÙˆØ¯ Ù†Ù‚ÙˆÙ„Ù‡ Ø¯Ø¹Ø§Ø¡ Ù„Ù„Ù‡ ØªØ¨Ø­Ø«ÙˆØ§ ÙÙŠÙ†Ø§ Ù‚ÙˆØ© Ø¥Ù†Ù†Ø§ ...\n","1           pos  ÙˆÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ© ÙŠØ¨Ù‚Ù‰ Ù…Ø¹Ùƒ Ø¢Ø­Ø¯Ø¥Ù„Ø§ Ø±Ø£Ù‰ Ø§Ù„Ø¬Ù…Ø§Ù„ Ø±ÙˆØ­Ùƒ Ø£Ù…...\n","2           pos                                        Ø§Ù„Ø®ÙŠØ± Ù†ÙØ³Ù‡ \n","3           pos  Ø²Ù„Ø²Ù„_Ø§Ù„Ù…Ù„Ø¹Ø¨_Ù†ØµØ±Ù†Ø§_Ø¨ÙŠÙ„Ø¹Ø¨ Ø¹Ø§Ù„ÙŠ Ø§Ù„Ù‡Ù…Ù‡ ØªØ±Ø¶Ù‰ Ø¨ØºÙŠØ± Ø§...\n","4           pos  Ø§Ù„Ø´ÙŠØ¡ Ø§Ù„ÙˆØ­ÙŠØ¯ ÙˆØµÙ„ÙˆØ§ Ù„Ù„Ø¹Ø§Ù„Ù…ÙŠØ©  Ø§Ù„Ù…Ø³ÙŠØ§Ø±   ØªØ±Ù‰ ÙƒØ§Ù†...\n","...         ...                                                ...\n","22509       neg  ØªØ±Ù‰ Ø£ÙˆØ±Ø§Ù†ÙˆØ³ ÙŠÙ‚Ø¹ Ù…ÙƒØ§Ù† Ø§Ù„Ù‚Ù…Ø±   ÙƒÙˆÙƒØ¨ Ø§Ù„Ø´Ù…Ø³ ÙŠØ¨Ù„Øº Ù‚...\n","22510       neg                                       Ø§Ø­Ø³Ø¯Ùƒ Ø§Ù„Ø§ÙŠÙ… \n","22511       neg                                Ù„Ø£ÙˆÙ„ Ù…Ø±Ø© Ø¨Ù†ÙƒÙˆÙ† Ø³ÙˆØ§ \n","22512       neg                                     Ø¨Ù‚Ù„Ù‡ Ù„ÙŠØ´ ÙˆØ§Ø·ÙŠ \n","22513       neg  Ø·Ø§Ù„ ØµØ¨Ø±ÙŠ Ø§Ù„Ù†ÙˆÙ‰ ØªØ±ÙƒØªÙ†ÙŠ ÙƒØ¦ÙŠØ¨Ø§  ØºØ±ÙŠØ¨Ø§ Ø¨Ø§ÙƒÙŠØ§ Ù…ØªÙˆØ¬Ø¹...\n","\n","[45275 rows x 2 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["X_train.content = X_train.content.apply(remove_stop_words)\n","X_train.content = X_train.content.apply(remove_emojis)\n","X_train.content = X_train.content.apply(remove_punctuation)\n","display(X_train)"]},{"cell_type":"markdown","id":"ed0b5b45","metadata":{"_cell_guid":"bc011b93-5ee3-47b1-b3e9-f66910d47ee1","_uuid":"f34fe2b0-69d1-4fc4-9154-2b23a28bfb66","papermill":{"duration":0.012054,"end_time":"2023-01-12T09:53:11.813831","exception":false,"start_time":"2023-01-12T09:53:11.801777","status":"completed"},"tags":[]},"source":["## Preprocessing dataset\n","Now we need to remove special characters## Tokenizing dataset\n","We need to write our tweets text as a feature-term dataframe (feature-term matrix). When using countvectorizer there is no need to preprocess the data, as it already removes stop words and speical characters.\n","\n","### Using CountVectorizer"]},{"cell_type":"code","execution_count":12,"id":"d08e7563","metadata":{"_cell_guid":"b5262f3a-9aa2-4b93-a6e7-8949ab7a9221","_uuid":"e3006a58-5728-4895-8ae6-36578619940b","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T09:53:11.840409Z","iopub.status.busy":"2023-01-12T09:53:11.83996Z","iopub.status.idle":"2023-01-12T09:53:12.694016Z","shell.execute_reply":"2023-01-12T09:53:12.692656Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.870937,"end_time":"2023-01-12T09:53:12.697046","exception":false,"start_time":"2023-01-12T09:53:11.826109","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(45275, 71031)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","count_vect = CountVectorizer()\n","X_train_counts = count_vect.fit_transform(X_train.content)\n","X_train_counts.shape"]},{"cell_type":"markdown","id":"1ed3b0f4","metadata":{"_cell_guid":"ca352397-dcdb-43c2-8e25-76fdd9f02ac2","_uuid":"3db34197-2475-4e45-a761-8e41e73a094b","papermill":{"duration":0.012296,"end_time":"2023-01-12T09:53:12.722099","exception":false,"start_time":"2023-01-12T09:53:12.709803","status":"completed"},"tags":[]},"source":["### Using TfidfTransformer"]},{"cell_type":"code","execution_count":13,"id":"7e1f1a5b","metadata":{"_cell_guid":"c12ed70f-ac79-4671-b21f-3c1259e30b0e","_uuid":"c70c0eda-68b1-4189-93d6-2030ab82e9e8","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T09:53:12.750347Z","iopub.status.busy":"2023-01-12T09:53:12.749177Z","iopub.status.idle":"2023-01-12T09:53:12.815553Z","shell.execute_reply":"2023-01-12T09:53:12.81429Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.083201,"end_time":"2023-01-12T09:53:12.817969","exception":false,"start_time":"2023-01-12T09:53:12.734768","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(45275, 71031)"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import TfidfTransformer\n","tfidf_transformer = TfidfTransformer()\n","X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n","X_train_tfidf.shape"]},{"cell_type":"markdown","id":"596c6436","metadata":{"_cell_guid":"e4fb18f7-951e-4373-ae73-345b9a60cf7f","_uuid":"15a5ad41-c2f5-4594-8384-0eb6d899cb5a","papermill":{"duration":0.012649,"end_time":"2023-01-12T09:53:12.843687","exception":false,"start_time":"2023-01-12T09:53:12.831038","status":"completed"},"tags":[]},"source":["## Training a classifier\n","### Using naÃ¯ve Bayes"]},{"cell_type":"code","execution_count":14,"id":"27ba42a0","metadata":{"_cell_guid":"645996d7-320d-4b51-b534-b142fc545514","_uuid":"4dd3088e-6f46-44eb-a7b7-d875a8af6e7f","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T09:53:12.871976Z","iopub.status.busy":"2023-01-12T09:53:12.871545Z","iopub.status.idle":"2023-01-12T09:53:13.009991Z","shell.execute_reply":"2023-01-12T09:53:13.008591Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.156612,"end_time":"2023-01-12T09:53:13.012929","exception":false,"start_time":"2023-01-12T09:53:12.856317","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.naive_bayes import MultinomialNB\n","clf = MultinomialNB().fit(X_train_tfidf, X_train.sentiment)"]},{"cell_type":"markdown","id":"b6bc401c","metadata":{"_cell_guid":"6dc0f87e-66d4-4973-9f11-768bdaf82254","_uuid":"dde58efe-9130-4465-bfe6-d09719bc25ef","papermill":{"duration":0.012211,"end_time":"2023-01-12T09:53:13.037748","exception":false,"start_time":"2023-01-12T09:53:13.025537","status":"completed"},"tags":[]},"source":["We will now use our testing dataset (we combine the negative and positivie tweests into one pandas dataframe), then we will call transfrom without calling fit in order to make a prediction."]},{"cell_type":"code","execution_count":15,"id":"b2ce3b71","metadata":{"_cell_guid":"2bc882d9-64f3-4084-839e-c15f1bf5f8f6","_uuid":"fe2ffab8-6e91-49ff-b6fd-0b558c7a3035","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T09:53:13.065399Z","iopub.status.busy":"2023-01-12T09:53:13.064716Z","iopub.status.idle":"2023-01-12T09:53:18.539513Z","shell.execute_reply":"2023-01-12T09:53:18.538231Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":5.491976,"end_time":"2023-01-12T09:53:18.542447","exception":false,"start_time":"2023-01-12T09:53:13.050471","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentiment</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>pos</td>\n","      <td>Ø§Ù„Ù‡Ù„Ø§Ù„_Ø§Ù„Ø§Ù‡Ù„ÙŠ ÙÙˆØ² Ù‡Ù„Ø§Ù„ÙŠ Ù…Ù‡Ù… Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡  Ø²ÙˆØ±Ø§Ù† Ø¨...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>pos</td>\n","      <td>ØµØ¨Ø§Ø­Ùƒ Ø®ÙŠØ±Ø§Øª ÙˆÙ…Ø³Ø±Ø§Øª</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>pos</td>\n","      <td>ØªØ£Ù…Ù„ Ù‚Ø§Ù„ Ø§Ù„Ù„Ù‡ ï·»  _Ø¨ÙˆØ§Ø¯ Ø²Ø±Ø¹ _    ÙˆÙ…Ø¹ Ù‡ØªÙ Ø¨Ø§Ù„Ø¯Ø¹Ø§...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>pos</td>\n","      <td>Ø¬Ø¯Ø¹Ø§Ù† Ø§Ù„Ø±Ø¬Ø§Ù„Ù‡ Ø§Ù„Ù„ÙŠ Ø§Ù„ Ø¯ÙˆÙ„ Ø®Ø·Ø± ØªÙˆÙŠØªØ± ÙˆØ±Ø¨Ù†Ø§ Ù…Ø´ ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>pos</td>\n","      <td>Ø±Ø³Ø§Ù„Ù‡ ØµØ¨Ø§Ø­ÙŠÙ‡   Ø§Ù„Ù„Ù‡Ù… Ø§Ø³Ø£Ù„Ùƒ Ø§Ù„ØªÙˆÙÙŠÙ‚ Ø§Ù…ÙˆØ±Ù†Ø§ ÙˆØ§ÙƒØª...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5763</th>\n","      <td>neg</td>\n","      <td>Ø§Ù„Ù†ÙˆÙ… ÙˆØ§Ù†Øª Ù…ÙƒØ³ÙˆØ± Ø¯Ù‡ Ø§Ø­Ø³Ø§Ø³ ØºØ¨ÙŠ Ø§Ù„Ù„ÙŠ Ù…Ø´ Ù‚Ø§Ø¯Ø± ØªÙ†Ø§...</td>\n","    </tr>\n","    <tr>\n","      <th>5764</th>\n","      <td>neg</td>\n","      <td>Ø§Ø³ØªØ´Ù‡Ø§Ø¯_Ø§Ù„Ø§Ù…Ø§Ù…_ÙƒØ§Ø¸Ù…_Ø§Ù„ØºÙŠØ¸ Ø§Ù„Ø³Ù„Ø§Ù… Ø§Ù„Ù…Ø¹Ø°Ø¨ Ù‚Ø¹Ø± Ø§Ù„...</td>\n","    </tr>\n","    <tr>\n","      <th>5765</th>\n","      <td>neg</td>\n","      <td>Ø§Ù†Ø§ ÙƒÙ†Øª Ø§ÙƒÙ„ Ø§Ù„ØµØ­Ù† Ø¨ÙƒØ¨Ø±Ù‡</td>\n","    </tr>\n","    <tr>\n","      <th>5766</th>\n","      <td>neg</td>\n","      <td>Ù‚ÙˆÙ„ÙˆØ§ Ø§ÙŠØ´ ØªØ´ÙˆÙÙˆØ§  Ù…Ù„Ø§Ø­Ø¸Ø© Ø§Ù„ØªÙ„Ø·Ù Ù„Ø£Ù†Ù‡ Ø§Ù„Ù…ÙˆØ¯</td>\n","    </tr>\n","    <tr>\n","      <th>5767</th>\n","      <td>neg</td>\n","      <td>Ø£Ø±Ø¯Øª ØªØ¹Ø±Ù Ø´ÙŠØ¦Ø§ Ø¹Ù†ÙŠ Ø¥Ø³Ø§Ù„Ù†ÙŠ ØªØ³Ø£Ù„ ØºÙŠØ±ÙŠ ÙØ¹Ø´Ø§Ù‚ Ø§Ù„Øª...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>11520 rows Ã— 2 columns</p>\n","</div>"],"text/plain":["     sentiment                                            content\n","0          pos  Ø§Ù„Ù‡Ù„Ø§Ù„_Ø§Ù„Ø§Ù‡Ù„ÙŠ ÙÙˆØ² Ù‡Ù„Ø§Ù„ÙŠ Ù…Ù‡Ù… Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡  Ø²ÙˆØ±Ø§Ù† Ø¨...\n","1          pos                                ØµØ¨Ø§Ø­Ùƒ Ø®ÙŠØ±Ø§Øª ÙˆÙ…Ø³Ø±Ø§Øª \n","2          pos  ØªØ£Ù…Ù„ Ù‚Ø§Ù„ Ø§Ù„Ù„Ù‡ ï·»  _Ø¨ÙˆØ§Ø¯ Ø²Ø±Ø¹ _    ÙˆÙ…Ø¹ Ù‡ØªÙ Ø¨Ø§Ù„Ø¯Ø¹Ø§...\n","3          pos   Ø¬Ø¯Ø¹Ø§Ù† Ø§Ù„Ø±Ø¬Ø§Ù„Ù‡ Ø§Ù„Ù„ÙŠ Ø§Ù„ Ø¯ÙˆÙ„ Ø®Ø·Ø± ØªÙˆÙŠØªØ± ÙˆØ±Ø¨Ù†Ø§ Ù…Ø´ ...\n","4          pos  Ø±Ø³Ø§Ù„Ù‡ ØµØ¨Ø§Ø­ÙŠÙ‡   Ø§Ù„Ù„Ù‡Ù… Ø§Ø³Ø£Ù„Ùƒ Ø§Ù„ØªÙˆÙÙŠÙ‚ Ø§Ù…ÙˆØ±Ù†Ø§ ÙˆØ§ÙƒØª...\n","...        ...                                                ...\n","5763       neg  Ø§Ù„Ù†ÙˆÙ… ÙˆØ§Ù†Øª Ù…ÙƒØ³ÙˆØ± Ø¯Ù‡ Ø§Ø­Ø³Ø§Ø³ ØºØ¨ÙŠ Ø§Ù„Ù„ÙŠ Ù…Ø´ Ù‚Ø§Ø¯Ø± ØªÙ†Ø§...\n","5764       neg  Ø§Ø³ØªØ´Ù‡Ø§Ø¯_Ø§Ù„Ø§Ù…Ø§Ù…_ÙƒØ§Ø¸Ù…_Ø§Ù„ØºÙŠØ¸ Ø§Ù„Ø³Ù„Ø§Ù… Ø§Ù„Ù…Ø¹Ø°Ø¨ Ù‚Ø¹Ø± Ø§Ù„...\n","5765       neg                           Ø§Ù†Ø§ ÙƒÙ†Øª Ø§ÙƒÙ„ Ø§Ù„ØµØ­Ù† Ø¨ÙƒØ¨Ø±Ù‡ \n","5766       neg        Ù‚ÙˆÙ„ÙˆØ§ Ø§ÙŠØ´ ØªØ´ÙˆÙÙˆØ§  Ù…Ù„Ø§Ø­Ø¸Ø© Ø§Ù„ØªÙ„Ø·Ù Ù„Ø£Ù†Ù‡ Ø§Ù„Ù…ÙˆØ¯ \n","5767       neg   Ø£Ø±Ø¯Øª ØªØ¹Ø±Ù Ø´ÙŠØ¦Ø§ Ø¹Ù†ÙŠ Ø¥Ø³Ø§Ù„Ù†ÙŠ ØªØ³Ø£Ù„ ØºÙŠØ±ÙŠ ÙØ¹Ø´Ø§Ù‚ Ø§Ù„Øª...\n","\n","[11520 rows x 2 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["X_test = pd.concat([test_tweets_positive,test_tweets_negative])\n","\n","X_test.content = X_test.content.apply(remove_stop_words)\n","X_test.content = X_test.content.apply(remove_emojis)\n","X_test.content = X_test.content.apply(remove_punctuation)\n","display(X_test)\n","\n","X_test_counts = count_vect.transform(X_test.content)\n","X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n","\n","predicted = clf.predict(X_test_tfidf)\n","\n","# for tweet, sentiment in zip(X_test.content, predicted):\n","#     print('%r => %s' % (tweet, sentiment))"]},{"cell_type":"markdown","id":"0eec854d","metadata":{"_cell_guid":"acb2606b-96b4-4d9f-9a71-cb155b57b87b","_uuid":"19c3b799-f686-4d8e-89a9-e1c5eb3b5c54","papermill":{"duration":0.012658,"end_time":"2023-01-12T09:53:18.568493","exception":false,"start_time":"2023-01-12T09:53:18.555835","status":"completed"},"tags":[]},"source":["## Calculating Accuracy Score\n","Now we need to get a percentage of the accuracy of our model, we have a list of predicted sentiment and a list of the actual sentiment. Whenever predicted sentiment is different of the actual one we will increment a counter, after going through all the tweets we will divide but the total number of tweets to get a percentage of the wrong predictions, to get the percentage of the right prediction all we need to do is subtract the calculated score from 100%."]},{"cell_type":"code","execution_count":16,"id":"dc44a40a","metadata":{"_cell_guid":"19c26993-64cb-49bb-b033-f7186a705740","_uuid":"afb6c115-b16d-4fb9-a060-ef68a1aa2bdf","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T09:53:18.597172Z","iopub.status.busy":"2023-01-12T09:53:18.59663Z","iopub.status.idle":"2023-01-12T09:53:18.621253Z","shell.execute_reply":"2023-01-12T09:53:18.619546Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.042305,"end_time":"2023-01-12T09:53:18.624449","exception":false,"start_time":"2023-01-12T09:53:18.582144","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["validity score: 78.11631944444444%\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentiment</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>pos</td>\n","      <td>Ø§Ù„Ù‡Ù„Ø§Ù„_Ø§Ù„Ø§Ù‡Ù„ÙŠ ÙÙˆØ² Ù‡Ù„Ø§Ù„ÙŠ Ù…Ù‡Ù… Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡  Ø²ÙˆØ±Ø§Ù† Ø¨...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>pos</td>\n","      <td>ØµØ¨Ø§Ø­Ùƒ Ø®ÙŠØ±Ø§Øª ÙˆÙ…Ø³Ø±Ø§Øª</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>pos</td>\n","      <td>ØªØ£Ù…Ù„ Ù‚Ø§Ù„ Ø§Ù„Ù„Ù‡ ï·»  _Ø¨ÙˆØ§Ø¯ Ø²Ø±Ø¹ _    ÙˆÙ…Ø¹ Ù‡ØªÙ Ø¨Ø§Ù„Ø¯Ø¹Ø§...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>pos</td>\n","      <td>Ø¬Ø¯Ø¹Ø§Ù† Ø§Ù„Ø±Ø¬Ø§Ù„Ù‡ Ø§Ù„Ù„ÙŠ Ø§Ù„ Ø¯ÙˆÙ„ Ø®Ø·Ø± ØªÙˆÙŠØªØ± ÙˆØ±Ø¨Ù†Ø§ Ù…Ø´ ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>pos</td>\n","      <td>Ø±Ø³Ø§Ù„Ù‡ ØµØ¨Ø§Ø­ÙŠÙ‡   Ø§Ù„Ù„Ù‡Ù… Ø§Ø³Ø£Ù„Ùƒ Ø§Ù„ØªÙˆÙÙŠÙ‚ Ø§Ù…ÙˆØ±Ù†Ø§ ÙˆØ§ÙƒØª...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5763</th>\n","      <td>neg</td>\n","      <td>Ø§Ù„Ù†ÙˆÙ… ÙˆØ§Ù†Øª Ù…ÙƒØ³ÙˆØ± Ø¯Ù‡ Ø§Ø­Ø³Ø§Ø³ ØºØ¨ÙŠ Ø§Ù„Ù„ÙŠ Ù…Ø´ Ù‚Ø§Ø¯Ø± ØªÙ†Ø§...</td>\n","    </tr>\n","    <tr>\n","      <th>5764</th>\n","      <td>neg</td>\n","      <td>Ø§Ø³ØªØ´Ù‡Ø§Ø¯_Ø§Ù„Ø§Ù…Ø§Ù…_ÙƒØ§Ø¸Ù…_Ø§Ù„ØºÙŠØ¸ Ø§Ù„Ø³Ù„Ø§Ù… Ø§Ù„Ù…Ø¹Ø°Ø¨ Ù‚Ø¹Ø± Ø§Ù„...</td>\n","    </tr>\n","    <tr>\n","      <th>5765</th>\n","      <td>neg</td>\n","      <td>Ø§Ù†Ø§ ÙƒÙ†Øª Ø§ÙƒÙ„ Ø§Ù„ØµØ­Ù† Ø¨ÙƒØ¨Ø±Ù‡</td>\n","    </tr>\n","    <tr>\n","      <th>5766</th>\n","      <td>neg</td>\n","      <td>Ù‚ÙˆÙ„ÙˆØ§ Ø§ÙŠØ´ ØªØ´ÙˆÙÙˆØ§  Ù…Ù„Ø§Ø­Ø¸Ø© Ø§Ù„ØªÙ„Ø·Ù Ù„Ø£Ù†Ù‡ Ø§Ù„Ù…ÙˆØ¯</td>\n","    </tr>\n","    <tr>\n","      <th>5767</th>\n","      <td>neg</td>\n","      <td>Ø£Ø±Ø¯Øª ØªØ¹Ø±Ù Ø´ÙŠØ¦Ø§ Ø¹Ù†ÙŠ Ø¥Ø³Ø§Ù„Ù†ÙŠ ØªØ³Ø£Ù„ ØºÙŠØ±ÙŠ ÙØ¹Ø´Ø§Ù‚ Ø§Ù„Øª...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>11520 rows Ã— 2 columns</p>\n","</div>"],"text/plain":["     sentiment                                            content\n","0          pos  Ø§Ù„Ù‡Ù„Ø§Ù„_Ø§Ù„Ø§Ù‡Ù„ÙŠ ÙÙˆØ² Ù‡Ù„Ø§Ù„ÙŠ Ù…Ù‡Ù… Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡  Ø²ÙˆØ±Ø§Ù† Ø¨...\n","1          pos                                ØµØ¨Ø§Ø­Ùƒ Ø®ÙŠØ±Ø§Øª ÙˆÙ…Ø³Ø±Ø§Øª \n","2          pos  ØªØ£Ù…Ù„ Ù‚Ø§Ù„ Ø§Ù„Ù„Ù‡ ï·»  _Ø¨ÙˆØ§Ø¯ Ø²Ø±Ø¹ _    ÙˆÙ…Ø¹ Ù‡ØªÙ Ø¨Ø§Ù„Ø¯Ø¹Ø§...\n","3          pos   Ø¬Ø¯Ø¹Ø§Ù† Ø§Ù„Ø±Ø¬Ø§Ù„Ù‡ Ø§Ù„Ù„ÙŠ Ø§Ù„ Ø¯ÙˆÙ„ Ø®Ø·Ø± ØªÙˆÙŠØªØ± ÙˆØ±Ø¨Ù†Ø§ Ù…Ø´ ...\n","4          pos  Ø±Ø³Ø§Ù„Ù‡ ØµØ¨Ø§Ø­ÙŠÙ‡   Ø§Ù„Ù„Ù‡Ù… Ø§Ø³Ø£Ù„Ùƒ Ø§Ù„ØªÙˆÙÙŠÙ‚ Ø§Ù…ÙˆØ±Ù†Ø§ ÙˆØ§ÙƒØª...\n","...        ...                                                ...\n","5763       neg  Ø§Ù„Ù†ÙˆÙ… ÙˆØ§Ù†Øª Ù…ÙƒØ³ÙˆØ± Ø¯Ù‡ Ø§Ø­Ø³Ø§Ø³ ØºØ¨ÙŠ Ø§Ù„Ù„ÙŠ Ù…Ø´ Ù‚Ø§Ø¯Ø± ØªÙ†Ø§...\n","5764       neg  Ø§Ø³ØªØ´Ù‡Ø§Ø¯_Ø§Ù„Ø§Ù…Ø§Ù…_ÙƒØ§Ø¸Ù…_Ø§Ù„ØºÙŠØ¸ Ø§Ù„Ø³Ù„Ø§Ù… Ø§Ù„Ù…Ø¹Ø°Ø¨ Ù‚Ø¹Ø± Ø§Ù„...\n","5765       neg                           Ø§Ù†Ø§ ÙƒÙ†Øª Ø§ÙƒÙ„ Ø§Ù„ØµØ­Ù† Ø¨ÙƒØ¨Ø±Ù‡ \n","5766       neg        Ù‚ÙˆÙ„ÙˆØ§ Ø§ÙŠØ´ ØªØ´ÙˆÙÙˆØ§  Ù…Ù„Ø§Ø­Ø¸Ø© Ø§Ù„ØªÙ„Ø·Ù Ù„Ø£Ù†Ù‡ Ø§Ù„Ù…ÙˆØ¯ \n","5767       neg   Ø£Ø±Ø¯Øª ØªØ¹Ø±Ù Ø´ÙŠØ¦Ø§ Ø¹Ù†ÙŠ Ø¥Ø³Ø§Ù„Ù†ÙŠ ØªØ³Ø£Ù„ ØºÙŠØ±ÙŠ ÙØ¹Ø´Ø§Ù‚ Ø§Ù„Øª...\n","\n","[11520 rows x 2 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["wrong_predictions = 0\n","validity_score = 0\n","for predicted_sentiment, actual_sentiment in zip(predicted, X_test.sentiment):\n","    if predicted_sentiment != actual_sentiment:\n","        wrong_predictions += 1\n","wrong_predictions_percentage = wrong_predictions / len(X_test.sentiment)\n","validity_score = 1 - wrong_predictions_percentage\n","print(\"validity score: \" + str(validity_score*100) + \"%\")\n","display(X_test)"]},{"cell_type":"markdown","id":"0483028a","metadata":{"_cell_guid":"6b21ef9a-866c-4b88-8577-164f8a1e29ec","_uuid":"48435d85-c47f-4438-b829-1aee03325841","papermill":{"duration":0.01322,"end_time":"2023-01-12T09:53:18.65265","exception":false,"start_time":"2023-01-12T09:53:18.63943","status":"completed"},"tags":[]},"source":["For the time being we have a validity score of `78.4375%` therefore our prediction model is considred bad, we think it is because we're studying text in `arabic` CountVectorizer is unable to correctly preprocess text and tokenize it. We will try to use another vectorizer to see if the validity increases.\n","\n","### Building a pipeline\n","To simplify our training and prediction process we will build a new Pipeline"]},{"cell_type":"code","execution_count":17,"id":"5c383e2e","metadata":{"_cell_guid":"12907a38-023f-4149-81c8-a385f64f7186","_uuid":"6ccd3360-b20a-4c43-a609-ed1b718c5dd0","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T09:53:18.682337Z","iopub.status.busy":"2023-01-12T09:53:18.681128Z","iopub.status.idle":"2023-01-12T09:53:18.690474Z","shell.execute_reply":"2023-01-12T09:53:18.689149Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.027086,"end_time":"2023-01-12T09:53:18.693376","exception":false,"start_time":"2023-01-12T09:53:18.66629","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import SGDClassifier\n","\n","text_clf = Pipeline([\n","    ('vect', CountVectorizer()),\n","    ('tfidf', TfidfTransformer()),\n","    ('clf', SGDClassifier(loss='hinge', penalty=None,\n","                          alpha=1e-3, random_state=2,\n","                          max_iter=5, tol=None)),\n","])"]},{"cell_type":"markdown","id":"f9b20506","metadata":{"_cell_guid":"2d6a1601-c2c5-46ab-93bf-4a1f7fcd57e6","_uuid":"0e26a857-855e-4042-bc5a-ff8758c44c09","papermill":{"duration":0.013406,"end_time":"2023-01-12T09:53:18.720396","exception":false,"start_time":"2023-01-12T09:53:18.70699","status":"completed"},"tags":[]},"source":["### Using SGDClassifierfrom sklearn.linear_model import SGDClassifier"]},{"cell_type":"code","execution_count":18,"id":"f1b56dd2","metadata":{"_cell_guid":"38158940-ab9a-4410-9808-30357a62a7fb","_uuid":"279b3351-878e-45ac-b87e-0799205f4640","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T09:53:18.750019Z","iopub.status.busy":"2023-01-12T09:53:18.74933Z","iopub.status.idle":"2023-01-12T09:53:20.016373Z","shell.execute_reply":"2023-01-12T09:53:20.014939Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":1.284493,"end_time":"2023-01-12T09:53:20.019331","exception":false,"start_time":"2023-01-12T09:53:18.734838","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["0.73515625"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["text_clf.fit(X_train.content, X_train.sentiment)\n","\n","predicted = text_clf.predict(X_test.content)\n","np.mean(predicted == X_test.sentiment)"]},{"cell_type":"markdown","id":"f3646064","metadata":{"_cell_guid":"68a204ce-0b94-43ec-8546-423aac5564e5","_uuid":"6ddfb7ef-00f4-4870-b61b-411604437e54","papermill":{"duration":0.013078,"end_time":"2023-01-12T09:53:20.046011","exception":false,"start_time":"2023-01-12T09:53:20.032933","status":"completed"},"tags":[]},"source":["Now we will try to `Lemmetize` tweets, we believe this is the reason why our prediction model isn't producing better results.\n","We could try `Farasa` lemmetizer as it has a good reputation of outperforming other lemmetizers. But it uses an API and is not available as an imported library usable directly.\n","So for now we will just stick with ntlk's `ISRIStemmer`"]},{"cell_type":"code","execution_count":19,"id":"a50cc676","metadata":{"_cell_guid":"ea1968d1-15bf-4de2-8fe0-181a0e161618","_uuid":"e8ed1312-35a1-4711-a76f-01e1c5296650","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T09:53:20.076335Z","iopub.status.busy":"2023-01-12T09:53:20.075909Z","iopub.status.idle":"2023-01-12T09:53:20.081928Z","shell.execute_reply":"2023-01-12T09:53:20.080623Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.024627,"end_time":"2023-01-12T09:53:20.084398","exception":false,"start_time":"2023-01-12T09:53:20.059771","status":"completed"},"tags":[]},"outputs":[],"source":["from nltk.stem.isri import ISRIStemmer\n","st = ISRIStemmer()"]},{"cell_type":"markdown","id":"fc5b6b47","metadata":{"_cell_guid":"9f278e4f-df5c-4a51-a45c-21447f11fdda","_uuid":"6f7b793d-49ca-48e1-9e0b-36a22d947f38","papermill":{"duration":0.013035,"end_time":"2023-01-12T09:53:20.110872","exception":false,"start_time":"2023-01-12T09:53:20.097837","status":"completed"},"tags":[]},"source":["A lemmetizer is also called a stemmer, NLTK has many stemmer solutions. We could try out each one of NLTK's stemmers and compare the results.\n","For now we will try out `ARLSTem Stemmer`\n","\n","* THE NEW PROCESS IS NOW:\n","1. tokenize\n","2. stem\n","3. remove stop words"]},{"cell_type":"code","execution_count":20,"id":"1db29f5e","metadata":{"_cell_guid":"a53fabfb-3f1e-4226-886c-d5889bb8af5e","_uuid":"ca44c7c4-b9d4-459c-a9f7-83ced75b3e21","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T09:53:20.138852Z","iopub.status.busy":"2023-01-12T09:53:20.138428Z","iopub.status.idle":"2023-01-12T09:53:20.354751Z","shell.execute_reply":"2023-01-12T09:53:20.353536Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.233755,"end_time":"2023-01-12T09:53:20.357774","exception":false,"start_time":"2023-01-12T09:53:20.124019","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["0       Ù‡Ù„Ø§Ù„_Ø§Ù„Ø§Ù‡Ù„ÙŠ ÙÙˆØ² Ù‡Ù„Ø§Ù„ÙŠ Ù…Ù‡Ù… Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡  Ø²ÙˆØ±Ø§Ù† Ø¨ÙŠØ³...\n","1                                     ØµØ¨Ø§Ø­Ùƒ Ø®ÙŠØ±Ø§Øª ÙˆÙ…Ø³Ø±Ø§Øª \n","2       Ø§Ù…Ù„ Ù‚Ø§Ù„ Ø§Ù„Ù„Ù‡ ï·»  _Ø¨ÙˆØ§Ø¯ Ø²Ø±Ø¹ _    ÙˆÙ…Ø¹ Ù‡ØªÙ Ø¨Ø§Ù„Ø¯Ø¹Ø§Ø¡...\n","3        Ø¬Ø¯Ø¹Ø§Ù† Ø§Ù„Ø±Ø¬Ø§Ù„Ù‡ Ø§Ù„Ù„ÙŠ Ø§Ù„ Ø¯ÙˆÙ„ Ø®Ø·Ø± ØªÙˆÙŠØªØ± ÙˆØ±Ø¨Ù†Ø§ Ù…Ø´ ...\n","4       Ø±Ø³Ø§Ù„Ù‡ ØµØ¨Ø§Ø­ÙŠÙ‡   Ø§Ù„Ù„Ù‡Ù… Ø§Ø³Ø§Ù„Ùƒ Ø§Ù„ØªÙˆÙÙŠÙ‚ Ø§Ù…ÙˆØ±Ù†Ø§ ÙˆØ§ÙƒØª...\n","                              ...                        \n","5763    Ù†ÙˆÙ… ÙˆØ§Ù†Øª Ù…ÙƒØ³ÙˆØ± Ø¯Ù‡ Ø§Ø­Ø³Ø§Ø³ ØºØ¨ÙŠ Ø§Ù„Ù„ÙŠ Ù…Ø´ Ù‚Ø§Ø¯Ø± ØªÙ†Ø§Ù… ...\n","5764    Ø³ØªØ´Ù‡Ø§Ø¯_Ø§Ù„Ø§Ù…Ø§Ù…_ÙƒØ§Ø¸Ù…_Ø§Ù„ØºÙŠØ¸ Ø§Ù„Ø³Ù„Ø§Ù… Ø§Ù„Ù…Ø¹Ø°Ø¨ Ù‚Ø¹Ø± Ø§Ù„Ø³...\n","5765                              Ø§Ù† ÙƒÙ†Øª Ø§ÙƒÙ„ Ø§Ù„ØµØ­Ù† Ø¨ÙƒØ¨Ø±Ù‡ \n","5766          Ù‚ÙˆÙ„ÙˆØ§ Ø§ÙŠØ´ ØªØ´ÙˆÙÙˆØ§  Ù…Ù„Ø§Ø­Ø¸Ø© Ø§Ù„ØªÙ„Ø·Ù Ù„Ø§Ù†Ù‡ Ø§Ù„Ù…ÙˆØ¯ \n","5767     Ø§Ø±Ø¯Øª ØªØ¹Ø±Ù Ø´ÙŠØ¦Ø§ Ø¹Ù†ÙŠ Ø§Ø³Ø§Ù„Ù†ÙŠ ØªØ³Ø§Ù„ ØºÙŠØ±ÙŠ ÙØ¹Ø´Ø§Ù‚ Ø§Ù„Øª...\n","Name: content, Length: 11520, dtype: object"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.stem.arlstem import ARLSTem\n","\n","stemmer = ARLSTem()\n","X_test.content.apply(stemmer.stem)"]},{"cell_type":"markdown","id":"b39c7e87","metadata":{"_cell_guid":"32fd7ae3-82bb-4873-8366-0fcd80965846","_uuid":"4415217a-9fe2-41e1-b88f-3dc74e334cf0","papermill":{"duration":0.013788,"end_time":"2023-01-12T09:53:20.385641","exception":false,"start_time":"2023-01-12T09:53:20.371853","status":"completed"},"tags":[]},"source":["# NLTK tokenization\n","    So we're still getting a very low validity score for our prediction model. It could be because we're using Arabic language and scikit learn is unable to correctly tokenize words, it could be that the words that have the same root arent' considred as the same token. I will try to preprocess the data first to turn each tweet text into tokens. \n","    A problem that could arise is not being able to detect the order of words. I'm not sure if NLtk will scramble the words or will they be in the same order for us to be able to use n-gram of words later on."]},{"cell_type":"code","execution_count":21,"id":"1f43bfc5","metadata":{"_cell_guid":"96aef751-938c-4c7d-a529-014ba033a901","_uuid":"4de24fee-81c6-40ea-941f-0b00169a6a6e","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T09:53:20.414977Z","iopub.status.busy":"2023-01-12T09:53:20.414553Z","iopub.status.idle":"2023-01-12T09:53:20.839514Z","shell.execute_reply":"2023-01-12T09:53:20.837878Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.442903,"end_time":"2023-01-12T09:53:20.842431","exception":false,"start_time":"2023-01-12T09:53:20.399528","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["0        [ÙŠØªØ­ÙˆÙ„, Ù†ÙˆØ¯, Ù†Ù‚ÙˆÙ„Ù‡, Ø¯Ø¹Ø§Ø¡, Ù„Ù„Ù‡, ØªØ¨Ø­Ø«ÙˆØ§, ÙÙŠÙ†Ø§, Ù‚...\n","1        [ÙˆÙÙŠ, Ø§Ù„Ù†Ù‡Ø§ÙŠØ©, ÙŠØ¨Ù‚Ù‰, Ù…Ø¹Ùƒ, Ø¢Ø­Ø¯Ø¥Ù„Ø§, Ø±Ø£Ù‰, Ø§Ù„Ø¬Ù…Ø§Ù„,...\n","2                                            [Ø§Ù„Ø®ÙŠØ±, Ù†ÙØ³Ù‡]\n","3        [Ø²Ù„Ø²Ù„_Ø§Ù„Ù…Ù„Ø¹Ø¨_Ù†ØµØ±Ù†Ø§_Ø¨ÙŠÙ„Ø¹Ø¨, Ø¹Ø§Ù„ÙŠ, Ø§Ù„Ù‡Ù…Ù‡, ØªØ±Ø¶Ù‰, Ø¨...\n","4        [Ø§Ù„Ø´ÙŠØ¡, Ø§Ù„ÙˆØ­ÙŠØ¯, ÙˆØµÙ„ÙˆØ§, Ù„Ù„Ø¹Ø§Ù„Ù…ÙŠØ©, Ø§Ù„Ù…Ø³ÙŠØ§Ø±, ØªØ±Ù‰,...\n","                               ...                        \n","22509    [ØªØ±Ù‰, Ø£ÙˆØ±Ø§Ù†ÙˆØ³, ÙŠÙ‚Ø¹, Ù…ÙƒØ§Ù†, Ø§Ù„Ù‚Ù…Ø±, ÙƒÙˆÙƒØ¨, Ø§Ù„Ø´Ù…Ø³, ...\n","22510                                       [Ø§Ø­Ø³Ø¯Ùƒ, Ø§Ù„Ø§ÙŠÙ…]\n","22511                              [Ù„Ø£ÙˆÙ„, Ù…Ø±Ø©, Ø¨Ù†ÙƒÙˆÙ†, Ø³ÙˆØ§]\n","22512                                    [Ø¨Ù‚Ù„Ù‡, Ù„ÙŠØ´, ÙˆØ§Ø·ÙŠ]\n","22513    [Ø·Ø§Ù„, ØµØ¨Ø±ÙŠ, Ø§Ù„Ù†ÙˆÙ‰, ØªØ±ÙƒØªÙ†ÙŠ, ÙƒØ¦ÙŠØ¨Ø§, ØºØ±ÙŠØ¨Ø§, Ø¨Ø§ÙƒÙŠØ§...\n","Name: content, Length: 45275, dtype: object"]},"metadata":{},"output_type":"display_data"}],"source":["from nltk.tokenize import wordpunct_tokenize,word_tokenize\n","\n","X_train = X_train.content.apply(wordpunct_tokenize)\n","display(X_train)"]},{"cell_type":"markdown","id":"d56b9aa3","metadata":{"_cell_guid":"015c2d51-da19-4e95-a093-cd475b67125b","_uuid":"35301da8-3cd9-46cb-b859-52045c8d4cf0","papermill":{"duration":0.014157,"end_time":"2023-01-12T09:53:20.87037","exception":false,"start_time":"2023-01-12T09:53:20.856213","status":"completed"},"tags":[]},"source":["arabic isn't supported by `nltk` so we'll use some other third party library to tokenize our te"]},{"cell_type":"code","execution_count":22,"id":"7fd624ba","metadata":{"_cell_guid":"76f933a7-228b-4b26-bdc6-8cbdf1fbd3fd","_uuid":"5d06e677-34eb-4331-9fa0-1234bd40fc62","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T09:53:20.900277Z","iopub.status.busy":"2023-01-12T09:53:20.899788Z","iopub.status.idle":"2023-01-12T09:53:20.905165Z","shell.execute_reply":"2023-01-12T09:53:20.903874Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.023357,"end_time":"2023-01-12T09:53:20.907907","exception":false,"start_time":"2023-01-12T09:53:20.88455","status":"completed"},"tags":[]},"outputs":[],"source":["# import tkseem as tk\n","\n","# tkseem_tokenizer = tk.WordTokenizer()\n","# X_train.content.apply(tkseem_tokenizer.tokenize)"]},{"cell_type":"markdown","id":"5e051402","metadata":{"_cell_guid":"f36c4f68-7a1b-483b-b4b3-45c7174ab3c2","_uuid":"20577384-4ae0-4530-9802-78167ed0ad6b","papermill":{"duration":0.013943,"end_time":"2023-01-12T09:53:20.936167","exception":false,"start_time":"2023-01-12T09:53:20.922224","status":"completed"},"tags":[]},"source":["### Parameter tuning using grid search"]},{"cell_type":"code","execution_count":23,"id":"5af6019d","metadata":{"_cell_guid":"94b5d231-80d4-4a38-8955-53d142cc0be7","_uuid":"de14a436-17c2-4e5a-9eec-705f85f65406","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T09:53:20.967093Z","iopub.status.busy":"2023-01-12T09:53:20.966608Z","iopub.status.idle":"2023-01-12T09:53:20.971604Z","shell.execute_reply":"2023-01-12T09:53:20.970454Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.023552,"end_time":"2023-01-12T09:53:20.974367","exception":false,"start_time":"2023-01-12T09:53:20.950815","status":"completed"},"tags":[]},"outputs":[],"source":["# from sklearn.model_selection import GridSearchCV\n","\n","# parameters = {\n","#     'vect__ngram_range': [(1, 1), (1, 2)],\n","#     'tfidf__use_idf': (True, False),\n","#     'clf__alpha': (1e-2, 1e-3),\n","# }\n","\n","# gs_clf = GridSearchCV(text_clf, parameters, cv=5, n_jobs=-1)\n","# gs_clf = gs_clf.fit(X_train.content[:400], X_train.sentiment[:400])"]},{"cell_type":"markdown","id":"bb6ec266","metadata":{"_cell_guid":"681e47fe-a024-4c79-b2ba-76c088564b66","_uuid":"29fb8111-45f0-480c-8a28-665dd790f1e3","papermill":{"duration":0.013889,"end_time":"2023-01-12T09:53:21.002506","exception":false,"start_time":"2023-01-12T09:53:20.988617","status":"completed"},"tags":[]},"source":["## Extracting features from tweets data\n","    After loading the data the next step is to try and extract features from our tweets corpus."]},{"cell_type":"markdown","id":"ea10d9bb","metadata":{"papermill":{"duration":0.013877,"end_time":"2023-01-12T09:53:21.030643","exception":false,"start_time":"2023-01-12T09:53:21.016766","status":"completed"},"tags":[]},"source":["# testing with nltk's own sentimentAnalyzer\n","Now we'll try to compare our model to NLTK's `sentimentIntensityAnalyzer` to see which one provides better results"]},{"cell_type":"code","execution_count":24,"id":"86b16dd5","metadata":{"execution":{"iopub.execute_input":"2023-01-12T09:53:21.060507Z","iopub.status.busy":"2023-01-12T09:53:21.060063Z","iopub.status.idle":"2023-01-12T09:53:21.087612Z","shell.execute_reply":"2023-01-12T09:53:21.086571Z"},"papermill":{"duration":0.045862,"end_time":"2023-01-12T09:53:21.090396","exception":false,"start_time":"2023-01-12T09:53:21.044534","status":"completed"},"tags":[]},"outputs":[],"source":["from nltk.sentiment import SentimentIntensityAnalyzer\n","sia = SentimentIntensityAnalyzer()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":57.590032,"end_time":"2023-01-12T09:53:22.027554","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-01-12T09:52:24.437522","version":"2.3.4"}},"nbformat":4,"nbformat_minor":5}