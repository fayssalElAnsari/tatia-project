{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/fayssalelansari/arabic-tweets-sentiment-analysis?scriptVersionId=116165826\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","id":"123c10df","metadata":{"papermill":{"duration":0.013854,"end_time":"2023-01-12T10:21:46.355025","exception":false,"start_time":"2023-01-12T10:21:46.341171","status":"completed"},"tags":[]},"source":["# TODO LIST\n","* [X] PREPROCESS DATA\n","* [X] STEM ARABIC WORDS\n","* [x] ADD DEFINITIONS FOR COMMON KEYWORDS\n","* [x] DEFINE THE EXACT STEPS USED IN THE PIPELINE (MAKE A DIAGRAM)\n","* [ ] DEFINE FOR EACH STEP THE LIST OF POSSIBLE METHODS TO USE\n","* [ ] DEFINE FOR EACH METHOD A SMALL LIST OF POSSIBLE CONFIGURATION FOR THE PARAMETERS TO USE\n","* [ ] MAKE PIPELINE\n","* [ ] EMBEDDINGS\n","* [ ] CONFUSION MATRIX"]},{"cell_type":"markdown","id":"e43488e7","metadata":{"papermill":{"duration":0.013662,"end_time":"2023-01-12T10:21:46.3805","exception":false,"start_time":"2023-01-12T10:21:46.366838","status":"completed"},"tags":[]},"source":["1. How to split data into `test` and `train` data-sets from one initial data-set?\n","2. Emojis convey emotions so instead of deleting them we should substitute each emoji with the emotion it represents\n","3. when applying different steps of the Pipeline, instead of putting new data in a new variable or data frame just add a `new column` with a representive tag."]},{"cell_type":"markdown","id":"92530f94","metadata":{"papermill":{"duration":0.011689,"end_time":"2023-01-12T10:21:46.40405","exception":false,"start_time":"2023-01-12T10:21:46.392361","status":"completed"},"tags":[]},"source":["# Ideas to implement\n","1. webapp:\n","    * make a front end connected to a trained model or give the user the ability to train his own model.\n","    * in the webpage there will be a way to fetch a predetermined number of tweets using a keyword or a hashtag (should be in arabic). \n","2. Since for the same step there could be different methods and for each method different methods add an easier way to change the method and the parameters, could be modified in the web page also.\n","3. show graphs for different stats of the chosen models and methods\n","4. a way to run all possible (or a lot) methods and parameters and order them by decreasing order of validity"]},{"cell_type":"markdown","id":"48b25515","metadata":{"papermill":{"duration":0.010898,"end_time":"2023-01-12T10:21:46.42682","exception":false,"start_time":"2023-01-12T10:21:46.415922","status":"completed"},"tags":[]},"source":["# Definitions of Common Terms\n","**Normalizing**: Also called text cleansing, is the process of preprocessing text data to be understood and used by Natural Language Processors and other text analytics software. It includes many other substeps such as `tokenizing`, `case conversion`, `correcting spelling`, `removing stop words`, `stemming` and `lemmatization`.\n","\n","**Tokenization**: It is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens. Usually we split a text document by word to obtain a token for each individual word in a list of tokens.\n","\n","**Stop words**: A list of the most common words in a language\n","\n","**Vectorization**: The process of writing tokens of text data in the form of vectors, since computer programs are only able to understand numbers it is important to vectorize text data in order to process it. Also called `Feature extraction`; There are many feature-extraction methods the most famous being `bag of words based frequency features`.\n","\n","**Feature Matrix**: In this matrix each row represents a document (here tweet) and each column represents a feature (processed token). It is used by ML and statistical models to provide predictions.\n","\n","**Stemming**: Consists of extracting only the smallest uncompressable part of a word not containing any suffixes or prefixes or any other inflections. Then converting it to the root or the source of the meaning of that word.\n","\n","**lemmatization**: Similar to `Stemming` is the same process of removing inflections from a feature (token) but without converting it to its origin of meaning. It is less representative for making prediction since more words can have the same stem than the the words having the same lemma.\n","\n","**Bag of Words Model**: One of the most popular yet simple Feature Extraction methods, for each document we build a set of words without any regard of the order of presence (hence the name bag of words). But keeping count for each word.\n"]},{"cell_type":"markdown","id":"715c817d","metadata":{"papermill":{"duration":0.011101,"end_time":"2023-01-12T10:21:46.449189","exception":false,"start_time":"2023-01-12T10:21:46.438088","status":"completed"},"tags":[]},"source":["# References\n","...\n"]},{"cell_type":"markdown","id":"a8983f51","metadata":{"papermill":{"duration":0.011289,"end_time":"2023-01-12T10:21:46.4723","exception":false,"start_time":"2023-01-12T10:21:46.461011","status":"completed"},"tags":[]},"source":["## Diagram of Steps \n","In this section we will build a diagram showcasing the different steps used to analyse a set of tweets. \n","* For each step of the process we will put all the possible methods in a cluster.\n","* Between each step there will be an arrow with a label describing how to reach the next step."]},{"cell_type":"code","execution_count":1,"id":"c5709fec","metadata":{"execution":{"iopub.execute_input":"2023-01-12T10:21:46.498415Z","iopub.status.busy":"2023-01-12T10:21:46.497758Z","iopub.status.idle":"2023-01-12T10:22:01.192154Z","shell.execute_reply":"2023-01-12T10:22:01.190694Z"},"papermill":{"duration":14.710612,"end_time":"2023-01-12T10:22:01.194858","exception":false,"start_time":"2023-01-12T10:21:46.484246","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting nb-js-diagrammers\r\n","  Downloading nb_js_diagrammers-0.0.7-py3-none-any.whl (8.8 kB)\r\n","Collecting pyflowchart\r\n","  Downloading pyflowchart-0.2.3-py3-none-any.whl (18 kB)\r\n","Requirement already satisfied: astunparse in /opt/conda/lib/python3.7/site-packages (from pyflowchart->nb-js-diagrammers) (1.6.3)\r\n","Requirement already satisfied: chardet in /opt/conda/lib/python3.7/site-packages (from pyflowchart->nb-js-diagrammers) (5.0.0)\r\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse->pyflowchart->nb-js-diagrammers) (0.37.1)\r\n","Requirement already satisfied: six<2.0,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from astunparse->pyflowchart->nb-js-diagrammers) (1.15.0)\r\n","Installing collected packages: pyflowchart, nb-js-diagrammers\r\n","Successfully installed nb-js-diagrammers-0.0.7 pyflowchart-0.2.3\r\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install --upgrade nb-js-diagrammers\n","%load_ext nb_js_diagrammers"]},{"cell_type":"code","execution_count":2,"id":"8d919d3c","metadata":{"execution":{"iopub.execute_input":"2023-01-12T10:22:01.219822Z","iopub.status.busy":"2023-01-12T10:22:01.219397Z","iopub.status.idle":"2023-01-12T10:22:01.232019Z","shell.execute_reply":"2023-01-12T10:22:01.231007Z"},"papermill":{"duration":0.027752,"end_time":"2023-01-12T10:22:01.234193","exception":false,"start_time":"2023-01-12T10:22:01.206441","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<iframe srcdoc=\"&lt;html&gt;\n","    &lt;body&gt;\n","        &lt;script src=&quot;https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js&quot;&gt;&lt;/script&gt;\n","        &lt;script&gt;\n","            mermaid.initialize({ startOnLoad: true });\n","        &lt;/script&gt;\n"," \n","        &lt;div class=&quot;mermaid&quot;&gt;\n","            \n","flowchart TB\n","    subgraph DATASET\n","        tweets[Load Tweets.tsv]\n","        fetch[fetch Tweets using API]\n","        tweets --- |or| fetch\n","    end\n","    split_decision{Data Already Split \\nInto Test And Train?}\n","    subgraph SPLIT_DATA[Split Data]\n","        train_test_split[sklearn&#x27;s train_test_split]\n","    end\n","    subgraph NORMALIZE\n","        tokenize[Tokenize]\n","        subgraph EXTRACT_ROOT[EXTRACT ROOT]\n","            lemmetize[Lemmetize]\n","            stem[Stem]\n","            lemmetize --- |or| stem\n","        end\n","        subgraph PRE_PROCESS[PRE-PROCESS]\n","            remove_stop_words[Remove Stop Words]\n","            remove_emojis[Remove Emojis]\n","            remove_ponctuation[Remove Ponctuation]\n","            remove_special_characters[Remove Special Characters]   \n","        end\n","        remove_stop_words --- |and/or| remove_emojis\n","        remove_emojis --- |and/or| remove_ponctuation\n","        remove_ponctuation --- |and/or| remove_special_characters\n","        \n","        tokenize --&gt; PRE_PROCESS\n","        PRE_PROCESS --&gt; EXTRACT_ROOT\n","    end\n","        \n","    subgraph VECTORIZE\n","        CountVectorizer --- |or| TfidfTransformer\n","    end\n","    \n","    subgraph TRAIN_CLASSIFIER[TRAIN A CLASSIFIER]\n","        MultinomialNB[Sklearn&#x27;s MultinomialNB naive bayes] \n","        SGDClassifier[Sklearn&#x27;s SGDClassifier]\n","        MultinomialNB --- |or| SGDClassifier\n","    end\n","                      \n","    subgraph PREDICT\n","        \n","    end\n","    \n","    subgraph ANALYSE\n","                      \n","    end\n","    \n","    DATASET --&gt; split_decision\n","    split_decision --&gt; |yes| NORMALIZE\n","    split_decision --&gt; |no| SPLIT_DATA\n","    SPLIT_DATA --&gt; NORMALIZE\n","    NORMALIZE --&gt; VECTORIZE\n","    VECTORIZE --&gt; TRAIN_CLASSIFIER;\n","    TRAIN_CLASSIFIER --&gt; PREDICT;\n","    PREDICT --&gt; ANALYSE\n","\n","        &lt;/div&gt;\n"," \n","    &lt;/body&gt;\n","&lt;/html&gt;\n","\" width=\"100%\" height=\"1750\"style=\"border:none !important;\" \"allowfullscreen\" \"webkitallowfullscreen\" \"mozallowfullscreen\"></iframe>"],"text/plain":["<nb_js_diagrammers.magics.JSDiagram at 0x7fa21463d990>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["%%mermaid_magic -h 1750\n","\n","flowchart TB\n","    subgraph DATASET\n","        tweets[Load Tweets.tsv]\n","        fetch[fetch Tweets using API]\n","        tweets --- |or| fetch\n","    end\n","    split_decision{Data Already Split \\nInto Test And Train?}\n","    subgraph SPLIT_DATA[Split Data]\n","        train_test_split[sklearn's train_test_split]\n","    end\n","    subgraph NORMALIZE\n","        tokenize[Tokenize]\n","        subgraph EXTRACT_ROOT[EXTRACT ROOT]\n","            lemmetize[Lemmetize]\n","            stem[Stem]\n","            lemmetize --- |or| stem\n","        end\n","        subgraph PRE_PROCESS[PRE-PROCESS]\n","            remove_stop_words[Remove Stop Words]\n","            remove_emojis[Remove Emojis]\n","            remove_ponctuation[Remove Ponctuation]\n","            remove_special_characters[Remove Special Characters]   \n","        end\n","        remove_stop_words --- |and/or| remove_emojis\n","        remove_emojis --- |and/or| remove_ponctuation\n","        remove_ponctuation --- |and/or| remove_special_characters\n","        \n","        tokenize --> PRE_PROCESS\n","        PRE_PROCESS --> EXTRACT_ROOT\n","    end\n","        \n","    subgraph VECTORIZE\n","        CountVectorizer --- |or| TfidfTransformer\n","    end\n","    \n","    subgraph TRAIN_CLASSIFIER[TRAIN A CLASSIFIER]\n","        MultinomialNB[Sklearn's MultinomialNB naive bayes] \n","        SGDClassifier[Sklearn's SGDClassifier]\n","        MultinomialNB --- |or| SGDClassifier\n","    end\n","                      \n","    subgraph PREDICT\n","        \n","    end\n","    \n","    subgraph ANALYSE\n","                      \n","    end\n","    \n","    DATASET --> split_decision\n","    split_decision --> |yes| NORMALIZE\n","    split_decision --> |no| SPLIT_DATA\n","    SPLIT_DATA --> NORMALIZE\n","    NORMALIZE --> VECTORIZE\n","    VECTORIZE --> TRAIN_CLASSIFIER;\n","    TRAIN_CLASSIFIER --> PREDICT;\n","    PREDICT --> ANALYSE\n"]},{"cell_type":"code","execution_count":3,"id":"42f45ca0","metadata":{"_cell_guid":"4c1ef890-63d4-444a-b58a-f41939d9e62f","_uuid":"87d30fb1-988f-4483-b632-02009c36d444","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T10:22:01.259226Z","iopub.status.busy":"2023-01-12T10:22:01.258806Z","iopub.status.idle":"2023-01-12T10:22:01.264853Z","shell.execute_reply":"2023-01-12T10:22:01.263618Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.021495,"end_time":"2023-01-12T10:22:01.267311","exception":false,"start_time":"2023-01-12T10:22:01.245816","status":"completed"},"tags":[]},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","id":"f8d6cce3","metadata":{"_cell_guid":"0380ce88-a279-42cb-b40b-c7476b152bd0","_uuid":"ec3e36fc-4064-4b35-951d-0b595aa23b14","papermill":{"duration":0.011171,"end_time":"2023-01-12T10:22:01.290412","exception":false,"start_time":"2023-01-12T10:22:01.279241","status":"completed"},"tags":[]},"source":["### constants"]},{"cell_type":"code","execution_count":4,"id":"4e1fa4bf","metadata":{"_cell_guid":"2bd62210-dad9-4e66-b738-4660c96b0fc9","_uuid":"7bbff164-5a1e-4f63-bd19-aa61814a168e","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T10:22:01.315286Z","iopub.status.busy":"2023-01-12T10:22:01.314895Z","iopub.status.idle":"2023-01-12T10:22:01.319876Z","shell.execute_reply":"2023-01-12T10:22:01.318795Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.020215,"end_time":"2023-01-12T10:22:01.322255","exception":false,"start_time":"2023-01-12T10:22:01.30204","status":"completed"},"tags":[]},"outputs":[],"source":["enc = \"UTF-8\""]},{"cell_type":"markdown","id":"163c5ae5","metadata":{"_cell_guid":"6ec183c9-8c80-45e5-9e8f-9c9681f60b13","_uuid":"d9cc1119-139a-4382-8c4e-8ac304c69b41","papermill":{"duration":0.011312,"end_time":"2023-01-12T10:22:01.345617","exception":false,"start_time":"2023-01-12T10:22:01.334305","status":"completed"},"tags":[]},"source":["### model files"]},{"cell_type":"code","execution_count":5,"id":"d0312a8b","metadata":{"_cell_guid":"9e978d47-5887-4d41-91fb-63ef6ffd63b0","_uuid":"45b66073-ec3a-4a74-9cf1-441edc87b64c","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T10:22:01.371223Z","iopub.status.busy":"2023-01-12T10:22:01.370815Z","iopub.status.idle":"2023-01-12T10:22:01.378618Z","shell.execute_reply":"2023-01-12T10:22:01.377476Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.023004,"end_time":"2023-01-12T10:22:01.380765","exception":false,"start_time":"2023-01-12T10:22:01.357761","status":"completed"},"tags":[]},"outputs":[],"source":["import enum\n","\n","class Tag(enum.Enum):\n","    Neutral = \"NEUTRAL\"\n","    Positive = \"POSITIVE\"\n","    Negative = \"NEGATIVE\"\n","    Not_defined = \"NOT_DEFINED\"\n","\n","class Tweet:\n","    def __init__(self, text, real_tag=Tag.Not_defined, given_tag=Tag.Not_defined):\n","        self.text = text\n","        self.real_tag = real_tag\n","        self.given_tag = given_tag\n","\n","    def __str__(self):\n","        txt = self.text\n","        return txt\n","\n","    def __repr__(self):\n","        txt = self.text \n","        return txt"]},{"cell_type":"markdown","id":"1dd9d670","metadata":{"_cell_guid":"ca0e9aec-e672-4faa-9b5f-8d43ee952691","_uuid":"c2c6de7b-7687-4684-8b7e-a8083b91d10b","papermill":{"duration":0.011327,"end_time":"2023-01-12T10:22:01.403625","exception":false,"start_time":"2023-01-12T10:22:01.392298","status":"completed"},"tags":[]},"source":["## 1. Loading DATASET\n","### 1.1. Using our own method and model files\n","    In this section we will use our own definition of a function that will load the files by traversing the folders containing the positive and the negative tweets\n","    We should compare this method to simply reading the `.tsv` files (which should be faster)."]},{"cell_type":"code","execution_count":6,"id":"87603120","metadata":{"_cell_guid":"ca64d88f-05f5-4080-a33f-5fbf92fc36e5","_uuid":"9b2d8e93-cc11-4c21-864b-f0f2a05339fa","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T10:22:01.429504Z","iopub.status.busy":"2023-01-12T10:22:01.429056Z","iopub.status.idle":"2023-01-12T10:22:01.434888Z","shell.execute_reply":"2023-01-12T10:22:01.433764Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.02211,"end_time":"2023-01-12T10:22:01.437343","exception":false,"start_time":"2023-01-12T10:22:01.415233","status":"completed"},"tags":[]},"outputs":[],"source":["import os\n","import pathlib\n","\n","# # current_path = pathlib.Path(__file__).parent.resolve()\n","# POS_COUNT = 29848\n","# NEG_COUNT = 28901\n","# tweets = []\n","\n","# def import_data():\n","#     for i in range(POS_COUNT):\n","#         text = \"\"\n","#         with open(\"/kaggle/input/arabic-sentiment-twitter-corpus/arabic_tweets/pos/\" + str(i) + '.txt', encoding=enc) as f:\n","#             for line in f:\n","#                 text += line\n","#         tweets.append(Tweet(text, Tag.Positive))\n","#     for i in range(NEG_COUNT):\n","#         text = \"\"\n","#         with open(\"/kaggle/input/arabic-sentiment-twitter-corpus/arabic_tweets/neg/\" + str(i) + '.txt', encoding=enc) as f:\n","#             for line in f:\n","#                 text += line\n","#         tweets.append(Tweet(text, Tag.Negative))\n","        \n","# import_data()"]},{"cell_type":"code","execution_count":7,"id":"4110fbf5","metadata":{"_cell_guid":"5a3260da-95d6-46c9-b0a8-228f471bca48","_uuid":"002d715e-41d8-4714-8737-a9909c57f4db","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T10:22:01.462267Z","iopub.status.busy":"2023-01-12T10:22:01.461862Z","iopub.status.idle":"2023-01-12T10:22:01.466361Z","shell.execute_reply":"2023-01-12T10:22:01.465322Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.019681,"end_time":"2023-01-12T10:22:01.468645","exception":false,"start_time":"2023-01-12T10:22:01.448964","status":"completed"},"tags":[]},"outputs":[],"source":["# print(tweets)"]},{"cell_type":"markdown","id":"b423e819","metadata":{"_cell_guid":"5fab9c36-56cb-4c8f-8db0-e91c821be46d","_uuid":"b3302efc-f1e9-4003-b14b-7efe6dff8091","papermill":{"duration":0.011077,"end_time":"2023-01-12T10:22:01.491263","exception":false,"start_time":"2023-01-12T10:22:01.480186","status":"completed"},"tags":[]},"source":["### 1.2. Using `Pandas` to read from `tsv` files directly\n","    same as the previous step we shall populate a list of tweets[Tweet] with our data.\n","    After deep thought it is better to use a matrix instead of classes. We shall use pandas to represent our dataset."]},{"cell_type":"code","execution_count":8,"id":"5d77e600","metadata":{"_cell_guid":"f4d75feb-2e8a-4357-b398-b76b762d188a","_uuid":"d1e8751f-ad65-4fd7-8025-9cc982ccd09b","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T10:22:01.516322Z","iopub.status.busy":"2023-01-12T10:22:01.515921Z","iopub.status.idle":"2023-01-12T10:22:01.868197Z","shell.execute_reply":"2023-01-12T10:22:01.866986Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.367632,"end_time":"2023-01-12T10:22:01.8707","exception":false,"start_time":"2023-01-12T10:22:01.503068","status":"completed"},"tags":[]},"outputs":[],"source":["column_names = [\"sentiment\", \"content\"]\n","train_tweets_positive = pd.read_table(\"/kaggle/input/arabic-sentiment-twitter-corpus/train_Arabic_tweets_positive_20190413.tsv\", names=column_names)\n","train_tweets_negative = pd.read_table(\"/kaggle/input/arabic-sentiment-twitter-corpus/train_Arabic_tweets_negative_20190413.tsv\", names=column_names)\n","test_tweets_positive = pd.read_table(\"/kaggle/input/arabic-sentiment-twitter-corpus/test_Arabic_tweets_positive_20190413.tsv\", names=column_names)\n","test_tweets_negative = pd.read_table(\"/kaggle/input/arabic-sentiment-twitter-corpus/test_Arabic_tweets_negative_20190413.tsv\", names=column_names)"]},{"cell_type":"code","execution_count":9,"id":"d8a18abb","metadata":{"_cell_guid":"4cb90bf1-f45d-4c2d-8e22-19dda25c3ef2","_uuid":"49a5e898-0cc0-40e7-aa9f-ca9624957948","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T10:22:01.89593Z","iopub.status.busy":"2023-01-12T10:22:01.895209Z","iopub.status.idle":"2023-01-12T10:22:01.919944Z","shell.execute_reply":"2023-01-12T10:22:01.918645Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.039886,"end_time":"2023-01-12T10:22:01.922281","exception":false,"start_time":"2023-01-12T10:22:01.882395","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentiment</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>pos</td>\n","      <td>نحن الذين يتحول كل ما نود أن نقوله إلى دعاء لل...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>pos</td>\n","      <td>وفي النهاية لن يبقىٰ معك آحدإلا من رأىٰ الجمال...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>pos</td>\n","      <td>من الخير نفسه 💛</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>pos</td>\n","      <td>#زلزل_الملعب_نصرنا_بيلعب كن عالي الهمه ولا ترض...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>pos</td>\n","      <td>الشيء الوحيد الذي وصلوا فيه للعالمية هو : المس...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>22509</th>\n","      <td>neg</td>\n","      <td>كيف ترى أورانوس لو كان يقع مكان القمر ؟ 💙💙 كوك...</td>\n","    </tr>\n","    <tr>\n","      <th>22510</th>\n","      <td>neg</td>\n","      <td>احسدك على الايم 💔</td>\n","    </tr>\n","    <tr>\n","      <th>22511</th>\n","      <td>neg</td>\n","      <td>لأول مرة ما بنكون سوا 💔</td>\n","    </tr>\n","    <tr>\n","      <th>22512</th>\n","      <td>neg</td>\n","      <td>بقله ليش يا واطي 🤔</td>\n","    </tr>\n","    <tr>\n","      <th>22513</th>\n","      <td>neg</td>\n","      <td>قد طال صبري في النوى إذ تركتني كئيبا ؛ غريبا ب...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>45275 rows × 2 columns</p>\n","</div>"],"text/plain":["      sentiment                                            content\n","0           pos  نحن الذين يتحول كل ما نود أن نقوله إلى دعاء لل...\n","1           pos  وفي النهاية لن يبقىٰ معك آحدإلا من رأىٰ الجمال...\n","2           pos                                    من الخير نفسه 💛\n","3           pos  #زلزل_الملعب_نصرنا_بيلعب كن عالي الهمه ولا ترض...\n","4           pos  الشيء الوحيد الذي وصلوا فيه للعالمية هو : المس...\n","...         ...                                                ...\n","22509       neg  كيف ترى أورانوس لو كان يقع مكان القمر ؟ 💙💙 كوك...\n","22510       neg                                  احسدك على الايم 💔\n","22511       neg                            لأول مرة ما بنكون سوا 💔\n","22512       neg                                 بقله ليش يا واطي 🤔\n","22513       neg  قد طال صبري في النوى إذ تركتني كئيبا ؛ غريبا ب...\n","\n","[45275 rows x 2 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["X_train = pd.concat([train_tweets_positive,train_tweets_negative])\n","\n","from IPython.display import display, HTML\n","display(X_train)"]},{"cell_type":"markdown","id":"3b50a579","metadata":{"papermill":{"duration":0.011621,"end_time":"2023-01-12T10:22:01.94606","exception":false,"start_time":"2023-01-12T10:22:01.934439","status":"completed"},"tags":[]},"source":["# 2. Normalize\n","## 1.2 Preprocessing dataset\n","Now we need to remove special characters including emoticones since apparently there are many emoticones in arabic tweets, we also need to remove punctuation and `tashkil` which is special character above letters to determine how they are pronounced\n","\n","So all that needs to be removed is:\n","* special characters and emoticones\n","* stop words\n","* ponctuation"]},{"cell_type":"code","execution_count":10,"id":"056bfbe3","metadata":{"execution":{"iopub.execute_input":"2023-01-12T10:22:01.972237Z","iopub.status.busy":"2023-01-12T10:22:01.971805Z","iopub.status.idle":"2023-01-12T10:22:03.949466Z","shell.execute_reply":"2023-01-12T10:22:03.947925Z"},"papermill":{"duration":1.994751,"end_time":"2023-01-12T10:22:03.952975","exception":false,"start_time":"2023-01-12T10:22:01.958224","status":"completed"},"tags":[]},"outputs":[],"source":["from nltk.corpus import stopwords\n","\n","def remove_stop_words(text):\n","    stop_words = set(stopwords.words(\"arabic\"))\n","    text = text.split()\n","    return \" \".join([word for word in text if word not in stop_words])\n","\n","import re\n","\n","def remove_emojis(text):\n","    emoji_pattern = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                           \"]+\", flags=re.UNICODE)\n","    return emoji_pattern.sub(r'', text)\n","\n","def remove_punctuation(text):\n","    return re.sub(r'[^\\w\\s]','',text)"]},{"cell_type":"code","execution_count":11,"id":"ecc602ec","metadata":{"execution":{"iopub.execute_input":"2023-01-12T10:22:03.980601Z","iopub.status.busy":"2023-01-12T10:22:03.980126Z","iopub.status.idle":"2023-01-12T10:22:24.944227Z","shell.execute_reply":"2023-01-12T10:22:24.942785Z"},"papermill":{"duration":20.981222,"end_time":"2023-01-12T10:22:24.946856","exception":false,"start_time":"2023-01-12T10:22:03.965634","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentiment</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>pos</td>\n","      <td>يتحول نود نقوله دعاء لله تبحثوا فينا قوة إننا ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>pos</td>\n","      <td>وفي النهاية يبقى معك آحدإلا رأى الجمال روحك أم...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>pos</td>\n","      <td>الخير نفسه</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>pos</td>\n","      <td>زلزل_الملعب_نصرنا_بيلعب عالي الهمه ترضى بغير ا...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>pos</td>\n","      <td>الشيء الوحيد وصلوا للعالمية  المسيار   ترى كان...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>22509</th>\n","      <td>neg</td>\n","      <td>ترى أورانوس يقع مكان القمر   كوكب الشمس يبلغ ق...</td>\n","    </tr>\n","    <tr>\n","      <th>22510</th>\n","      <td>neg</td>\n","      <td>احسدك الايم</td>\n","    </tr>\n","    <tr>\n","      <th>22511</th>\n","      <td>neg</td>\n","      <td>لأول مرة بنكون سوا</td>\n","    </tr>\n","    <tr>\n","      <th>22512</th>\n","      <td>neg</td>\n","      <td>بقله ليش واطي</td>\n","    </tr>\n","    <tr>\n","      <th>22513</th>\n","      <td>neg</td>\n","      <td>طال صبري النوى تركتني كئيبا  غريبا باكيا متوجع...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>45275 rows × 2 columns</p>\n","</div>"],"text/plain":["      sentiment                                            content\n","0           pos  يتحول نود نقوله دعاء لله تبحثوا فينا قوة إننا ...\n","1           pos  وفي النهاية يبقى معك آحدإلا رأى الجمال روحك أم...\n","2           pos                                        الخير نفسه \n","3           pos  زلزل_الملعب_نصرنا_بيلعب عالي الهمه ترضى بغير ا...\n","4           pos  الشيء الوحيد وصلوا للعالمية  المسيار   ترى كان...\n","...         ...                                                ...\n","22509       neg  ترى أورانوس يقع مكان القمر   كوكب الشمس يبلغ ق...\n","22510       neg                                       احسدك الايم \n","22511       neg                                لأول مرة بنكون سوا \n","22512       neg                                     بقله ليش واطي \n","22513       neg  طال صبري النوى تركتني كئيبا  غريبا باكيا متوجع...\n","\n","[45275 rows x 2 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["X_train.content = X_train.content.apply(remove_stop_words)\n","X_train.content = X_train.content.apply(remove_emojis)\n","X_train.content = X_train.content.apply(remove_punctuation)\n","display(X_train)"]},{"cell_type":"markdown","id":"4207b605","metadata":{"_cell_guid":"bc011b93-5ee3-47b1-b3e9-f66910d47ee1","_uuid":"f34fe2b0-69d1-4fc4-9154-2b23a28bfb66","papermill":{"duration":0.011925,"end_time":"2023-01-12T10:22:24.971338","exception":false,"start_time":"2023-01-12T10:22:24.959413","status":"completed"},"tags":[]},"source":["## Preprocessing dataset\n","Now we need to remove special characters## Tokenizing dataset\n","We need to write our tweets text as a feature-term dataframe (feature-term matrix). When using countvectorizer there is no need to preprocess the data, as it already removes stop words and speical characters.\n","\n","### Using CountVectorizer"]},{"cell_type":"code","execution_count":12,"id":"8568c660","metadata":{"_cell_guid":"b5262f3a-9aa2-4b93-a6e7-8949ab7a9221","_uuid":"e3006a58-5728-4895-8ae6-36578619940b","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T10:22:24.998712Z","iopub.status.busy":"2023-01-12T10:22:24.998205Z","iopub.status.idle":"2023-01-12T10:22:25.946797Z","shell.execute_reply":"2023-01-12T10:22:25.945471Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.965811,"end_time":"2023-01-12T10:22:25.949454","exception":false,"start_time":"2023-01-12T10:22:24.983643","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(45275, 71031)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","count_vect = CountVectorizer()\n","X_train_counts = count_vect.fit_transform(X_train.content)\n","X_train_counts.shape"]},{"cell_type":"markdown","id":"891763a6","metadata":{"_cell_guid":"ca352397-dcdb-43c2-8e25-76fdd9f02ac2","_uuid":"3db34197-2475-4e45-a761-8e41e73a094b","papermill":{"duration":0.012043,"end_time":"2023-01-12T10:22:25.974134","exception":false,"start_time":"2023-01-12T10:22:25.962091","status":"completed"},"tags":[]},"source":["### Using TfidfTransformer"]},{"cell_type":"code","execution_count":13,"id":"1cb41cba","metadata":{"_cell_guid":"c12ed70f-ac79-4671-b21f-3c1259e30b0e","_uuid":"c70c0eda-68b1-4189-93d6-2030ab82e9e8","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T10:22:26.002001Z","iopub.status.busy":"2023-01-12T10:22:26.000547Z","iopub.status.idle":"2023-01-12T10:22:26.067757Z","shell.execute_reply":"2023-01-12T10:22:26.066886Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.083356,"end_time":"2023-01-12T10:22:26.069887","exception":false,"start_time":"2023-01-12T10:22:25.986531","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(45275, 71031)"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import TfidfTransformer\n","tfidf_transformer = TfidfTransformer()\n","X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n","X_train_tfidf.shape"]},{"cell_type":"markdown","id":"41657d26","metadata":{"_cell_guid":"e4fb18f7-951e-4373-ae73-345b9a60cf7f","_uuid":"15a5ad41-c2f5-4594-8384-0eb6d899cb5a","papermill":{"duration":0.012045,"end_time":"2023-01-12T10:22:26.094349","exception":false,"start_time":"2023-01-12T10:22:26.082304","status":"completed"},"tags":[]},"source":["## Training a classifier\n","### Using naïve Bayes"]},{"cell_type":"code","execution_count":14,"id":"b1047cba","metadata":{"_cell_guid":"645996d7-320d-4b51-b534-b142fc545514","_uuid":"4dd3088e-6f46-44eb-a7b7-d875a8af6e7f","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T10:22:26.122119Z","iopub.status.busy":"2023-01-12T10:22:26.12131Z","iopub.status.idle":"2023-01-12T10:22:26.259661Z","shell.execute_reply":"2023-01-12T10:22:26.258647Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.15552,"end_time":"2023-01-12T10:22:26.262328","exception":false,"start_time":"2023-01-12T10:22:26.106808","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.naive_bayes import MultinomialNB\n","clf = MultinomialNB().fit(X_train_tfidf, X_train.sentiment)"]},{"cell_type":"markdown","id":"8350d45d","metadata":{"_cell_guid":"6dc0f87e-66d4-4973-9f11-768bdaf82254","_uuid":"dde58efe-9130-4465-bfe6-d09719bc25ef","papermill":{"duration":0.012014,"end_time":"2023-01-12T10:22:26.286714","exception":false,"start_time":"2023-01-12T10:22:26.2747","status":"completed"},"tags":[]},"source":["We will now use our testing dataset (we combine the negative and positivie tweests into one pandas dataframe), then we will call transfrom without calling fit in order to make a prediction."]},{"cell_type":"code","execution_count":15,"id":"c4da77ad","metadata":{"_cell_guid":"2bc882d9-64f3-4084-839e-c15f1bf5f8f6","_uuid":"fe2ffab8-6e91-49ff-b6fd-0b558c7a3035","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T10:22:26.314377Z","iopub.status.busy":"2023-01-12T10:22:26.313526Z","iopub.status.idle":"2023-01-12T10:22:31.757577Z","shell.execute_reply":"2023-01-12T10:22:31.756306Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":5.461375,"end_time":"2023-01-12T10:22:31.760386","exception":false,"start_time":"2023-01-12T10:22:26.299011","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentiment</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>pos</td>\n","      <td>الهلال_الاهلي فوز هلالي مهم الحمد لله  زوران ب...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>pos</td>\n","      <td>صباحك خيرات ومسرات</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>pos</td>\n","      <td>تأمل قال الله ﷻ  _بواد زرع _    ومع هتف بالدعا...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>pos</td>\n","      <td>جدعان الرجاله اللي ال دول خطر تويتر وربنا مش ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>pos</td>\n","      <td>رساله صباحيه   اللهم اسألك التوفيق امورنا واكت...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5763</th>\n","      <td>neg</td>\n","      <td>النوم وانت مكسور ده احساس غبي اللي مش قادر تنا...</td>\n","    </tr>\n","    <tr>\n","      <th>5764</th>\n","      <td>neg</td>\n","      <td>استشهاد_الامام_كاظم_الغيظ السلام المعذب قعر ال...</td>\n","    </tr>\n","    <tr>\n","      <th>5765</th>\n","      <td>neg</td>\n","      <td>انا كنت اكل الصحن بكبره</td>\n","    </tr>\n","    <tr>\n","      <th>5766</th>\n","      <td>neg</td>\n","      <td>قولوا ايش تشوفوا  ملاحظة التلطف لأنه المود</td>\n","    </tr>\n","    <tr>\n","      <th>5767</th>\n","      <td>neg</td>\n","      <td>أردت تعرف شيئا عني إسالني تسأل غيري فعشاق الت...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>11520 rows × 2 columns</p>\n","</div>"],"text/plain":["     sentiment                                            content\n","0          pos  الهلال_الاهلي فوز هلالي مهم الحمد لله  زوران ب...\n","1          pos                                صباحك خيرات ومسرات \n","2          pos  تأمل قال الله ﷻ  _بواد زرع _    ومع هتف بالدعا...\n","3          pos   جدعان الرجاله اللي ال دول خطر تويتر وربنا مش ...\n","4          pos  رساله صباحيه   اللهم اسألك التوفيق امورنا واكت...\n","...        ...                                                ...\n","5763       neg  النوم وانت مكسور ده احساس غبي اللي مش قادر تنا...\n","5764       neg  استشهاد_الامام_كاظم_الغيظ السلام المعذب قعر ال...\n","5765       neg                           انا كنت اكل الصحن بكبره \n","5766       neg        قولوا ايش تشوفوا  ملاحظة التلطف لأنه المود \n","5767       neg   أردت تعرف شيئا عني إسالني تسأل غيري فعشاق الت...\n","\n","[11520 rows x 2 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["X_test = pd.concat([test_tweets_positive,test_tweets_negative])\n","\n","X_test.content = X_test.content.apply(remove_stop_words)\n","X_test.content = X_test.content.apply(remove_emojis)\n","X_test.content = X_test.content.apply(remove_punctuation)\n","display(X_test)\n","\n","X_test_counts = count_vect.transform(X_test.content)\n","X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n","\n","predicted = clf.predict(X_test_tfidf)\n","\n","# for tweet, sentiment in zip(X_test.content, predicted):\n","#     print('%r => %s' % (tweet, sentiment))"]},{"cell_type":"markdown","id":"0f93d19a","metadata":{"_cell_guid":"acb2606b-96b4-4d9f-9a71-cb155b57b87b","_uuid":"19c3b799-f686-4d8e-89a9-e1c5eb3b5c54","papermill":{"duration":0.012699,"end_time":"2023-01-12T10:22:31.786037","exception":false,"start_time":"2023-01-12T10:22:31.773338","status":"completed"},"tags":[]},"source":["## Calculating Accuracy Score\n","Now we need to get a percentage of the accuracy of our model, we have a list of predicted sentiment and a list of the actual sentiment. Whenever predicted sentiment is different of the actual one we will increment a counter, after going through all the tweets we will divide but the total number of tweets to get a percentage of the wrong predictions, to get the percentage of the right prediction all we need to do is subtract the calculated score from 100%."]},{"cell_type":"code","execution_count":16,"id":"b2240199","metadata":{"_cell_guid":"19c26993-64cb-49bb-b033-f7186a705740","_uuid":"afb6c115-b16d-4fb9-a060-ef68a1aa2bdf","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T10:22:31.81409Z","iopub.status.busy":"2023-01-12T10:22:31.813619Z","iopub.status.idle":"2023-01-12T10:22:31.836739Z","shell.execute_reply":"2023-01-12T10:22:31.835596Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.039773,"end_time":"2023-01-12T10:22:31.83924","exception":false,"start_time":"2023-01-12T10:22:31.799467","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["validity score: 78.11631944444444%\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentiment</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>pos</td>\n","      <td>الهلال_الاهلي فوز هلالي مهم الحمد لله  زوران ب...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>pos</td>\n","      <td>صباحك خيرات ومسرات</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>pos</td>\n","      <td>تأمل قال الله ﷻ  _بواد زرع _    ومع هتف بالدعا...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>pos</td>\n","      <td>جدعان الرجاله اللي ال دول خطر تويتر وربنا مش ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>pos</td>\n","      <td>رساله صباحيه   اللهم اسألك التوفيق امورنا واكت...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5763</th>\n","      <td>neg</td>\n","      <td>النوم وانت مكسور ده احساس غبي اللي مش قادر تنا...</td>\n","    </tr>\n","    <tr>\n","      <th>5764</th>\n","      <td>neg</td>\n","      <td>استشهاد_الامام_كاظم_الغيظ السلام المعذب قعر ال...</td>\n","    </tr>\n","    <tr>\n","      <th>5765</th>\n","      <td>neg</td>\n","      <td>انا كنت اكل الصحن بكبره</td>\n","    </tr>\n","    <tr>\n","      <th>5766</th>\n","      <td>neg</td>\n","      <td>قولوا ايش تشوفوا  ملاحظة التلطف لأنه المود</td>\n","    </tr>\n","    <tr>\n","      <th>5767</th>\n","      <td>neg</td>\n","      <td>أردت تعرف شيئا عني إسالني تسأل غيري فعشاق الت...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>11520 rows × 2 columns</p>\n","</div>"],"text/plain":["     sentiment                                            content\n","0          pos  الهلال_الاهلي فوز هلالي مهم الحمد لله  زوران ب...\n","1          pos                                صباحك خيرات ومسرات \n","2          pos  تأمل قال الله ﷻ  _بواد زرع _    ومع هتف بالدعا...\n","3          pos   جدعان الرجاله اللي ال دول خطر تويتر وربنا مش ...\n","4          pos  رساله صباحيه   اللهم اسألك التوفيق امورنا واكت...\n","...        ...                                                ...\n","5763       neg  النوم وانت مكسور ده احساس غبي اللي مش قادر تنا...\n","5764       neg  استشهاد_الامام_كاظم_الغيظ السلام المعذب قعر ال...\n","5765       neg                           انا كنت اكل الصحن بكبره \n","5766       neg        قولوا ايش تشوفوا  ملاحظة التلطف لأنه المود \n","5767       neg   أردت تعرف شيئا عني إسالني تسأل غيري فعشاق الت...\n","\n","[11520 rows x 2 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["wrong_predictions = 0\n","validity_score = 0\n","for predicted_sentiment, actual_sentiment in zip(predicted, X_test.sentiment):\n","    if predicted_sentiment != actual_sentiment:\n","        wrong_predictions += 1\n","wrong_predictions_percentage = wrong_predictions / len(X_test.sentiment)\n","validity_score = 1 - wrong_predictions_percentage\n","print(\"validity score: \" + str(validity_score*100) + \"%\")\n","display(X_test)"]},{"cell_type":"markdown","id":"82ede85a","metadata":{"_cell_guid":"6b21ef9a-866c-4b88-8577-164f8a1e29ec","_uuid":"48435d85-c47f-4438-b829-1aee03325841","papermill":{"duration":0.012853,"end_time":"2023-01-12T10:22:31.865287","exception":false,"start_time":"2023-01-12T10:22:31.852434","status":"completed"},"tags":[]},"source":["For the time being we have a validity score of `78.4375%` therefore our prediction model is considred bad, we think it is because we're studying text in `arabic` CountVectorizer is unable to correctly preprocess text and tokenize it. We will try to use another vectorizer to see if the validity increases.\n","\n","### Building a pipeline\n","To simplify our training and prediction process we will build a new Pipeline"]},{"cell_type":"code","execution_count":17,"id":"9097fe46","metadata":{"_cell_guid":"12907a38-023f-4149-81c8-a385f64f7186","_uuid":"6ccd3360-b20a-4c43-a609-ed1b718c5dd0","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T10:22:31.893412Z","iopub.status.busy":"2023-01-12T10:22:31.893001Z","iopub.status.idle":"2023-01-12T10:22:31.901365Z","shell.execute_reply":"2023-01-12T10:22:31.900173Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.025451,"end_time":"2023-01-12T10:22:31.903766","exception":false,"start_time":"2023-01-12T10:22:31.878315","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import SGDClassifier\n","\n","text_clf = Pipeline([\n","    ('vect', CountVectorizer()),\n","    ('tfidf', TfidfTransformer()),\n","    ('clf', SGDClassifier(loss='hinge', penalty=None,\n","                          alpha=1e-3, random_state=2,\n","                          max_iter=5, tol=None)),\n","])"]},{"cell_type":"markdown","id":"d6931283","metadata":{"_cell_guid":"2d6a1601-c2c5-46ab-93bf-4a1f7fcd57e6","_uuid":"0e26a857-855e-4042-bc5a-ff8758c44c09","papermill":{"duration":0.012515,"end_time":"2023-01-12T10:22:31.929336","exception":false,"start_time":"2023-01-12T10:22:31.916821","status":"completed"},"tags":[]},"source":["### Using SGDClassifierfrom sklearn.linear_model import SGDClassifier"]},{"cell_type":"code","execution_count":18,"id":"6bea6fbe","metadata":{"_cell_guid":"38158940-ab9a-4410-9808-30357a62a7fb","_uuid":"279b3351-878e-45ac-b87e-0799205f4640","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T10:22:31.956919Z","iopub.status.busy":"2023-01-12T10:22:31.956483Z","iopub.status.idle":"2023-01-12T10:22:33.166011Z","shell.execute_reply":"2023-01-12T10:22:33.164718Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":1.226311,"end_time":"2023-01-12T10:22:33.168539","exception":false,"start_time":"2023-01-12T10:22:31.942228","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["0.73515625"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["text_clf.fit(X_train.content, X_train.sentiment)\n","\n","predicted = text_clf.predict(X_test.content)\n","np.mean(predicted == X_test.sentiment)"]},{"cell_type":"markdown","id":"1e8891f8","metadata":{"_cell_guid":"68a204ce-0b94-43ec-8546-423aac5564e5","_uuid":"6ddfb7ef-00f4-4870-b61b-411604437e54","papermill":{"duration":0.013191,"end_time":"2023-01-12T10:22:33.195151","exception":false,"start_time":"2023-01-12T10:22:33.18196","status":"completed"},"tags":[]},"source":["Now we will try to `Lemmetize` tweets, we believe this is the reason why our prediction model isn't producing better results.\n","We could try `Farasa` lemmetizer as it has a good reputation of outperforming other lemmetizers. But it uses an API and is not available as an imported library usable directly.\n","So for now we will just stick with ntlk's `ISRIStemmer`"]},{"cell_type":"code","execution_count":19,"id":"25444cef","metadata":{"_cell_guid":"ea1968d1-15bf-4de2-8fe0-181a0e161618","_uuid":"e8ed1312-35a1-4711-a76f-01e1c5296650","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T10:22:33.223539Z","iopub.status.busy":"2023-01-12T10:22:33.223102Z","iopub.status.idle":"2023-01-12T10:22:33.228474Z","shell.execute_reply":"2023-01-12T10:22:33.227382Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.022164,"end_time":"2023-01-12T10:22:33.230561","exception":false,"start_time":"2023-01-12T10:22:33.208397","status":"completed"},"tags":[]},"outputs":[],"source":["from nltk.stem.isri import ISRIStemmer\n","st = ISRIStemmer()"]},{"cell_type":"markdown","id":"98d55c6f","metadata":{"_cell_guid":"9f278e4f-df5c-4a51-a45c-21447f11fdda","_uuid":"6f7b793d-49ca-48e1-9e0b-36a22d947f38","papermill":{"duration":0.013008,"end_time":"2023-01-12T10:22:33.256882","exception":false,"start_time":"2023-01-12T10:22:33.243874","status":"completed"},"tags":[]},"source":["A lemmetizer is also called a stemmer, NLTK has many stemmer solutions. We could try out each one of NLTK's stemmers and compare the results.\n","For now we will try out `ARLSTem Stemmer`\n","\n","* THE NEW PROCESS IS NOW:\n","1. tokenize\n","2. stem\n","3. remove stop words"]},{"cell_type":"code","execution_count":20,"id":"634ff8a0","metadata":{"_cell_guid":"a53fabfb-3f1e-4226-886c-d5889bb8af5e","_uuid":"ca44c7c4-b9d4-459c-a9f7-83ced75b3e21","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T10:22:33.285678Z","iopub.status.busy":"2023-01-12T10:22:33.284887Z","iopub.status.idle":"2023-01-12T10:22:33.50062Z","shell.execute_reply":"2023-01-12T10:22:33.499492Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.233031,"end_time":"2023-01-12T10:22:33.50309","exception":false,"start_time":"2023-01-12T10:22:33.270059","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["0       هلال_الاهلي فوز هلالي مهم الحمد لله  زوران بيس...\n","1                                     صباحك خيرات ومسرات \n","2       امل قال الله ﷻ  _بواد زرع _    ومع هتف بالدعاء...\n","3        جدعان الرجاله اللي ال دول خطر تويتر وربنا مش ...\n","4       رساله صباحيه   اللهم اسالك التوفيق امورنا واكت...\n","                              ...                        \n","5763    نوم وانت مكسور ده احساس غبي اللي مش قادر تنام ...\n","5764    ستشهاد_الامام_كاظم_الغيظ السلام المعذب قعر الس...\n","5765                              ان كنت اكل الصحن بكبره \n","5766          قولوا ايش تشوفوا  ملاحظة التلطف لانه المود \n","5767     اردت تعرف شيئا عني اسالني تسال غيري فعشاق الت...\n","Name: content, Length: 11520, dtype: object"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.stem.arlstem import ARLSTem\n","\n","stemmer = ARLSTem()\n","X_test.content.apply(stemmer.stem)"]},{"cell_type":"markdown","id":"fb7c5dc7","metadata":{"_cell_guid":"32fd7ae3-82bb-4873-8366-0fcd80965846","_uuid":"4415217a-9fe2-41e1-b88f-3dc74e334cf0","papermill":{"duration":0.012876,"end_time":"2023-01-12T10:22:33.529388","exception":false,"start_time":"2023-01-12T10:22:33.516512","status":"completed"},"tags":[]},"source":["# NLTK tokenization\n","    So we're still getting a very low validity score for our prediction model. It could be because we're using Arabic language and scikit learn is unable to correctly tokenize words, it could be that the words that have the same root arent' considred as the same token. I will try to preprocess the data first to turn each tweet text into tokens. \n","    A problem that could arise is not being able to detect the order of words. I'm not sure if NLtk will scramble the words or will they be in the same order for us to be able to use n-gram of words later on."]},{"cell_type":"code","execution_count":21,"id":"94e34aff","metadata":{"_cell_guid":"96aef751-938c-4c7d-a529-014ba033a901","_uuid":"4de24fee-81c6-40ea-941f-0b00169a6a6e","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T10:22:33.557789Z","iopub.status.busy":"2023-01-12T10:22:33.557368Z","iopub.status.idle":"2023-01-12T10:22:34.044773Z","shell.execute_reply":"2023-01-12T10:22:34.043326Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.504844,"end_time":"2023-01-12T10:22:34.047463","exception":false,"start_time":"2023-01-12T10:22:33.542619","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["0        [يتحول, نود, نقوله, دعاء, لله, تبحثوا, فينا, ق...\n","1        [وفي, النهاية, يبقى, معك, آحدإلا, رأى, الجمال,...\n","2                                            [الخير, نفسه]\n","3        [زلزل_الملعب_نصرنا_بيلعب, عالي, الهمه, ترضى, ب...\n","4        [الشيء, الوحيد, وصلوا, للعالمية, المسيار, ترى,...\n","                               ...                        \n","22509    [ترى, أورانوس, يقع, مكان, القمر, كوكب, الشمس, ...\n","22510                                       [احسدك, الايم]\n","22511                              [لأول, مرة, بنكون, سوا]\n","22512                                    [بقله, ليش, واطي]\n","22513    [طال, صبري, النوى, تركتني, كئيبا, غريبا, باكيا...\n","Name: content, Length: 45275, dtype: object"]},"metadata":{},"output_type":"display_data"}],"source":["from nltk.tokenize import wordpunct_tokenize,word_tokenize\n","\n","X_train = X_train.content.apply(wordpunct_tokenize)\n","display(X_train)"]},{"cell_type":"markdown","id":"666daea3","metadata":{"_cell_guid":"015c2d51-da19-4e95-a093-cd475b67125b","_uuid":"35301da8-3cd9-46cb-b859-52045c8d4cf0","papermill":{"duration":0.013104,"end_time":"2023-01-12T10:22:34.074303","exception":false,"start_time":"2023-01-12T10:22:34.061199","status":"completed"},"tags":[]},"source":["arabic isn't supported by `nltk` so we'll use some other third party library to tokenize our te"]},{"cell_type":"code","execution_count":22,"id":"56471d94","metadata":{"_cell_guid":"76f933a7-228b-4b26-bdc6-8cbdf1fbd3fd","_uuid":"5d06e677-34eb-4331-9fa0-1234bd40fc62","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T10:22:34.103693Z","iopub.status.busy":"2023-01-12T10:22:34.103197Z","iopub.status.idle":"2023-01-12T10:22:34.108377Z","shell.execute_reply":"2023-01-12T10:22:34.107153Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.022811,"end_time":"2023-01-12T10:22:34.110719","exception":false,"start_time":"2023-01-12T10:22:34.087908","status":"completed"},"tags":[]},"outputs":[],"source":["# import tkseem as tk\n","\n","# tkseem_tokenizer = tk.WordTokenizer()\n","# X_train.content.apply(tkseem_tokenizer.tokenize)"]},{"cell_type":"markdown","id":"d8d68d6c","metadata":{"_cell_guid":"f36c4f68-7a1b-483b-b4b3-45c7174ab3c2","_uuid":"20577384-4ae0-4530-9802-78167ed0ad6b","papermill":{"duration":0.013153,"end_time":"2023-01-12T10:22:34.137372","exception":false,"start_time":"2023-01-12T10:22:34.124219","status":"completed"},"tags":[]},"source":["### Parameter tuning using grid search"]},{"cell_type":"code","execution_count":23,"id":"74f52a38","metadata":{"_cell_guid":"94b5d231-80d4-4a38-8955-53d142cc0be7","_uuid":"de14a436-17c2-4e5a-9eec-705f85f65406","collapsed":false,"execution":{"iopub.execute_input":"2023-01-12T10:22:34.166401Z","iopub.status.busy":"2023-01-12T10:22:34.165933Z","iopub.status.idle":"2023-01-12T10:22:34.171251Z","shell.execute_reply":"2023-01-12T10:22:34.169798Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.022811,"end_time":"2023-01-12T10:22:34.17357","exception":false,"start_time":"2023-01-12T10:22:34.150759","status":"completed"},"tags":[]},"outputs":[],"source":["# from sklearn.model_selection import GridSearchCV\n","\n","# parameters = {\n","#     'vect__ngram_range': [(1, 1), (1, 2)],\n","#     'tfidf__use_idf': (True, False),\n","#     'clf__alpha': (1e-2, 1e-3),\n","# }\n","\n","# gs_clf = GridSearchCV(text_clf, parameters, cv=5, n_jobs=-1)\n","# gs_clf = gs_clf.fit(X_train.content[:400], X_train.sentiment[:400])"]},{"cell_type":"markdown","id":"14d12858","metadata":{"_cell_guid":"681e47fe-a024-4c79-b2ba-76c088564b66","_uuid":"29fb8111-45f0-480c-8a28-665dd790f1e3","papermill":{"duration":0.01329,"end_time":"2023-01-12T10:22:34.200522","exception":false,"start_time":"2023-01-12T10:22:34.187232","status":"completed"},"tags":[]},"source":["## Extracting features from tweets data\n","    After loading the data the next step is to try and extract features from our tweets corpus."]},{"cell_type":"markdown","id":"faa38a1b","metadata":{"papermill":{"duration":0.013267,"end_time":"2023-01-12T10:22:34.227715","exception":false,"start_time":"2023-01-12T10:22:34.214448","status":"completed"},"tags":[]},"source":["# testing with nltk's own sentimentAnalyzer\n","Now we'll try to compare our model to NLTK's `sentimentIntensityAnalyzer` to see which one provides better results"]},{"cell_type":"code","execution_count":24,"id":"60b7bbb2","metadata":{"execution":{"iopub.execute_input":"2023-01-12T10:22:34.258269Z","iopub.status.busy":"2023-01-12T10:22:34.256693Z","iopub.status.idle":"2023-01-12T10:22:34.284711Z","shell.execute_reply":"2023-01-12T10:22:34.283745Z"},"papermill":{"duration":0.046049,"end_time":"2023-01-12T10:22:34.287349","exception":false,"start_time":"2023-01-12T10:22:34.2413","status":"completed"},"tags":[]},"outputs":[],"source":["from nltk.sentiment import SentimentIntensityAnalyzer\n","sia = SentimentIntensityAnalyzer()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":59.419792,"end_time":"2023-01-12T10:22:35.426384","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-01-12T10:21:36.006592","version":"2.3.4"}},"nbformat":4,"nbformat_minor":5}