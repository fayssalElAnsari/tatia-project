{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/fayssalelansari/notebookc111a6c6b8?scriptVersionId=115513992\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","execution_count":1,"id":"d08f8f80","metadata":{"_cell_guid":"4c1ef890-63d4-444a-b58a-f41939d9e62f","_uuid":"87d30fb1-988f-4483-b632-02009c36d444","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:34.729193Z","iopub.status.busy":"2023-01-04T21:52:34.728733Z","iopub.status.idle":"2023-01-04T21:52:34.738559Z","shell.execute_reply":"2023-01-04T21:52:34.737858Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.021973,"end_time":"2023-01-04T21:52:34.740543","exception":false,"start_time":"2023-01-04T21:52:34.71857","status":"completed"},"tags":[]},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","id":"aa8a5216","metadata":{"papermill":{"duration":0.007515,"end_time":"2023-01-04T21:52:34.756337","exception":false,"start_time":"2023-01-04T21:52:34.748822","status":"completed"},"tags":[]},"source":["# TASKS\n","* [ ] PREPROCESS DATA\n","* [ ] STEM ARABIC WORDS\n","* [ ] MAKE PIPELINE"]},{"cell_type":"markdown","id":"8d3922b3","metadata":{"_cell_guid":"0380ce88-a279-42cb-b40b-c7476b152bd0","_uuid":"ec3e36fc-4064-4b35-951d-0b595aa23b14","papermill":{"duration":0.007495,"end_time":"2023-01-04T21:52:34.771579","exception":false,"start_time":"2023-01-04T21:52:34.764084","status":"completed"},"tags":[]},"source":["### constants"]},{"cell_type":"code","execution_count":2,"id":"df4cff84","metadata":{"_cell_guid":"2bd62210-dad9-4e66-b738-4660c96b0fc9","_uuid":"7bbff164-5a1e-4f63-bd19-aa61814a168e","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:34.788503Z","iopub.status.busy":"2023-01-04T21:52:34.788164Z","iopub.status.idle":"2023-01-04T21:52:34.792739Z","shell.execute_reply":"2023-01-04T21:52:34.791669Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.015206,"end_time":"2023-01-04T21:52:34.794579","exception":false,"start_time":"2023-01-04T21:52:34.779373","status":"completed"},"tags":[]},"outputs":[],"source":["enc = \"UTF-8\""]},{"cell_type":"markdown","id":"cd63bc45","metadata":{"_cell_guid":"6ec183c9-8c80-45e5-9e8f-9c9681f60b13","_uuid":"d9cc1119-139a-4382-8c4e-8ac304c69b41","papermill":{"duration":0.007422,"end_time":"2023-01-04T21:52:34.811102","exception":false,"start_time":"2023-01-04T21:52:34.80368","status":"completed"},"tags":[]},"source":["### model files"]},{"cell_type":"code","execution_count":3,"id":"4d69b5fc","metadata":{"_cell_guid":"9e978d47-5887-4d41-91fb-63ef6ffd63b0","_uuid":"45b66073-ec3a-4a74-9cf1-441edc87b64c","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:34.828138Z","iopub.status.busy":"2023-01-04T21:52:34.827633Z","iopub.status.idle":"2023-01-04T21:52:34.837861Z","shell.execute_reply":"2023-01-04T21:52:34.836782Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.021067,"end_time":"2023-01-04T21:52:34.839884","exception":false,"start_time":"2023-01-04T21:52:34.818817","status":"completed"},"tags":[]},"outputs":[],"source":["import enum\n","\n","class Tag(enum.Enum):\n","    Neutral = \"NEUTRAL\"\n","    Positive = \"POSITIVE\"\n","    Negative = \"NEGATIVE\"\n","    Not_defined = \"NOT_DEFINED\"\n","\n","class Tweet:\n","    def __init__(self, text, real_tag=Tag.Not_defined, given_tag=Tag.Not_defined):\n","        self.text = text\n","        self.real_tag = real_tag\n","        self.given_tag = given_tag\n","\n","    def __str__(self):\n","        txt = self.text\n","        return txt\n","\n","    def __repr__(self):\n","        txt = self.text \n","        return txt"]},{"cell_type":"markdown","id":"59fc45b5","metadata":{"_cell_guid":"ca0e9aec-e672-4faa-9b5f-8d43ee952691","_uuid":"c2c6de7b-7687-4684-8b7e-a8083b91d10b","papermill":{"duration":0.007646,"end_time":"2023-01-04T21:52:34.855531","exception":false,"start_time":"2023-01-04T21:52:34.847885","status":"completed"},"tags":[]},"source":["## Loading data\n","### Using our own method and model files\n","    In this section we will use our own definition of a function that will load the files by traversing the folders containing the positive and the negative tweets\n","    We should compare this method to simply reading the `.tsv` files (which should be faster)."]},{"cell_type":"code","execution_count":4,"id":"a4f51a90","metadata":{"_cell_guid":"ca64d88f-05f5-4080-a33f-5fbf92fc36e5","_uuid":"9b2d8e93-cc11-4c21-864b-f0f2a05339fa","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:34.872889Z","iopub.status.busy":"2023-01-04T21:52:34.872556Z","iopub.status.idle":"2023-01-04T21:52:34.877416Z","shell.execute_reply":"2023-01-04T21:52:34.876466Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.01577,"end_time":"2023-01-04T21:52:34.879197","exception":false,"start_time":"2023-01-04T21:52:34.863427","status":"completed"},"tags":[]},"outputs":[],"source":["import os\n","import pathlib\n","\n","# # current_path = pathlib.Path(__file__).parent.resolve()\n","# POS_COUNT = 29848\n","# NEG_COUNT = 28901\n","# tweets = []\n","\n","# def import_data():\n","#     for i in range(POS_COUNT):\n","#         text = \"\"\n","#         with open(\"/kaggle/input/arabic-sentiment-twitter-corpus/arabic_tweets/pos/\" + str(i) + '.txt', encoding=enc) as f:\n","#             for line in f:\n","#                 text += line\n","#         tweets.append(Tweet(text, Tag.Positive))\n","#     for i in range(NEG_COUNT):\n","#         text = \"\"\n","#         with open(\"/kaggle/input/arabic-sentiment-twitter-corpus/arabic_tweets/neg/\" + str(i) + '.txt', encoding=enc) as f:\n","#             for line in f:\n","#                 text += line\n","#         tweets.append(Tweet(text, Tag.Negative))\n","        \n","# import_data()"]},{"cell_type":"code","execution_count":5,"id":"85c90e0d","metadata":{"_cell_guid":"5a3260da-95d6-46c9-b0a8-228f471bca48","_uuid":"002d715e-41d8-4714-8737-a9909c57f4db","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:34.896979Z","iopub.status.busy":"2023-01-04T21:52:34.895953Z","iopub.status.idle":"2023-01-04T21:52:34.900347Z","shell.execute_reply":"2023-01-04T21:52:34.899535Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.015339,"end_time":"2023-01-04T21:52:34.902509","exception":false,"start_time":"2023-01-04T21:52:34.88717","status":"completed"},"tags":[]},"outputs":[],"source":["# print(tweets)"]},{"cell_type":"markdown","id":"3cfce95f","metadata":{"_cell_guid":"5fab9c36-56cb-4c8f-8db0-e91c821be46d","_uuid":"b3302efc-f1e9-4003-b14b-7efe6dff8091","papermill":{"duration":0.007576,"end_time":"2023-01-04T21:52:34.918573","exception":false,"start_time":"2023-01-04T21:52:34.910997","status":"completed"},"tags":[]},"source":["### Using our own method and reading from `tsv` files directly\n","    same as the previous step we shall populate a list of tweets[Tweet] with our data.\n","    After deep thought it is better to use a matrix instead of classes. We shall use pandas to represent our dataset."]},{"cell_type":"code","execution_count":6,"id":"c0d8d1ff","metadata":{"_cell_guid":"f4d75feb-2e8a-4357-b398-b76b762d188a","_uuid":"d1e8751f-ad65-4fd7-8025-9cc982ccd09b","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:34.936517Z","iopub.status.busy":"2023-01-04T21:52:34.935311Z","iopub.status.idle":"2023-01-04T21:52:35.181524Z","shell.execute_reply":"2023-01-04T21:52:35.180716Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.25744,"end_time":"2023-01-04T21:52:35.183828","exception":false,"start_time":"2023-01-04T21:52:34.926388","status":"completed"},"tags":[]},"outputs":[],"source":["column_names = [\"sentiment\", \"content\"]\n","train_tweets_positive = pd.read_table(\"/kaggle/input/arabic-sentiment-twitter-corpus/train_Arabic_tweets_positive_20190413.tsv\", names=column_names)\n","train_tweets_negative = pd.read_table(\"/kaggle/input/arabic-sentiment-twitter-corpus/train_Arabic_tweets_negative_20190413.tsv\", names=column_names)\n","test_tweets_positive = pd.read_table(\"/kaggle/input/arabic-sentiment-twitter-corpus/test_Arabic_tweets_positive_20190413.tsv\", names=column_names)\n","test_tweets_negative = pd.read_table(\"/kaggle/input/arabic-sentiment-twitter-corpus/test_Arabic_tweets_negative_20190413.tsv\", names=column_names)"]},{"cell_type":"code","execution_count":7,"id":"5862ff07","metadata":{"_cell_guid":"4cb90bf1-f45d-4c2d-8e22-19dda25c3ef2","_uuid":"49a5e898-0cc0-40e7-aa9f-ca9624957948","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:35.201865Z","iopub.status.busy":"2023-01-04T21:52:35.201365Z","iopub.status.idle":"2023-01-04T21:52:35.226591Z","shell.execute_reply":"2023-01-04T21:52:35.225349Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.036354,"end_time":"2023-01-04T21:52:35.228571","exception":false,"start_time":"2023-01-04T21:52:35.192217","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentiment</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>pos</td>\n","      <td>Ù†Ø­Ù† Ø§Ù„Ø°ÙŠÙ† ÙŠØªØ­ÙˆÙ„ ÙƒÙ„ Ù…Ø§ Ù†ÙˆØ¯ Ø£Ù† Ù†Ù‚ÙˆÙ„Ù‡ Ø¥Ù„Ù‰ Ø¯Ø¹Ø§Ø¡ Ù„Ù„...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>pos</td>\n","      <td>ÙˆÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ© Ù„Ù† ÙŠØ¨Ù‚Ù‰Ù° Ù…Ø¹Ùƒ Ø¢Ø­Ø¯Ø¥Ù„Ø§ Ù…Ù† Ø±Ø£Ù‰Ù° Ø§Ù„Ø¬Ù…Ø§Ù„...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>pos</td>\n","      <td>Ù…Ù† Ø§Ù„Ø®ÙŠØ± Ù†ÙØ³Ù‡ ğŸ’›</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>pos</td>\n","      <td>#Ø²Ù„Ø²Ù„_Ø§Ù„Ù…Ù„Ø¹Ø¨_Ù†ØµØ±Ù†Ø§_Ø¨ÙŠÙ„Ø¹Ø¨ ÙƒÙ† Ø¹Ø§Ù„ÙŠ Ø§Ù„Ù‡Ù…Ù‡ ÙˆÙ„Ø§ ØªØ±Ø¶...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>pos</td>\n","      <td>Ø§Ù„Ø´ÙŠØ¡ Ø§Ù„ÙˆØ­ÙŠØ¯ Ø§Ù„Ø°ÙŠ ÙˆØµÙ„ÙˆØ§ ÙÙŠÙ‡ Ù„Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ù‡Ùˆ : Ø§Ù„Ù…Ø³...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>22509</th>\n","      <td>neg</td>\n","      <td>ÙƒÙŠÙ ØªØ±Ù‰ Ø£ÙˆØ±Ø§Ù†ÙˆØ³ Ù„Ùˆ ÙƒØ§Ù† ÙŠÙ‚Ø¹ Ù…ÙƒØ§Ù† Ø§Ù„Ù‚Ù…Ø± ØŸ ğŸ’™ğŸ’™ ÙƒÙˆÙƒ...</td>\n","    </tr>\n","    <tr>\n","      <th>22510</th>\n","      <td>neg</td>\n","      <td>Ø§Ø­Ø³Ø¯Ùƒ Ø¹Ù„Ù‰ Ø§Ù„Ø§ÙŠÙ… ğŸ’”</td>\n","    </tr>\n","    <tr>\n","      <th>22511</th>\n","      <td>neg</td>\n","      <td>Ù„Ø£ÙˆÙ„ Ù…Ø±Ø© Ù…Ø§ Ø¨Ù†ÙƒÙˆÙ† Ø³ÙˆØ§ ğŸ’”</td>\n","    </tr>\n","    <tr>\n","      <th>22512</th>\n","      <td>neg</td>\n","      <td>Ø¨Ù‚Ù„Ù‡ Ù„ÙŠØ´ ÙŠØ§ ÙˆØ§Ø·ÙŠ ğŸ¤”</td>\n","    </tr>\n","    <tr>\n","      <th>22513</th>\n","      <td>neg</td>\n","      <td>Ù‚Ø¯ Ø·Ø§Ù„ ØµØ¨Ø±ÙŠ ÙÙŠ Ø§Ù„Ù†ÙˆÙ‰ Ø¥Ø° ØªØ±ÙƒØªÙ†ÙŠ ÙƒØ¦ÙŠØ¨Ø§ Ø› ØºØ±ÙŠØ¨Ø§ Ø¨...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>45275 rows Ã— 2 columns</p>\n","</div>"],"text/plain":["      sentiment                                            content\n","0           pos  Ù†Ø­Ù† Ø§Ù„Ø°ÙŠÙ† ÙŠØªØ­ÙˆÙ„ ÙƒÙ„ Ù…Ø§ Ù†ÙˆØ¯ Ø£Ù† Ù†Ù‚ÙˆÙ„Ù‡ Ø¥Ù„Ù‰ Ø¯Ø¹Ø§Ø¡ Ù„Ù„...\n","1           pos  ÙˆÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ© Ù„Ù† ÙŠØ¨Ù‚Ù‰Ù° Ù…Ø¹Ùƒ Ø¢Ø­Ø¯Ø¥Ù„Ø§ Ù…Ù† Ø±Ø£Ù‰Ù° Ø§Ù„Ø¬Ù…Ø§Ù„...\n","2           pos                                    Ù…Ù† Ø§Ù„Ø®ÙŠØ± Ù†ÙØ³Ù‡ ğŸ’›\n","3           pos  #Ø²Ù„Ø²Ù„_Ø§Ù„Ù…Ù„Ø¹Ø¨_Ù†ØµØ±Ù†Ø§_Ø¨ÙŠÙ„Ø¹Ø¨ ÙƒÙ† Ø¹Ø§Ù„ÙŠ Ø§Ù„Ù‡Ù…Ù‡ ÙˆÙ„Ø§ ØªØ±Ø¶...\n","4           pos  Ø§Ù„Ø´ÙŠØ¡ Ø§Ù„ÙˆØ­ÙŠØ¯ Ø§Ù„Ø°ÙŠ ÙˆØµÙ„ÙˆØ§ ÙÙŠÙ‡ Ù„Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ù‡Ùˆ : Ø§Ù„Ù…Ø³...\n","...         ...                                                ...\n","22509       neg  ÙƒÙŠÙ ØªØ±Ù‰ Ø£ÙˆØ±Ø§Ù†ÙˆØ³ Ù„Ùˆ ÙƒØ§Ù† ÙŠÙ‚Ø¹ Ù…ÙƒØ§Ù† Ø§Ù„Ù‚Ù…Ø± ØŸ ğŸ’™ğŸ’™ ÙƒÙˆÙƒ...\n","22510       neg                                  Ø§Ø­Ø³Ø¯Ùƒ Ø¹Ù„Ù‰ Ø§Ù„Ø§ÙŠÙ… ğŸ’”\n","22511       neg                            Ù„Ø£ÙˆÙ„ Ù…Ø±Ø© Ù…Ø§ Ø¨Ù†ÙƒÙˆÙ† Ø³ÙˆØ§ ğŸ’”\n","22512       neg                                 Ø¨Ù‚Ù„Ù‡ Ù„ÙŠØ´ ÙŠØ§ ÙˆØ§Ø·ÙŠ ğŸ¤”\n","22513       neg  Ù‚Ø¯ Ø·Ø§Ù„ ØµØ¨Ø±ÙŠ ÙÙŠ Ø§Ù„Ù†ÙˆÙ‰ Ø¥Ø° ØªØ±ÙƒØªÙ†ÙŠ ÙƒØ¦ÙŠØ¨Ø§ Ø› ØºØ±ÙŠØ¨Ø§ Ø¨...\n","\n","[45275 rows x 2 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["X_train = pd.concat([train_tweets_positive,train_tweets_negative])\n","\n","from IPython.display import display, HTML\n","display(X_train)"]},{"cell_type":"markdown","id":"5d7b0ea2","metadata":{"papermill":{"duration":0.008248,"end_time":"2023-01-04T21:52:35.245265","exception":false,"start_time":"2023-01-04T21:52:35.237017","status":"completed"},"tags":[]},"source":["## Preprocessing dataset\n","Now we need to remove special characters including emoticones since apparently there are many emoticones in arabic tweets, we also need to remove punctuation and `tashkil` which is special character above letters to determine how they are pronounced\n","\n","So all that needs to be removed is:\n","* special characters and emoticones\n","* stop words\n","* ponctuation"]},{"cell_type":"code","execution_count":8,"id":"89b9246c","metadata":{"execution":{"iopub.execute_input":"2023-01-04T21:52:35.262914Z","iopub.status.busy":"2023-01-04T21:52:35.262567Z","iopub.status.idle":"2023-01-04T21:52:52.492517Z","shell.execute_reply":"2023-01-04T21:52:52.491555Z"},"papermill":{"duration":17.240986,"end_time":"2023-01-04T21:52:52.494306","exception":false,"start_time":"2023-01-04T21:52:35.25332","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["0        Ù†Ø­Ù† Ø§Ù„Ø°ÙŠÙ† ÙŠØªØ­ÙˆÙ„ ÙƒÙ„ Ù…Ø§ Ù†ÙˆØ¯ Ø£Ù† Ù†Ù‚ÙˆÙ„Ù‡ Ø¥Ù„Ù‰ Ø¯Ø¹Ø§Ø¡ Ù„Ù„...\n","1        ÙˆÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ© Ù„Ù† ÙŠØ¨Ù‚Ù‰ Ù…Ø¹Ùƒ Ø¢Ø­Ø¯Ø¥Ù„Ø§ Ù…Ù† Ø±Ø£Ù‰ Ø§Ù„Ø¬Ù…Ø§Ù„ Ù...\n","2                                           Ù…Ù† Ø§Ù„Ø®ÙŠØ± Ù†ÙØ³Ù‡ \n","3        Ø²Ù„Ø²Ù„_Ø§Ù„Ù…Ù„Ø¹Ø¨_Ù†ØµØ±Ù†Ø§_Ø¨ÙŠÙ„Ø¹Ø¨ ÙƒÙ† Ø¹Ø§Ù„ÙŠ Ø§Ù„Ù‡Ù…Ù‡ ÙˆÙ„Ø§ ØªØ±Ø¶Ù‰...\n","4        Ø§Ù„Ø´ÙŠØ¡ Ø§Ù„ÙˆØ­ÙŠØ¯ Ø§Ù„Ø°ÙŠ ÙˆØµÙ„ÙˆØ§ ÙÙŠÙ‡ Ù„Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ù‡Ùˆ  Ø§Ù„Ù…Ø³ÙŠ...\n","                               ...                        \n","22509    ÙƒÙŠÙ ØªØ±Ù‰ Ø£ÙˆØ±Ø§Ù†ÙˆØ³ Ù„Ùˆ ÙƒØ§Ù† ÙŠÙ‚Ø¹ Ù…ÙƒØ§Ù† Ø§Ù„Ù‚Ù…Ø±   ÙƒÙˆÙƒØ¨ Ø¨...\n","22510                                     Ø§Ø­Ø³Ø¯Ùƒ Ø¹Ù„Ù‰ Ø§Ù„Ø§ÙŠÙ… \n","22511                               Ù„Ø£ÙˆÙ„ Ù…Ø±Ø© Ù…Ø§ Ø¨Ù†ÙƒÙˆÙ† Ø³ÙˆØ§ \n","22512                                    Ø¨Ù‚Ù„Ù‡ Ù„ÙŠØ´ ÙŠØ§ ÙˆØ§Ø·ÙŠ \n","22513    Ù‚Ø¯ Ø·Ø§Ù„ ØµØ¨Ø±ÙŠ ÙÙŠ Ø§Ù„Ù†ÙˆÙ‰ Ø¥Ø° ØªØ±ÙƒØªÙ†ÙŠ ÙƒØ¦ÙŠØ¨Ø§  ØºØ±ÙŠØ¨Ø§ Ø¨Ø§...\n","Name: content, Length: 45275, dtype: object"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.corpus import stopwords\n","\n","def remove_stop_words(text):\n","    stop_words = set(stopwords.words(\"arabic\"))\n","    text = text.split()\n","    return \" \".join([word for word in text if word not in stop_words])\n","\n","import re\n","\n","def remove_emojis(text):\n","    emoji_pattern = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                           \"]+\", flags=re.UNICODE)\n","    return emoji_pattern.sub(r'', text)\n","\n","def remove_punctuation(text):\n","    return re.sub(r'[^\\w\\s]','',text)\n","\n","X_train.content.apply(remove_stop_words)\n","X_train.content.apply(remove_emojis)\n","X_train.content.apply(remove_punctuation)"]},{"cell_type":"markdown","id":"ef4ee65d","metadata":{"_cell_guid":"bc011b93-5ee3-47b1-b3e9-f66910d47ee1","_uuid":"f34fe2b0-69d1-4fc4-9154-2b23a28bfb66","papermill":{"duration":0.008002,"end_time":"2023-01-04T21:52:52.510866","exception":false,"start_time":"2023-01-04T21:52:52.502864","status":"completed"},"tags":[]},"source":["## Preprocessing dataset\n","Now we need to remove special characters## Tokenizing dataset\n","We need to write our tweets text as a feature-term dataframe (feature-term matrix). When using countvectorizer there is no need to preprocess the data, as it already removes stop words and speical characters.\n","\n","### Using CountVectorizer"]},{"cell_type":"code","execution_count":9,"id":"8e248f07","metadata":{"_cell_guid":"b5262f3a-9aa2-4b93-a6e7-8949ab7a9221","_uuid":"e3006a58-5728-4895-8ae6-36578619940b","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:52.529159Z","iopub.status.busy":"2023-01-04T21:52:52.528799Z","iopub.status.idle":"2023-01-04T21:52:53.219354Z","shell.execute_reply":"2023-01-04T21:52:53.218101Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.701991,"end_time":"2023-01-04T21:52:53.221361","exception":false,"start_time":"2023-01-04T21:52:52.51937","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(45275, 70856)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","count_vect = CountVectorizer()\n","X_train_counts = count_vect.fit_transform(X_train.content)\n","X_train_counts.shape"]},{"cell_type":"markdown","id":"a93a0be4","metadata":{"_cell_guid":"ca352397-dcdb-43c2-8e25-76fdd9f02ac2","_uuid":"3db34197-2475-4e45-a761-8e41e73a094b","papermill":{"duration":0.007899,"end_time":"2023-01-04T21:52:53.237803","exception":false,"start_time":"2023-01-04T21:52:53.229904","status":"completed"},"tags":[]},"source":["### Using TfidfTransformer"]},{"cell_type":"code","execution_count":10,"id":"1f8a1ffd","metadata":{"_cell_guid":"c12ed70f-ac79-4671-b21f-3c1259e30b0e","_uuid":"c70c0eda-68b1-4189-93d6-2030ab82e9e8","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:53.25627Z","iopub.status.busy":"2023-01-04T21:52:53.255916Z","iopub.status.idle":"2023-01-04T21:52:53.32066Z","shell.execute_reply":"2023-01-04T21:52:53.319725Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.076465,"end_time":"2023-01-04T21:52:53.322661","exception":false,"start_time":"2023-01-04T21:52:53.246196","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(45275, 70856)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import TfidfTransformer\n","tfidf_transformer = TfidfTransformer()\n","X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n","X_train_tfidf.shape"]},{"cell_type":"markdown","id":"03f3d2f1","metadata":{"_cell_guid":"e4fb18f7-951e-4373-ae73-345b9a60cf7f","_uuid":"15a5ad41-c2f5-4594-8384-0eb6d899cb5a","papermill":{"duration":0.008058,"end_time":"2023-01-04T21:52:53.339295","exception":false,"start_time":"2023-01-04T21:52:53.331237","status":"completed"},"tags":[]},"source":["## Training a classifier\n","### Using naÃ¯ve Bayes"]},{"cell_type":"code","execution_count":11,"id":"db030f51","metadata":{"_cell_guid":"645996d7-320d-4b51-b534-b142fc545514","_uuid":"4dd3088e-6f46-44eb-a7b7-d875a8af6e7f","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:53.357811Z","iopub.status.busy":"2023-01-04T21:52:53.357428Z","iopub.status.idle":"2023-01-04T21:52:53.461975Z","shell.execute_reply":"2023-01-04T21:52:53.461317Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.116371,"end_time":"2023-01-04T21:52:53.464042","exception":false,"start_time":"2023-01-04T21:52:53.347671","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.naive_bayes import MultinomialNB\n","clf = MultinomialNB().fit(X_train_tfidf, X_train.sentiment)"]},{"cell_type":"markdown","id":"84fb1d04","metadata":{"_cell_guid":"6dc0f87e-66d4-4973-9f11-768bdaf82254","_uuid":"dde58efe-9130-4465-bfe6-d09719bc25ef","papermill":{"duration":0.008336,"end_time":"2023-01-04T21:52:53.481047","exception":false,"start_time":"2023-01-04T21:52:53.472711","status":"completed"},"tags":[]},"source":["We will now use our testing dataset (we combine the negative and positivie tweests into one pandas dataframe), then we will call transfrom without calling fit in order to make a prediction."]},{"cell_type":"code","execution_count":12,"id":"9c0d21bc","metadata":{"_cell_guid":"2bc882d9-64f3-4084-839e-c15f1bf5f8f6","_uuid":"fe2ffab8-6e91-49ff-b6fd-0b558c7a3035","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:53.501195Z","iopub.status.busy":"2023-01-04T21:52:53.499749Z","iopub.status.idle":"2023-01-04T21:52:53.66366Z","shell.execute_reply":"2023-01-04T21:52:53.662679Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.175742,"end_time":"2023-01-04T21:52:53.666008","exception":false,"start_time":"2023-01-04T21:52:53.490266","status":"completed"},"tags":[]},"outputs":[],"source":["X_test = pd.concat([test_tweets_positive,test_tweets_negative])\n","\n","X_test_counts = count_vect.transform(X_test.content)\n","X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n","\n","predicted = clf.predict(X_test_tfidf)\n","\n","# for tweet, sentiment in zip(X_test.content, predicted):\n","#     print('%r => %s' % (tweet, sentiment))"]},{"cell_type":"markdown","id":"f57c7b72","metadata":{"_cell_guid":"acb2606b-96b4-4d9f-9a71-cb155b57b87b","_uuid":"19c3b799-f686-4d8e-89a9-e1c5eb3b5c54","papermill":{"duration":0.007882,"end_time":"2023-01-04T21:52:53.682549","exception":false,"start_time":"2023-01-04T21:52:53.674667","status":"completed"},"tags":[]},"source":["## Calculating Accuracy Score\n","Now we need to get a percentage of the accuracy of our model, we have a list of predicted sentiment and a list of the actual sentiment. Whenever predicted sentiment is different of the actual one we will increment a counter, after going through all the tweets we will divide but the total number of tweets to get a percentage of the wrong predictions, to get the percentage of the right prediction all we need to do is subtract the calculated score from 100%."]},{"cell_type":"code","execution_count":13,"id":"390668e8","metadata":{"_cell_guid":"19c26993-64cb-49bb-b033-f7186a705740","_uuid":"afb6c115-b16d-4fb9-a060-ef68a1aa2bdf","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:53.700216Z","iopub.status.busy":"2023-01-04T21:52:53.699854Z","iopub.status.idle":"2023-01-04T21:52:53.711679Z","shell.execute_reply":"2023-01-04T21:52:53.710229Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.023174,"end_time":"2023-01-04T21:52:53.713834","exception":false,"start_time":"2023-01-04T21:52:53.69066","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["validity score: 78.4375%\n"]}],"source":["wrong_predictions = 0\n","validity_score = 0\n","for predicted_sentiment, actual_sentiment in zip(predicted, X_test.sentiment):\n","    if predicted_sentiment != actual_sentiment:\n","        wrong_predictions += 1\n","wrong_predictions_percentage = wrong_predictions / len(X_test.sentiment)\n","validity_score = 1 - wrong_predictions_percentage\n","print(\"validity score: \" + str(validity_score*100) + \"%\")"]},{"cell_type":"markdown","id":"7c84c70b","metadata":{"_cell_guid":"6b21ef9a-866c-4b88-8577-164f8a1e29ec","_uuid":"48435d85-c47f-4438-b829-1aee03325841","papermill":{"duration":0.008094,"end_time":"2023-01-04T21:52:53.730377","exception":false,"start_time":"2023-01-04T21:52:53.722283","status":"completed"},"tags":[]},"source":["For the time being we have a validity score of `78.4375%` therefore our prediction model is considred bad, we think it is because we're studying text in `arabic` CountVectorizer is unable to correctly preprocess text and tokenize it. We will try to use another vectorizer to see if the validity increases.\n","\n","### Building a pipeline\n","To simplify our training and prediction process we will build a new Pipeline"]},{"cell_type":"code","execution_count":14,"id":"6a6f440f","metadata":{"_cell_guid":"12907a38-023f-4149-81c8-a385f64f7186","_uuid":"6ccd3360-b20a-4c43-a609-ed1b718c5dd0","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:53.748521Z","iopub.status.busy":"2023-01-04T21:52:53.748169Z","iopub.status.idle":"2023-01-04T21:52:53.75576Z","shell.execute_reply":"2023-01-04T21:52:53.755046Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.018679,"end_time":"2023-01-04T21:52:53.757431","exception":false,"start_time":"2023-01-04T21:52:53.738752","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import SGDClassifier\n","\n","text_clf = Pipeline([\n","    ('vect', CountVectorizer()),\n","    ('tfidf', TfidfTransformer()),\n","    ('clf', SGDClassifier(loss='hinge', penalty=None,\n","                          alpha=1e-3, random_state=2,\n","                          max_iter=5, tol=None)),\n","])"]},{"cell_type":"markdown","id":"54ba5e24","metadata":{"_cell_guid":"2d6a1601-c2c5-46ab-93bf-4a1f7fcd57e6","_uuid":"0e26a857-855e-4042-bc5a-ff8758c44c09","papermill":{"duration":0.008226,"end_time":"2023-01-04T21:52:53.774338","exception":false,"start_time":"2023-01-04T21:52:53.766112","status":"completed"},"tags":[]},"source":["### Using SGDClassifierfrom sklearn.linear_model import SGDClassifier"]},{"cell_type":"code","execution_count":15,"id":"8cb6c323","metadata":{"_cell_guid":"38158940-ab9a-4410-9808-30357a62a7fb","_uuid":"279b3351-878e-45ac-b87e-0799205f4640","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:53.792844Z","iopub.status.busy":"2023-01-04T21:52:53.792312Z","iopub.status.idle":"2023-01-04T21:52:54.844498Z","shell.execute_reply":"2023-01-04T21:52:54.84347Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":1.063944,"end_time":"2023-01-04T21:52:54.846764","exception":false,"start_time":"2023-01-04T21:52:53.78282","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["0.7395833333333334"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["text_clf.fit(X_train.content, X_train.sentiment)\n","\n","predicted = text_clf.predict(X_test.content)\n","np.mean(predicted == X_test.sentiment)"]},{"cell_type":"markdown","id":"f7548eea","metadata":{"_cell_guid":"68a204ce-0b94-43ec-8546-423aac5564e5","_uuid":"6ddfb7ef-00f4-4870-b61b-411604437e54","papermill":{"duration":0.008162,"end_time":"2023-01-04T21:52:54.863905","exception":false,"start_time":"2023-01-04T21:52:54.855743","status":"completed"},"tags":[]},"source":["Now we will try to `Lemmetize` tweets, we believe this is the reason why our prediction model isn't producing better results.\n","We could try `Farasa` lemmetizer as it has a good reputation of outperforming other lemmetizers. But it uses an API and is not available as an imported library usable directly.\n","So for now we will just stick with ntlk's `ISRIStemmer`"]},{"cell_type":"code","execution_count":16,"id":"7ba61755","metadata":{"_cell_guid":"ea1968d1-15bf-4de2-8fe0-181a0e161618","_uuid":"e8ed1312-35a1-4711-a76f-01e1c5296650","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:54.882864Z","iopub.status.busy":"2023-01-04T21:52:54.882314Z","iopub.status.idle":"2023-01-04T21:52:54.886896Z","shell.execute_reply":"2023-01-04T21:52:54.885808Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.016391,"end_time":"2023-01-04T21:52:54.888794","exception":false,"start_time":"2023-01-04T21:52:54.872403","status":"completed"},"tags":[]},"outputs":[],"source":["from nltk.stem.isri import ISRIStemmer\n","st = ISRIStemmer()"]},{"cell_type":"markdown","id":"0d7ba666","metadata":{"_cell_guid":"9f278e4f-df5c-4a51-a45c-21447f11fdda","_uuid":"6f7b793d-49ca-48e1-9e0b-36a22d947f38","papermill":{"duration":0.008413,"end_time":"2023-01-04T21:52:54.906246","exception":false,"start_time":"2023-01-04T21:52:54.897833","status":"completed"},"tags":[]},"source":["A lemmetizer is also called a stemmer, NLTK has many stemmer solutions. We could try out each one of NLTK's stemmers and compare the results.\n","For now we will try out `ARLSTem Stemmer`\n","\n","* THE NEW PROCESS IS NOW:\n","1. tokenize\n","2. stem\n","3. remove stop words"]},{"cell_type":"code","execution_count":17,"id":"4ff75695","metadata":{"_cell_guid":"a53fabfb-3f1e-4226-886c-d5889bb8af5e","_uuid":"ca44c7c4-b9d4-459c-a9f7-83ced75b3e21","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:54.925333Z","iopub.status.busy":"2023-01-04T21:52:54.924792Z","iopub.status.idle":"2023-01-04T21:52:55.114374Z","shell.execute_reply":"2023-01-04T21:52:55.113224Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.201668,"end_time":"2023-01-04T21:52:55.116607","exception":false,"start_time":"2023-01-04T21:52:54.914939","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["0       #Ø§Ù„Ù‡Ù„Ø§Ù„_Ø§Ù„Ø§Ù‡Ù„ÙŠ ÙÙˆØ² Ù‡Ù„Ø§Ù„ÙŠ Ù…Ù‡Ù… Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡ ğŸ’™ Ø²ÙˆØ±Ø§Ù†...\n","1                                    ØµØ¨Ø§Ø­Ùƒ Ø®ÙŠØ±Ø§Øª ÙˆÙ…Ø³Ø±Ø§Øª ğŸŒ¸\n","2       #ØªØ§Ù…Ù„ Ù‚Ø§Ù„ Ø§Ù„Ù„Ù‡ ï·» :- _*â€‹ï´¿Ø¨ÙˆØ§Ø¯ ØºÙŠØ± Ø°ÙŠ Ø²Ø±Ø¹ ï´¾*_ ğŸ’«ğŸ’«...\n","3       ğŸ˜‚ğŸ˜‚ ÙŠØ§ Ø¬Ø¯Ø¹Ø§Ù† Ø§Ù„Ø±Ø¬Ø§Ù„Ù‡ Ø§Ù„Ù„ÙŠ ÙÙˆÙ‚ Ø§Ù„ Ø¯ÙˆÙ„ Ø®Ø·Ø± Ø¹ ØªÙˆÙŠØª...\n","4       Ø±Ø³Ø§Ù„Ù‡ ØµØ¨Ø§Ø­ÙŠÙ‡ : ğŸ’› Ø§Ù„Ù„Ù‡Ù… Ø§Ø³Ø§Ù„Ùƒ Ø§Ù„ØªÙˆÙÙŠÙ‚ ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§...\n","                              ...                        \n","5763    Ù†ÙˆÙ… ÙˆØ§Ù†Øª Ù…ÙƒØ³ÙˆØ± Ø¯Ù‡ Ø§Ø­Ø³Ø§Ø³ ØºØ¨ÙŠ Ø§Ù„Ù„ÙŠ Ù‡Ùˆ Ù…Ø´ Ù‚Ø§Ø¯Ø± ØªÙ†...\n","5764    Ø³ØªØ´Ù‡Ø§Ø¯_Ø§Ù„Ø§Ù…Ø§Ù…_ÙƒØ§Ø¸Ù…_Ø§Ù„ØºÙŠØ¸ Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠ Ø§Ù„Ù…Ø¹Ø°Ø¨ ÙÙŠ ...\n","5765                             Ø§Ù† ÙƒÙ†Øª Ø§ÙƒÙ„ Ø§Ù„ØµØ­Ù† Ø¨ÙƒØ¨Ø±Ù‡ ğŸ˜\n","5766    Ù‚ÙˆÙ„ÙˆØ§ Ù„ÙŠ Ø§ÙŠØ´ ØªØ´ÙˆÙÙˆØ§ .. Ù…Ø¹ Ù…Ù„Ø§Ø­Ø¸Ø© Ø§Ù„ØªÙ„Ø·Ù Ù„Ø§Ù†Ù‡ Ø§...\n","5767    âœ Ø§Ø°Ø§ Ø§Ø±Ø¯Øª Ø§Ù† ØªØ¹Ø±Ù Ø´ÙŠØ¦Ø§ Ø¹Ù†ÙŠ Ø§Ø³Ø§Ù„Ù†ÙŠ Ù‚Ø¨Ù„ Ø§Ù† ØªØ³Ø§Ù„...\n","Name: content, Length: 11520, dtype: object"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.stem.arlstem import ARLSTem\n","\n","stemmer = ARLSTem()\n","X_test.content.apply(stemmer.stem)"]},{"cell_type":"markdown","id":"89437569","metadata":{"_cell_guid":"32fd7ae3-82bb-4873-8366-0fcd80965846","_uuid":"4415217a-9fe2-41e1-b88f-3dc74e334cf0","papermill":{"duration":0.00853,"end_time":"2023-01-04T21:52:55.134278","exception":false,"start_time":"2023-01-04T21:52:55.125748","status":"completed"},"tags":[]},"source":["# NLTK tokenization\n","    So we're still getting a very low validity score for our prediction model. It could be because we're using Arabic language and scikit learn is unable to correctly tokenize words, it could be that the words that have the same root arent' considred as the same token. I will try to preprocess the data first to turn each tweet text into tokens. \n","    A problem that could arise is not being able to detect the order of words. I'm not sure if NLtk will scramble the words or will they be in the same order for us to be able to use n-gram of words later on."]},{"cell_type":"code","execution_count":18,"id":"3519ba39","metadata":{"_cell_guid":"96aef751-938c-4c7d-a529-014ba033a901","_uuid":"4de24fee-81c6-40ea-941f-0b00169a6a6e","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:55.153748Z","iopub.status.busy":"2023-01-04T21:52:55.153365Z","iopub.status.idle":"2023-01-04T21:52:55.545182Z","shell.execute_reply":"2023-01-04T21:52:55.544096Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.404059,"end_time":"2023-01-04T21:52:55.547324","exception":false,"start_time":"2023-01-04T21:52:55.143265","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["0        [Ù†Ø­Ù†, Ø§Ù„Ø°ÙŠÙ†, ÙŠØªØ­ÙˆÙ„, ÙƒÙ„, Ù…Ø§, Ù†ÙˆØ¯, Ø£Ù†, Ù†Ù‚ÙˆÙ„Ù‡, Ø¥Ù„...\n","1        [ÙˆÙÙŠ, Ø§Ù„Ù†Ù‡Ø§ÙŠØ©, Ù„Ù†, ÙŠØ¨Ù‚Ù‰, Ù°, Ù…Ø¹Ùƒ, Ø¢Ø­Ø¯Ø¥Ù„Ø§, Ù…Ù†, Ø±...\n","2                                     [Ù…Ù†, Ø§Ù„Ø®ÙŠØ±, Ù†ÙØ³Ù‡, ğŸ’›]\n","3        [#, Ø²Ù„Ø²Ù„_Ø§Ù„Ù…Ù„Ø¹Ø¨_Ù†ØµØ±Ù†Ø§_Ø¨ÙŠÙ„Ø¹Ø¨, ÙƒÙ†, Ø¹Ø§Ù„ÙŠ, Ø§Ù„Ù‡Ù…Ù‡, ...\n","4        [Ø§Ù„Ø´ÙŠØ¡, Ø§Ù„ÙˆØ­ÙŠØ¯, Ø§Ù„Ø°ÙŠ, ÙˆØµÙ„ÙˆØ§, ÙÙŠÙ‡, Ù„Ù„Ø¹Ø§Ù„Ù…ÙŠØ©, Ù‡Ùˆ...\n","                               ...                        \n","22509    [ÙƒÙŠÙ, ØªØ±Ù‰, Ø£ÙˆØ±Ø§Ù†ÙˆØ³, Ù„Ùˆ, ÙƒØ§Ù†, ÙŠÙ‚Ø¹, Ù…ÙƒØ§Ù†, Ø§Ù„Ù‚Ù…Ø±,...\n","22510                               [Ø§Ø­Ø³Ø¯Ùƒ, Ø¹Ù„Ù‰, Ø§Ù„Ø§ÙŠÙ…, ğŸ’”]\n","22511                       [Ù„Ø£ÙˆÙ„, Ù…Ø±Ø©, Ù…Ø§, Ø¨Ù†ÙƒÙˆÙ†, Ø³ÙˆØ§, ğŸ’”]\n","22512                             [Ø¨Ù‚Ù„Ù‡, Ù„ÙŠØ´, ÙŠØ§, ÙˆØ§Ø·ÙŠ, ğŸ¤”]\n","22513    [Ù‚Ø¯, Ø·Ø§Ù„, ØµØ¨Ø±ÙŠ, ÙÙŠ, Ø§Ù„Ù†ÙˆÙ‰, Ø¥Ø°, ØªØ±ÙƒØªÙ†ÙŠ, ÙƒØ¦ÙŠØ¨Ø§, ...\n","Name: content, Length: 45275, dtype: object"]},"metadata":{},"output_type":"display_data"}],"source":["from nltk.tokenize import wordpunct_tokenize,word_tokenize\n","\n","X_train = X_train.content.apply(wordpunct_tokenize)\n","display(X_train)"]},{"cell_type":"markdown","id":"014e0593","metadata":{"_cell_guid":"015c2d51-da19-4e95-a093-cd475b67125b","_uuid":"35301da8-3cd9-46cb-b859-52045c8d4cf0","papermill":{"duration":0.008363,"end_time":"2023-01-04T21:52:55.564742","exception":false,"start_time":"2023-01-04T21:52:55.556379","status":"completed"},"tags":[]},"source":["arabic isn't supported by `nltk` so we'll use some other third party library to tokenize our te"]},{"cell_type":"code","execution_count":19,"id":"52a6661d","metadata":{"_cell_guid":"76f933a7-228b-4b26-bdc6-8cbdf1fbd3fd","_uuid":"5d06e677-34eb-4331-9fa0-1234bd40fc62","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:55.5835Z","iopub.status.busy":"2023-01-04T21:52:55.583178Z","iopub.status.idle":"2023-01-04T21:52:55.587755Z","shell.execute_reply":"2023-01-04T21:52:55.586493Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.016195,"end_time":"2023-01-04T21:52:55.589632","exception":false,"start_time":"2023-01-04T21:52:55.573437","status":"completed"},"tags":[]},"outputs":[],"source":["# import tkseem as tk\n","\n","# tkseem_tokenizer = tk.WordTokenizer()\n","# X_train.content.apply(tkseem_tokenizer.tokenize)"]},{"cell_type":"markdown","id":"b3410145","metadata":{"_cell_guid":"f36c4f68-7a1b-483b-b4b3-45c7174ab3c2","_uuid":"20577384-4ae0-4530-9802-78167ed0ad6b","papermill":{"duration":0.008649,"end_time":"2023-01-04T21:52:55.607421","exception":false,"start_time":"2023-01-04T21:52:55.598772","status":"completed"},"tags":[]},"source":["### Parameter tuning using grid search"]},{"cell_type":"code","execution_count":20,"id":"c7a4e2cc","metadata":{"_cell_guid":"94b5d231-80d4-4a38-8955-53d142cc0be7","_uuid":"de14a436-17c2-4e5a-9eec-705f85f65406","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:55.626304Z","iopub.status.busy":"2023-01-04T21:52:55.625909Z","iopub.status.idle":"2023-01-04T21:52:55.630834Z","shell.execute_reply":"2023-01-04T21:52:55.629533Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.016924,"end_time":"2023-01-04T21:52:55.633054","exception":false,"start_time":"2023-01-04T21:52:55.61613","status":"completed"},"tags":[]},"outputs":[],"source":["# from sklearn.model_selection import GridSearchCV\n","\n","# parameters = {\n","#     'vect__ngram_range': [(1, 1), (1, 2)],\n","#     'tfidf__use_idf': (True, False),\n","#     'clf__alpha': (1e-2, 1e-3),\n","# }\n","\n","# gs_clf = GridSearchCV(text_clf, parameters, cv=5, n_jobs=-1)\n","# gs_clf = gs_clf.fit(X_train.content[:400], X_train.sentiment[:400])"]},{"cell_type":"markdown","id":"64489ea6","metadata":{"_cell_guid":"681e47fe-a024-4c79-b2ba-76c088564b66","_uuid":"29fb8111-45f0-480c-8a28-665dd790f1e3","papermill":{"duration":0.008541,"end_time":"2023-01-04T21:52:55.650865","exception":false,"start_time":"2023-01-04T21:52:55.642324","status":"completed"},"tags":[]},"source":["## Extracting features from tweets data\n","    After loading the data the next step is to try and extract features from our tweets corpus."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":29.307634,"end_time":"2023-01-04T21:52:56.480725","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-01-04T21:52:27.173091","version":"2.3.4"}},"nbformat":4,"nbformat_minor":5}