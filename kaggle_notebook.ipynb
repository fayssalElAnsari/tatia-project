{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/fayssalelansari/notebookc111a6c6b8?scriptVersionId=115511459\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","execution_count":1,"id":"6dcc92ba","metadata":{"_cell_guid":"4c1ef890-63d4-444a-b58a-f41939d9e62f","_uuid":"87d30fb1-988f-4483-b632-02009c36d444","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T20:50:30.969072Z","iopub.status.busy":"2023-01-04T20:50:30.968553Z","iopub.status.idle":"2023-01-04T20:50:30.979339Z","shell.execute_reply":"2023-01-04T20:50:30.978262Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.025481,"end_time":"2023-01-04T20:50:30.982139","exception":false,"start_time":"2023-01-04T20:50:30.956658","status":"completed"},"tags":[]},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","id":"4e8679ed","metadata":{"_cell_guid":"0380ce88-a279-42cb-b40b-c7476b152bd0","_uuid":"ec3e36fc-4064-4b35-951d-0b595aa23b14","papermill":{"duration":0.008463,"end_time":"2023-01-04T20:50:30.999587","exception":false,"start_time":"2023-01-04T20:50:30.991124","status":"completed"},"tags":[]},"source":["### constants"]},{"cell_type":"code","execution_count":2,"id":"459f11a1","metadata":{"_cell_guid":"2bd62210-dad9-4e66-b738-4660c96b0fc9","_uuid":"7bbff164-5a1e-4f63-bd19-aa61814a168e","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T20:50:31.018712Z","iopub.status.busy":"2023-01-04T20:50:31.018237Z","iopub.status.idle":"2023-01-04T20:50:31.023971Z","shell.execute_reply":"2023-01-04T20:50:31.022707Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.018037,"end_time":"2023-01-04T20:50:31.026367","exception":false,"start_time":"2023-01-04T20:50:31.00833","status":"completed"},"tags":[]},"outputs":[],"source":["enc = \"UTF-8\""]},{"cell_type":"markdown","id":"b68d020e","metadata":{"_cell_guid":"6ec183c9-8c80-45e5-9e8f-9c9681f60b13","_uuid":"d9cc1119-139a-4382-8c4e-8ac304c69b41","papermill":{"duration":0.008148,"end_time":"2023-01-04T20:50:31.04332","exception":false,"start_time":"2023-01-04T20:50:31.035172","status":"completed"},"tags":[]},"source":["### model files"]},{"cell_type":"code","execution_count":3,"id":"1c2b199c","metadata":{"_cell_guid":"9e978d47-5887-4d41-91fb-63ef6ffd63b0","_uuid":"45b66073-ec3a-4a74-9cf1-441edc87b64c","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T20:50:31.06407Z","iopub.status.busy":"2023-01-04T20:50:31.063573Z","iopub.status.idle":"2023-01-04T20:50:31.077804Z","shell.execute_reply":"2023-01-04T20:50:31.076582Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.026719,"end_time":"2023-01-04T20:50:31.0803","exception":false,"start_time":"2023-01-04T20:50:31.053581","status":"completed"},"tags":[]},"outputs":[],"source":["import enum\n","\n","class Tag(enum.Enum):\n","    Neutral = \"NEUTRAL\"\n","    Positive = \"POSITIVE\"\n","    Negative = \"NEGATIVE\"\n","    Not_defined = \"NOT_DEFINED\"\n","\n","class Tweet:\n","    def __init__(self, text, real_tag=Tag.Not_defined, given_tag=Tag.Not_defined):\n","        self.text = text\n","        self.real_tag = real_tag\n","        self.given_tag = given_tag\n","\n","\n","    def __str__(self):\n","        txt = self.text\n","        return txt\n","\n","    def __repr__(self):\n","        txt = self.text \n","        return txt"]},{"cell_type":"markdown","id":"847f738a","metadata":{"_cell_guid":"ca0e9aec-e672-4faa-9b5f-8d43ee952691","_uuid":"c2c6de7b-7687-4684-8b7e-a8083b91d10b","papermill":{"duration":0.008094,"end_time":"2023-01-04T20:50:31.097044","exception":false,"start_time":"2023-01-04T20:50:31.08895","status":"completed"},"tags":[]},"source":["## Loading data\n","### Using our own method and model files\n","    In this section we will use our own definition of a function that will load the files by traversing the folders containing the positive and the negative tweets\n","    We should compare this method to simply reading the `.tsv` files (which should be faster)."]},{"cell_type":"code","execution_count":4,"id":"e4dc8e9d","metadata":{"_cell_guid":"ca64d88f-05f5-4080-a33f-5fbf92fc36e5","_uuid":"9b2d8e93-cc11-4c21-864b-f0f2a05339fa","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T20:50:31.115828Z","iopub.status.busy":"2023-01-04T20:50:31.115342Z","iopub.status.idle":"2023-01-04T20:50:31.121448Z","shell.execute_reply":"2023-01-04T20:50:31.120206Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.0183,"end_time":"2023-01-04T20:50:31.123836","exception":false,"start_time":"2023-01-04T20:50:31.105536","status":"completed"},"tags":[]},"outputs":[],"source":["import os\n","import pathlib\n","\n","# # current_path = pathlib.Path(__file__).parent.resolve()\n","# POS_COUNT = 29848\n","# NEG_COUNT = 28901\n","# tweets = []\n","\n","# def import_data():\n","#     for i in range(POS_COUNT):\n","#         text = \"\"\n","#         with open(\"/kaggle/input/arabic-sentiment-twitter-corpus/arabic_tweets/pos/\" + str(i) + '.txt', encoding=enc) as f:\n","#             for line in f:\n","#                 text += line\n","#         tweets.append(Tweet(text, Tag.Positive))\n","#     for i in range(NEG_COUNT):\n","#         text = \"\"\n","#         with open(\"/kaggle/input/arabic-sentiment-twitter-corpus/arabic_tweets/neg/\" + str(i) + '.txt', encoding=enc) as f:\n","#             for line in f:\n","#                 text += line\n","#         tweets.append(Tweet(text, Tag.Negative))\n","        \n","# import_data()"]},{"cell_type":"code","execution_count":5,"id":"7a1083d6","metadata":{"_cell_guid":"5a3260da-95d6-46c9-b0a8-228f471bca48","_uuid":"002d715e-41d8-4714-8737-a9909c57f4db","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T20:50:31.142861Z","iopub.status.busy":"2023-01-04T20:50:31.142406Z","iopub.status.idle":"2023-01-04T20:50:31.147467Z","shell.execute_reply":"2023-01-04T20:50:31.146129Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.017454,"end_time":"2023-01-04T20:50:31.149881","exception":false,"start_time":"2023-01-04T20:50:31.132427","status":"completed"},"tags":[]},"outputs":[],"source":["# print(tweets)"]},{"cell_type":"markdown","id":"b71d96bf","metadata":{"_cell_guid":"5fab9c36-56cb-4c8f-8db0-e91c821be46d","_uuid":"b3302efc-f1e9-4003-b14b-7efe6dff8091","papermill":{"duration":0.00823,"end_time":"2023-01-04T20:50:31.166578","exception":false,"start_time":"2023-01-04T20:50:31.158348","status":"completed"},"tags":[]},"source":["### Using our own method and reading from `tsv` files directly\n","    same as the previous step we shall populate a list of tweets[Tweet] with our data.\n","    After deep thought it is better to use a matrix instead of classes. We shall use pandas to represent our dataset."]},{"cell_type":"code","execution_count":6,"id":"f32678bc","metadata":{"_cell_guid":"f4d75feb-2e8a-4357-b398-b76b762d188a","_uuid":"d1e8751f-ad65-4fd7-8025-9cc982ccd09b","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T20:50:31.185446Z","iopub.status.busy":"2023-01-04T20:50:31.185005Z","iopub.status.idle":"2023-01-04T20:50:31.500066Z","shell.execute_reply":"2023-01-04T20:50:31.498884Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.328105,"end_time":"2023-01-04T20:50:31.503114","exception":false,"start_time":"2023-01-04T20:50:31.175009","status":"completed"},"tags":[]},"outputs":[],"source":["column_names = [\"sentiment\", \"content\"]\n","train_tweets_positive = pd.read_table(\"/kaggle/input/arabic-sentiment-twitter-corpus/train_Arabic_tweets_positive_20190413.tsv\", names=column_names)\n","train_tweets_negative = pd.read_table(\"/kaggle/input/arabic-sentiment-twitter-corpus/train_Arabic_tweets_negative_20190413.tsv\", names=column_names)\n","test_tweets_positive = pd.read_table(\"/kaggle/input/arabic-sentiment-twitter-corpus/test_Arabic_tweets_positive_20190413.tsv\", names=column_names)\n","test_tweets_negative = pd.read_table(\"/kaggle/input/arabic-sentiment-twitter-corpus/test_Arabic_tweets_negative_20190413.tsv\", names=column_names)"]},{"cell_type":"code","execution_count":7,"id":"d99c465b","metadata":{"_cell_guid":"4cb90bf1-f45d-4c2d-8e22-19dda25c3ef2","_uuid":"49a5e898-0cc0-40e7-aa9f-ca9624957948","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T20:50:31.522451Z","iopub.status.busy":"2023-01-04T20:50:31.522033Z","iopub.status.idle":"2023-01-04T20:50:31.548365Z","shell.execute_reply":"2023-01-04T20:50:31.547038Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.039103,"end_time":"2023-01-04T20:50:31.550986","exception":false,"start_time":"2023-01-04T20:50:31.511883","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentiment</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>pos</td>\n","      <td>نحن الذين يتحول كل ما نود أن نقوله إلى دعاء لل...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>pos</td>\n","      <td>وفي النهاية لن يبقىٰ معك آحدإلا من رأىٰ الجمال...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>pos</td>\n","      <td>من الخير نفسه 💛</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>pos</td>\n","      <td>#زلزل_الملعب_نصرنا_بيلعب كن عالي الهمه ولا ترض...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>pos</td>\n","      <td>الشيء الوحيد الذي وصلوا فيه للعالمية هو : المس...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>22509</th>\n","      <td>neg</td>\n","      <td>كيف ترى أورانوس لو كان يقع مكان القمر ؟ 💙💙 كوك...</td>\n","    </tr>\n","    <tr>\n","      <th>22510</th>\n","      <td>neg</td>\n","      <td>احسدك على الايم 💔</td>\n","    </tr>\n","    <tr>\n","      <th>22511</th>\n","      <td>neg</td>\n","      <td>لأول مرة ما بنكون سوا 💔</td>\n","    </tr>\n","    <tr>\n","      <th>22512</th>\n","      <td>neg</td>\n","      <td>بقله ليش يا واطي 🤔</td>\n","    </tr>\n","    <tr>\n","      <th>22513</th>\n","      <td>neg</td>\n","      <td>قد طال صبري في النوى إذ تركتني كئيبا ؛ غريبا ب...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>45275 rows × 2 columns</p>\n","</div>"],"text/plain":["      sentiment                                            content\n","0           pos  نحن الذين يتحول كل ما نود أن نقوله إلى دعاء لل...\n","1           pos  وفي النهاية لن يبقىٰ معك آحدإلا من رأىٰ الجمال...\n","2           pos                                    من الخير نفسه 💛\n","3           pos  #زلزل_الملعب_نصرنا_بيلعب كن عالي الهمه ولا ترض...\n","4           pos  الشيء الوحيد الذي وصلوا فيه للعالمية هو : المس...\n","...         ...                                                ...\n","22509       neg  كيف ترى أورانوس لو كان يقع مكان القمر ؟ 💙💙 كوك...\n","22510       neg                                  احسدك على الايم 💔\n","22511       neg                            لأول مرة ما بنكون سوا 💔\n","22512       neg                                 بقله ليش يا واطي 🤔\n","22513       neg  قد طال صبري في النوى إذ تركتني كئيبا ؛ غريبا ب...\n","\n","[45275 rows x 2 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["X_train = pd.concat([train_tweets_positive,train_tweets_negative])\n","\n","from IPython.display import display, HTML\n","display(X_train)"]},{"cell_type":"markdown","id":"74c98dc2","metadata":{"_cell_guid":"bc011b93-5ee3-47b1-b3e9-f66910d47ee1","_uuid":"f34fe2b0-69d1-4fc4-9154-2b23a28bfb66","papermill":{"duration":0.00916,"end_time":"2023-01-04T20:50:31.569407","exception":false,"start_time":"2023-01-04T20:50:31.560247","status":"completed"},"tags":[]},"source":["## Preprocessing dataset\n","Now we need to remove special characters\n","\n","## Tokenizing dataset\n","We need to write our tweets text as a feature-term dataframe (feature-term matrix). When using countvectorizer there is no need to preprocess the data, as it already removes stop words and speical characters.\n","\n","### Using CountVectorizer"]},{"cell_type":"code","execution_count":8,"id":"822698a2","metadata":{"_cell_guid":"b5262f3a-9aa2-4b93-a6e7-8949ab7a9221","_uuid":"e3006a58-5728-4895-8ae6-36578619940b","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T20:50:31.589503Z","iopub.status.busy":"2023-01-04T20:50:31.589076Z","iopub.status.idle":"2023-01-04T20:50:33.657417Z","shell.execute_reply":"2023-01-04T20:50:33.65609Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":2.081843,"end_time":"2023-01-04T20:50:33.660318","exception":false,"start_time":"2023-01-04T20:50:31.578475","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(45275, 70856)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","count_vect = CountVectorizer()\n","X_train_counts = count_vect.fit_transform(X_train.content)\n","X_train_counts.shape"]},{"cell_type":"markdown","id":"048c1811","metadata":{"_cell_guid":"ca352397-dcdb-43c2-8e25-76fdd9f02ac2","_uuid":"3db34197-2475-4e45-a761-8e41e73a094b","papermill":{"duration":0.009124,"end_time":"2023-01-04T20:50:33.678731","exception":false,"start_time":"2023-01-04T20:50:33.669607","status":"completed"},"tags":[]},"source":["### Using TfidfTransformer"]},{"cell_type":"code","execution_count":9,"id":"d92625a9","metadata":{"_cell_guid":"c12ed70f-ac79-4671-b21f-3c1259e30b0e","_uuid":"c70c0eda-68b1-4189-93d6-2030ab82e9e8","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T20:50:33.699029Z","iopub.status.busy":"2023-01-04T20:50:33.698561Z","iopub.status.idle":"2023-01-04T20:50:33.771188Z","shell.execute_reply":"2023-01-04T20:50:33.770126Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.086076,"end_time":"2023-01-04T20:50:33.773934","exception":false,"start_time":"2023-01-04T20:50:33.687858","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(45275, 70856)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import TfidfTransformer\n","tfidf_transformer = TfidfTransformer()\n","X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n","X_train_tfidf.shape"]},{"cell_type":"markdown","id":"92a1ff38","metadata":{"_cell_guid":"e4fb18f7-951e-4373-ae73-345b9a60cf7f","_uuid":"15a5ad41-c2f5-4594-8384-0eb6d899cb5a","papermill":{"duration":0.00873,"end_time":"2023-01-04T20:50:33.791868","exception":false,"start_time":"2023-01-04T20:50:33.783138","status":"completed"},"tags":[]},"source":["## Training a classifier\n","### Using naïve Bayes"]},{"cell_type":"code","execution_count":10,"id":"72427bbe","metadata":{"_cell_guid":"645996d7-320d-4b51-b534-b142fc545514","_uuid":"4dd3088e-6f46-44eb-a7b7-d875a8af6e7f","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T20:50:33.812427Z","iopub.status.busy":"2023-01-04T20:50:33.811605Z","iopub.status.idle":"2023-01-04T20:50:33.948145Z","shell.execute_reply":"2023-01-04T20:50:33.947076Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.14974,"end_time":"2023-01-04T20:50:33.95069","exception":false,"start_time":"2023-01-04T20:50:33.80095","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.naive_bayes import MultinomialNB\n","clf = MultinomialNB().fit(X_train_tfidf, X_train.sentiment)"]},{"cell_type":"markdown","id":"1364d702","metadata":{"_cell_guid":"6dc0f87e-66d4-4973-9f11-768bdaf82254","_uuid":"dde58efe-9130-4465-bfe6-d09719bc25ef","papermill":{"duration":0.008768,"end_time":"2023-01-04T20:50:33.968672","exception":false,"start_time":"2023-01-04T20:50:33.959904","status":"completed"},"tags":[]},"source":["We will now use our testing dataset (we combine the negative and positivie tweests into one pandas dataframe), then we will call transfrom without calling fit in order to make a prediction."]},{"cell_type":"code","execution_count":11,"id":"ec05e65c","metadata":{"_cell_guid":"2bc882d9-64f3-4084-839e-c15f1bf5f8f6","_uuid":"fe2ffab8-6e91-49ff-b6fd-0b558c7a3035","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T20:50:33.988526Z","iopub.status.busy":"2023-01-04T20:50:33.988103Z","iopub.status.idle":"2023-01-04T20:50:34.180749Z","shell.execute_reply":"2023-01-04T20:50:34.179066Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.206029,"end_time":"2023-01-04T20:50:34.183802","exception":false,"start_time":"2023-01-04T20:50:33.977773","status":"completed"},"tags":[]},"outputs":[],"source":["X_test = pd.concat([test_tweets_positive,test_tweets_negative])\n","\n","X_test_counts = count_vect.transform(X_test.content)\n","X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n","\n","predicted = clf.predict(X_test_tfidf)\n","\n","# for tweet, sentiment in zip(X_test.content, predicted):\n","#     print('%r => %s' % (tweet, sentiment))"]},{"cell_type":"markdown","id":"64f89991","metadata":{"_cell_guid":"acb2606b-96b4-4d9f-9a71-cb155b57b87b","_uuid":"19c3b799-f686-4d8e-89a9-e1c5eb3b5c54","papermill":{"duration":0.00903,"end_time":"2023-01-04T20:50:34.202202","exception":false,"start_time":"2023-01-04T20:50:34.193172","status":"completed"},"tags":[]},"source":["## Calculating Accuracy Score\n","Now we need to get a percentage of the accuracy of our model, we have a list of predicted sentiment and a list of the actual sentiment. Whenever predicted sentiment is different of the actual one we will increment a counter, after going through all the tweets we will divide but the total number of tweets to get a percentage of the wrong predictions, to get the percentage of the right prediction all we need to do is subtract the calculated score from 100%."]},{"cell_type":"code","execution_count":12,"id":"eb908ba9","metadata":{"_cell_guid":"19c26993-64cb-49bb-b033-f7186a705740","_uuid":"afb6c115-b16d-4fb9-a060-ef68a1aa2bdf","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T20:50:34.224114Z","iopub.status.busy":"2023-01-04T20:50:34.223664Z","iopub.status.idle":"2023-01-04T20:50:34.238058Z","shell.execute_reply":"2023-01-04T20:50:34.236831Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.029117,"end_time":"2023-01-04T20:50:34.241486","exception":false,"start_time":"2023-01-04T20:50:34.212369","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["validity score: 78.4375%\n"]}],"source":["wrong_predictions = 0\n","validity_score = 0\n","for predicted_sentiment, actual_sentiment in zip(predicted, X_test.sentiment):\n","    if predicted_sentiment != actual_sentiment:\n","        wrong_predictions += 1\n","wrong_predictions_percentage = wrong_predictions / len(X_test.sentiment)\n","validity_score = 1 - wrong_predictions_percentage\n","print(\"validity score: \" + str(validity_score*100) + \"%\")"]},{"cell_type":"markdown","id":"0fdc239e","metadata":{"_cell_guid":"6b21ef9a-866c-4b88-8577-164f8a1e29ec","_uuid":"48435d85-c47f-4438-b829-1aee03325841","papermill":{"duration":0.009285,"end_time":"2023-01-04T20:50:34.261018","exception":false,"start_time":"2023-01-04T20:50:34.251733","status":"completed"},"tags":[]},"source":["For the time being we have a validity score of `78.4375%` therefore our prediction model is considred bad, we think it is because we're studying text in `arabic` CountVectorizer is unable to correctly preprocess text and tokenize it. We will try to use another vectorizer to see if the validity increases.\n","\n","### Building a pipeline\n","To simplify our training and prediction process we will build a new Pipeline"]},{"cell_type":"code","execution_count":13,"id":"2fb3b2d7","metadata":{"_cell_guid":"12907a38-023f-4149-81c8-a385f64f7186","_uuid":"6ccd3360-b20a-4c43-a609-ed1b718c5dd0","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T20:50:34.281663Z","iopub.status.busy":"2023-01-04T20:50:34.281223Z","iopub.status.idle":"2023-01-04T20:50:34.418249Z","shell.execute_reply":"2023-01-04T20:50:34.41689Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.150604,"end_time":"2023-01-04T20:50:34.4212","exception":false,"start_time":"2023-01-04T20:50:34.270596","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import SGDClassifier\n","\n","text_clf = Pipeline([\n","    ('vect', CountVectorizer()),\n","    ('tfidf', TfidfTransformer()),\n","    ('clf', SGDClassifier(loss='hinge', penalty=None,\n","                          alpha=1e-3, random_state=2,\n","                          max_iter=5, tol=None)),\n","])"]},{"cell_type":"markdown","id":"5c46fa60","metadata":{"_cell_guid":"2d6a1601-c2c5-46ab-93bf-4a1f7fcd57e6","_uuid":"0e26a857-855e-4042-bc5a-ff8758c44c09","papermill":{"duration":0.009279,"end_time":"2023-01-04T20:50:34.440019","exception":false,"start_time":"2023-01-04T20:50:34.43074","status":"completed"},"tags":[]},"source":["### Using SGDClassifierfrom sklearn.linear_model import SGDClassifier"]},{"cell_type":"code","execution_count":14,"id":"841c0561","metadata":{"_cell_guid":"38158940-ab9a-4410-9808-30357a62a7fb","_uuid":"279b3351-878e-45ac-b87e-0799205f4640","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T20:50:34.461437Z","iopub.status.busy":"2023-01-04T20:50:34.460098Z","iopub.status.idle":"2023-01-04T20:50:35.817263Z","shell.execute_reply":"2023-01-04T20:50:35.816149Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":1.370249,"end_time":"2023-01-04T20:50:35.819707","exception":false,"start_time":"2023-01-04T20:50:34.449458","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["0.7395833333333334"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["text_clf.fit(X_train.content, X_train.sentiment)\n","\n","predicted = text_clf.predict(X_test.content)\n","np.mean(predicted == X_test.sentiment)"]},{"cell_type":"markdown","id":"c34a222d","metadata":{"_cell_guid":"68a204ce-0b94-43ec-8546-423aac5564e5","_uuid":"6ddfb7ef-00f4-4870-b61b-411604437e54","papermill":{"duration":0.009005,"end_time":"2023-01-04T20:50:35.838241","exception":false,"start_time":"2023-01-04T20:50:35.829236","status":"completed"},"tags":[]},"source":["Now we will try to `Lemmetize` tweets, we believe this is the reason why our prediction model isn't producing better results.\n","We could try `Farasa` lemmetizer as it has a good reputation of outperforming other lemmetizers. But it uses an API and is not available as an imported library usable directly.\n","So for now we will just stick with ntlk's `ISRIStemmer`"]},{"cell_type":"code","execution_count":15,"id":"8c4da20d","metadata":{"_cell_guid":"ea1968d1-15bf-4de2-8fe0-181a0e161618","_uuid":"e8ed1312-35a1-4711-a76f-01e1c5296650","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T20:50:35.858851Z","iopub.status.busy":"2023-01-04T20:50:35.858394Z","iopub.status.idle":"2023-01-04T20:50:36.369928Z","shell.execute_reply":"2023-01-04T20:50:36.368599Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.525106,"end_time":"2023-01-04T20:50:36.372681","exception":false,"start_time":"2023-01-04T20:50:35.847575","status":"completed"},"tags":[]},"outputs":[],"source":["from nltk.stem.isri import ISRIStemmer\n","st = ISRIStemmer()"]},{"cell_type":"markdown","id":"fbca9dc3","metadata":{"_cell_guid":"9f278e4f-df5c-4a51-a45c-21447f11fdda","_uuid":"6f7b793d-49ca-48e1-9e0b-36a22d947f38","papermill":{"duration":0.009797,"end_time":"2023-01-04T20:50:36.392053","exception":false,"start_time":"2023-01-04T20:50:36.382256","status":"completed"},"tags":[]},"source":["A lemmetizer is also called a stemmer, NLTK has many stemmer solutions. We could try out each one of NLTK's stemmers and compare the results.\n","For now we will try out `ARLSTem Stemmer`"]},{"cell_type":"code","execution_count":16,"id":"005dbc3c","metadata":{"_cell_guid":"a53fabfb-3f1e-4226-886c-d5889bb8af5e","_uuid":"ca44c7c4-b9d4-459c-a9f7-83ced75b3e21","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T20:50:36.413079Z","iopub.status.busy":"2023-01-04T20:50:36.412669Z","iopub.status.idle":"2023-01-04T20:50:36.427686Z","shell.execute_reply":"2023-01-04T20:50:36.426432Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.028396,"end_time":"2023-01-04T20:50:36.430032","exception":false,"start_time":"2023-01-04T20:50:36.401636","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentiment</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>pos</td>\n","      <td>#الهلال_الاهلي فوز هلالي مهم الحمد لله 💙 زوران...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>pos</td>\n","      <td>صباحك خيرات ومسرات 🌸</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>pos</td>\n","      <td>#تأمل قال الله ﷻ :- _*​﴿بواد غير ذي زرع ﴾*_ 💫💫...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>pos</td>\n","      <td>😂😂 يا جدعان الرجاله اللي فوق ال دول خطر ع تويت...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>pos</td>\n","      <td>رساله صباحيه : 💛 اللهم اسألك التوفيق في جميع ا...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5763</th>\n","      <td>neg</td>\n","      <td>النوم وانت مكسور ده احساس غبي اللي هو مش قادر ...</td>\n","    </tr>\n","    <tr>\n","      <th>5764</th>\n","      <td>neg</td>\n","      <td>استشهاد_الامام_كاظم_الغيظ السلام على المعذب في...</td>\n","    </tr>\n","    <tr>\n","      <th>5765</th>\n","      <td>neg</td>\n","      <td>انا كنت اكل الصحن بكبره 😐</td>\n","    </tr>\n","    <tr>\n","      <th>5766</th>\n","      <td>neg</td>\n","      <td>قولوا لي ايش تشوفوا .. مع ملاحظة التلطف لأنه ا...</td>\n","    </tr>\n","    <tr>\n","      <th>5767</th>\n","      <td>neg</td>\n","      <td>✍ إذا أردت أن تعرف شيئا عني إسالني قبل أن تسأل...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>11520 rows × 2 columns</p>\n","</div>"],"text/plain":["     sentiment                                            content\n","0          pos  #الهلال_الاهلي فوز هلالي مهم الحمد لله 💙 زوران...\n","1          pos                               صباحك خيرات ومسرات 🌸\n","2          pos  #تأمل قال الله ﷻ :- _*​﴿بواد غير ذي زرع ﴾*_ 💫💫...\n","3          pos  😂😂 يا جدعان الرجاله اللي فوق ال دول خطر ع تويت...\n","4          pos  رساله صباحيه : 💛 اللهم اسألك التوفيق في جميع ا...\n","...        ...                                                ...\n","5763       neg  النوم وانت مكسور ده احساس غبي اللي هو مش قادر ...\n","5764       neg  استشهاد_الامام_كاظم_الغيظ السلام على المعذب في...\n","5765       neg                          انا كنت اكل الصحن بكبره 😐\n","5766       neg  قولوا لي ايش تشوفوا .. مع ملاحظة التلطف لأنه ا...\n","5767       neg  ✍ إذا أردت أن تعرف شيئا عني إسالني قبل أن تسأل...\n","\n","[11520 rows x 2 columns]"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.stem.arlstem import ARLSTem\n","\n","X_test"]},{"cell_type":"markdown","id":"4e5bfa6c","metadata":{"_cell_guid":"32fd7ae3-82bb-4873-8366-0fcd80965846","_uuid":"4415217a-9fe2-41e1-b88f-3dc74e334cf0","papermill":{"duration":0.009386,"end_time":"2023-01-04T20:50:36.449123","exception":false,"start_time":"2023-01-04T20:50:36.439737","status":"completed"},"tags":[]},"source":["# NLTK tokenization\n","    So we're still getting a very low validity score for our prediction model. It could be because we're using Arabic language and scikit learn is unable to correctly tokenize words, it could be that the words that have the same root arent' considred as the same token. I will try to preprocess the data first to turn each tweet text into tokens. \n","    A problem that could arise is not being able to detect the order of words. I'm not sure if NLtk will scramble the words or will they be in the same order for us to be able to use n-gram of words later on."]},{"cell_type":"code","execution_count":17,"id":"1f19cb5b","metadata":{"_cell_guid":"96aef751-938c-4c7d-a529-014ba033a901","_uuid":"4de24fee-81c6-40ea-941f-0b00169a6a6e","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T20:50:36.470162Z","iopub.status.busy":"2023-01-04T20:50:36.469743Z","iopub.status.idle":"2023-01-04T20:50:36.823955Z","shell.execute_reply":"2023-01-04T20:50:36.822668Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.368171,"end_time":"2023-01-04T20:50:36.826875","exception":false,"start_time":"2023-01-04T20:50:36.458704","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["0        [نحن, الذين, يتحول, كل, ما, نود, أن, نقوله, إل...\n","1        [وفي, النهاية, لن, يبقى, ٰ, معك, آحدإلا, من, ر...\n","2                                     [من, الخير, نفسه, 💛]\n","3        [#, زلزل_الملعب_نصرنا_بيلعب, كن, عالي, الهمه, ...\n","4        [الشيء, الوحيد, الذي, وصلوا, فيه, للعالمية, هو...\n","                               ...                        \n","22509    [كيف, ترى, أورانوس, لو, كان, يقع, مكان, القمر,...\n","22510                               [احسدك, على, الايم, 💔]\n","22511                       [لأول, مرة, ما, بنكون, سوا, 💔]\n","22512                             [بقله, ليش, يا, واطي, 🤔]\n","22513    [قد, طال, صبري, في, النوى, إذ, تركتني, كئيبا, ...\n","Name: content, Length: 45275, dtype: object"]},"metadata":{},"output_type":"display_data"}],"source":["from nltk.tokenize import wordpunct_tokenize,word_tokenize\n","\n","X_train = X_train.content.apply(wordpunct_tokenize)\n","display(X_train)"]},{"cell_type":"markdown","id":"9183a637","metadata":{"_cell_guid":"015c2d51-da19-4e95-a093-cd475b67125b","_uuid":"35301da8-3cd9-46cb-b859-52045c8d4cf0","papermill":{"duration":0.00951,"end_time":"2023-01-04T20:50:36.846301","exception":false,"start_time":"2023-01-04T20:50:36.836791","status":"completed"},"tags":[]},"source":["arabic isn't supported by `nltk` so we'll use some other third party library to tokenize our te"]},{"cell_type":"code","execution_count":18,"id":"7e41c3e0","metadata":{"_cell_guid":"76f933a7-228b-4b26-bdc6-8cbdf1fbd3fd","_uuid":"5d06e677-34eb-4331-9fa0-1234bd40fc62","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T20:50:36.868386Z","iopub.status.busy":"2023-01-04T20:50:36.867143Z","iopub.status.idle":"2023-01-04T20:50:36.871545Z","shell.execute_reply":"2023-01-04T20:50:36.870792Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.017662,"end_time":"2023-01-04T20:50:36.873702","exception":false,"start_time":"2023-01-04T20:50:36.85604","status":"completed"},"tags":[]},"outputs":[],"source":["# import tkseem as tk\n","\n","# tkseem_tokenizer = tk.WordTokenizer()\n","# X_train.content.apply(tkseem_tokenizer.tokenize)"]},{"cell_type":"markdown","id":"8041a35f","metadata":{"_cell_guid":"f36c4f68-7a1b-483b-b4b3-45c7174ab3c2","_uuid":"20577384-4ae0-4530-9802-78167ed0ad6b","papermill":{"duration":0.010436,"end_time":"2023-01-04T20:50:36.894245","exception":false,"start_time":"2023-01-04T20:50:36.883809","status":"completed"},"tags":[]},"source":["### Parameter tuning using grid search"]},{"cell_type":"code","execution_count":19,"id":"e23baa8f","metadata":{"_cell_guid":"94b5d231-80d4-4a38-8955-53d142cc0be7","_uuid":"de14a436-17c2-4e5a-9eec-705f85f65406","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T20:50:36.916772Z","iopub.status.busy":"2023-01-04T20:50:36.915511Z","iopub.status.idle":"2023-01-04T20:50:36.920722Z","shell.execute_reply":"2023-01-04T20:50:36.919663Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.019081,"end_time":"2023-01-04T20:50:36.923306","exception":false,"start_time":"2023-01-04T20:50:36.904225","status":"completed"},"tags":[]},"outputs":[],"source":["# from sklearn.model_selection import GridSearchCV\n","\n","# parameters = {\n","#     'vect__ngram_range': [(1, 1), (1, 2)],\n","#     'tfidf__use_idf': (True, False),\n","#     'clf__alpha': (1e-2, 1e-3),\n","# }\n","\n","# gs_clf = GridSearchCV(text_clf, parameters, cv=5, n_jobs=-1)\n","# gs_clf = gs_clf.fit(X_train.content[:400], X_train.sentiment[:400])"]},{"cell_type":"markdown","id":"22c23807","metadata":{"_cell_guid":"681e47fe-a024-4c79-b2ba-76c088564b66","_uuid":"29fb8111-45f0-480c-8a28-665dd790f1e3","papermill":{"duration":0.010025,"end_time":"2023-01-04T20:50:36.943829","exception":false,"start_time":"2023-01-04T20:50:36.933804","status":"completed"},"tags":[]},"source":["## Extracting features from tweets data\n","    After loading the data the next step is to try and extract features from our tweets corpus."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":15.803919,"end_time":"2023-01-04T20:50:37.876552","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-01-04T20:50:22.072633","version":"2.3.4"}},"nbformat":4,"nbformat_minor":5}