{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/fayssalelansari/notebookc111a6c6b8?scriptVersionId=115513992\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","execution_count":1,"id":"d08f8f80","metadata":{"_cell_guid":"4c1ef890-63d4-444a-b58a-f41939d9e62f","_uuid":"87d30fb1-988f-4483-b632-02009c36d444","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:34.729193Z","iopub.status.busy":"2023-01-04T21:52:34.728733Z","iopub.status.idle":"2023-01-04T21:52:34.738559Z","shell.execute_reply":"2023-01-04T21:52:34.737858Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.021973,"end_time":"2023-01-04T21:52:34.740543","exception":false,"start_time":"2023-01-04T21:52:34.71857","status":"completed"},"tags":[]},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","id":"aa8a5216","metadata":{"papermill":{"duration":0.007515,"end_time":"2023-01-04T21:52:34.756337","exception":false,"start_time":"2023-01-04T21:52:34.748822","status":"completed"},"tags":[]},"source":["# TASKS\n","* [ ] PREPROCESS DATA\n","* [ ] STEM ARABIC WORDS\n","* [ ] MAKE PIPELINE"]},{"cell_type":"markdown","id":"8d3922b3","metadata":{"_cell_guid":"0380ce88-a279-42cb-b40b-c7476b152bd0","_uuid":"ec3e36fc-4064-4b35-951d-0b595aa23b14","papermill":{"duration":0.007495,"end_time":"2023-01-04T21:52:34.771579","exception":false,"start_time":"2023-01-04T21:52:34.764084","status":"completed"},"tags":[]},"source":["### constants"]},{"cell_type":"code","execution_count":2,"id":"df4cff84","metadata":{"_cell_guid":"2bd62210-dad9-4e66-b738-4660c96b0fc9","_uuid":"7bbff164-5a1e-4f63-bd19-aa61814a168e","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:34.788503Z","iopub.status.busy":"2023-01-04T21:52:34.788164Z","iopub.status.idle":"2023-01-04T21:52:34.792739Z","shell.execute_reply":"2023-01-04T21:52:34.791669Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.015206,"end_time":"2023-01-04T21:52:34.794579","exception":false,"start_time":"2023-01-04T21:52:34.779373","status":"completed"},"tags":[]},"outputs":[],"source":["enc = \"UTF-8\""]},{"cell_type":"markdown","id":"cd63bc45","metadata":{"_cell_guid":"6ec183c9-8c80-45e5-9e8f-9c9681f60b13","_uuid":"d9cc1119-139a-4382-8c4e-8ac304c69b41","papermill":{"duration":0.007422,"end_time":"2023-01-04T21:52:34.811102","exception":false,"start_time":"2023-01-04T21:52:34.80368","status":"completed"},"tags":[]},"source":["### model files"]},{"cell_type":"code","execution_count":3,"id":"4d69b5fc","metadata":{"_cell_guid":"9e978d47-5887-4d41-91fb-63ef6ffd63b0","_uuid":"45b66073-ec3a-4a74-9cf1-441edc87b64c","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:34.828138Z","iopub.status.busy":"2023-01-04T21:52:34.827633Z","iopub.status.idle":"2023-01-04T21:52:34.837861Z","shell.execute_reply":"2023-01-04T21:52:34.836782Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.021067,"end_time":"2023-01-04T21:52:34.839884","exception":false,"start_time":"2023-01-04T21:52:34.818817","status":"completed"},"tags":[]},"outputs":[],"source":["import enum\n","\n","class Tag(enum.Enum):\n","    Neutral = \"NEUTRAL\"\n","    Positive = \"POSITIVE\"\n","    Negative = \"NEGATIVE\"\n","    Not_defined = \"NOT_DEFINED\"\n","\n","class Tweet:\n","    def __init__(self, text, real_tag=Tag.Not_defined, given_tag=Tag.Not_defined):\n","        self.text = text\n","        self.real_tag = real_tag\n","        self.given_tag = given_tag\n","\n","    def __str__(self):\n","        txt = self.text\n","        return txt\n","\n","    def __repr__(self):\n","        txt = self.text \n","        return txt"]},{"cell_type":"markdown","id":"59fc45b5","metadata":{"_cell_guid":"ca0e9aec-e672-4faa-9b5f-8d43ee952691","_uuid":"c2c6de7b-7687-4684-8b7e-a8083b91d10b","papermill":{"duration":0.007646,"end_time":"2023-01-04T21:52:34.855531","exception":false,"start_time":"2023-01-04T21:52:34.847885","status":"completed"},"tags":[]},"source":["## Loading data\n","### Using our own method and model files\n","    In this section we will use our own definition of a function that will load the files by traversing the folders containing the positive and the negative tweets\n","    We should compare this method to simply reading the `.tsv` files (which should be faster)."]},{"cell_type":"code","execution_count":4,"id":"a4f51a90","metadata":{"_cell_guid":"ca64d88f-05f5-4080-a33f-5fbf92fc36e5","_uuid":"9b2d8e93-cc11-4c21-864b-f0f2a05339fa","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:34.872889Z","iopub.status.busy":"2023-01-04T21:52:34.872556Z","iopub.status.idle":"2023-01-04T21:52:34.877416Z","shell.execute_reply":"2023-01-04T21:52:34.876466Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.01577,"end_time":"2023-01-04T21:52:34.879197","exception":false,"start_time":"2023-01-04T21:52:34.863427","status":"completed"},"tags":[]},"outputs":[],"source":["import os\n","import pathlib\n","\n","# # current_path = pathlib.Path(__file__).parent.resolve()\n","# POS_COUNT = 29848\n","# NEG_COUNT = 28901\n","# tweets = []\n","\n","# def import_data():\n","#     for i in range(POS_COUNT):\n","#         text = \"\"\n","#         with open(\"/kaggle/input/arabic-sentiment-twitter-corpus/arabic_tweets/pos/\" + str(i) + '.txt', encoding=enc) as f:\n","#             for line in f:\n","#                 text += line\n","#         tweets.append(Tweet(text, Tag.Positive))\n","#     for i in range(NEG_COUNT):\n","#         text = \"\"\n","#         with open(\"/kaggle/input/arabic-sentiment-twitter-corpus/arabic_tweets/neg/\" + str(i) + '.txt', encoding=enc) as f:\n","#             for line in f:\n","#                 text += line\n","#         tweets.append(Tweet(text, Tag.Negative))\n","        \n","# import_data()"]},{"cell_type":"code","execution_count":5,"id":"85c90e0d","metadata":{"_cell_guid":"5a3260da-95d6-46c9-b0a8-228f471bca48","_uuid":"002d715e-41d8-4714-8737-a9909c57f4db","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:34.896979Z","iopub.status.busy":"2023-01-04T21:52:34.895953Z","iopub.status.idle":"2023-01-04T21:52:34.900347Z","shell.execute_reply":"2023-01-04T21:52:34.899535Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.015339,"end_time":"2023-01-04T21:52:34.902509","exception":false,"start_time":"2023-01-04T21:52:34.88717","status":"completed"},"tags":[]},"outputs":[],"source":["# print(tweets)"]},{"cell_type":"markdown","id":"3cfce95f","metadata":{"_cell_guid":"5fab9c36-56cb-4c8f-8db0-e91c821be46d","_uuid":"b3302efc-f1e9-4003-b14b-7efe6dff8091","papermill":{"duration":0.007576,"end_time":"2023-01-04T21:52:34.918573","exception":false,"start_time":"2023-01-04T21:52:34.910997","status":"completed"},"tags":[]},"source":["### Using our own method and reading from `tsv` files directly\n","    same as the previous step we shall populate a list of tweets[Tweet] with our data.\n","    After deep thought it is better to use a matrix instead of classes. We shall use pandas to represent our dataset."]},{"cell_type":"code","execution_count":6,"id":"c0d8d1ff","metadata":{"_cell_guid":"f4d75feb-2e8a-4357-b398-b76b762d188a","_uuid":"d1e8751f-ad65-4fd7-8025-9cc982ccd09b","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:34.936517Z","iopub.status.busy":"2023-01-04T21:52:34.935311Z","iopub.status.idle":"2023-01-04T21:52:35.181524Z","shell.execute_reply":"2023-01-04T21:52:35.180716Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.25744,"end_time":"2023-01-04T21:52:35.183828","exception":false,"start_time":"2023-01-04T21:52:34.926388","status":"completed"},"tags":[]},"outputs":[],"source":["column_names = [\"sentiment\", \"content\"]\n","train_tweets_positive = pd.read_table(\"/kaggle/input/arabic-sentiment-twitter-corpus/train_Arabic_tweets_positive_20190413.tsv\", names=column_names)\n","train_tweets_negative = pd.read_table(\"/kaggle/input/arabic-sentiment-twitter-corpus/train_Arabic_tweets_negative_20190413.tsv\", names=column_names)\n","test_tweets_positive = pd.read_table(\"/kaggle/input/arabic-sentiment-twitter-corpus/test_Arabic_tweets_positive_20190413.tsv\", names=column_names)\n","test_tweets_negative = pd.read_table(\"/kaggle/input/arabic-sentiment-twitter-corpus/test_Arabic_tweets_negative_20190413.tsv\", names=column_names)"]},{"cell_type":"code","execution_count":7,"id":"5862ff07","metadata":{"_cell_guid":"4cb90bf1-f45d-4c2d-8e22-19dda25c3ef2","_uuid":"49a5e898-0cc0-40e7-aa9f-ca9624957948","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:35.201865Z","iopub.status.busy":"2023-01-04T21:52:35.201365Z","iopub.status.idle":"2023-01-04T21:52:35.226591Z","shell.execute_reply":"2023-01-04T21:52:35.225349Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.036354,"end_time":"2023-01-04T21:52:35.228571","exception":false,"start_time":"2023-01-04T21:52:35.192217","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentiment</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>pos</td>\n","      <td>نحن الذين يتحول كل ما نود أن نقوله إلى دعاء لل...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>pos</td>\n","      <td>وفي النهاية لن يبقىٰ معك آحدإلا من رأىٰ الجمال...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>pos</td>\n","      <td>من الخير نفسه 💛</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>pos</td>\n","      <td>#زلزل_الملعب_نصرنا_بيلعب كن عالي الهمه ولا ترض...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>pos</td>\n","      <td>الشيء الوحيد الذي وصلوا فيه للعالمية هو : المس...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>22509</th>\n","      <td>neg</td>\n","      <td>كيف ترى أورانوس لو كان يقع مكان القمر ؟ 💙💙 كوك...</td>\n","    </tr>\n","    <tr>\n","      <th>22510</th>\n","      <td>neg</td>\n","      <td>احسدك على الايم 💔</td>\n","    </tr>\n","    <tr>\n","      <th>22511</th>\n","      <td>neg</td>\n","      <td>لأول مرة ما بنكون سوا 💔</td>\n","    </tr>\n","    <tr>\n","      <th>22512</th>\n","      <td>neg</td>\n","      <td>بقله ليش يا واطي 🤔</td>\n","    </tr>\n","    <tr>\n","      <th>22513</th>\n","      <td>neg</td>\n","      <td>قد طال صبري في النوى إذ تركتني كئيبا ؛ غريبا ب...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>45275 rows × 2 columns</p>\n","</div>"],"text/plain":["      sentiment                                            content\n","0           pos  نحن الذين يتحول كل ما نود أن نقوله إلى دعاء لل...\n","1           pos  وفي النهاية لن يبقىٰ معك آحدإلا من رأىٰ الجمال...\n","2           pos                                    من الخير نفسه 💛\n","3           pos  #زلزل_الملعب_نصرنا_بيلعب كن عالي الهمه ولا ترض...\n","4           pos  الشيء الوحيد الذي وصلوا فيه للعالمية هو : المس...\n","...         ...                                                ...\n","22509       neg  كيف ترى أورانوس لو كان يقع مكان القمر ؟ 💙💙 كوك...\n","22510       neg                                  احسدك على الايم 💔\n","22511       neg                            لأول مرة ما بنكون سوا 💔\n","22512       neg                                 بقله ليش يا واطي 🤔\n","22513       neg  قد طال صبري في النوى إذ تركتني كئيبا ؛ غريبا ب...\n","\n","[45275 rows x 2 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["X_train = pd.concat([train_tweets_positive,train_tweets_negative])\n","\n","from IPython.display import display, HTML\n","display(X_train)"]},{"cell_type":"markdown","id":"5d7b0ea2","metadata":{"papermill":{"duration":0.008248,"end_time":"2023-01-04T21:52:35.245265","exception":false,"start_time":"2023-01-04T21:52:35.237017","status":"completed"},"tags":[]},"source":["## Preprocessing dataset\n","Now we need to remove special characters including emoticones since apparently there are many emoticones in arabic tweets, we also need to remove punctuation and `tashkil` which is special character above letters to determine how they are pronounced\n","\n","So all that needs to be removed is:\n","* special characters and emoticones\n","* stop words\n","* ponctuation"]},{"cell_type":"code","execution_count":8,"id":"89b9246c","metadata":{"execution":{"iopub.execute_input":"2023-01-04T21:52:35.262914Z","iopub.status.busy":"2023-01-04T21:52:35.262567Z","iopub.status.idle":"2023-01-04T21:52:52.492517Z","shell.execute_reply":"2023-01-04T21:52:52.491555Z"},"papermill":{"duration":17.240986,"end_time":"2023-01-04T21:52:52.494306","exception":false,"start_time":"2023-01-04T21:52:35.25332","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["0        نحن الذين يتحول كل ما نود أن نقوله إلى دعاء لل...\n","1        وفي النهاية لن يبقى معك آحدإلا من رأى الجمال ف...\n","2                                           من الخير نفسه \n","3        زلزل_الملعب_نصرنا_بيلعب كن عالي الهمه ولا ترضى...\n","4        الشيء الوحيد الذي وصلوا فيه للعالمية هو  المسي...\n","                               ...                        \n","22509    كيف ترى أورانوس لو كان يقع مكان القمر   كوكب ب...\n","22510                                     احسدك على الايم \n","22511                               لأول مرة ما بنكون سوا \n","22512                                    بقله ليش يا واطي \n","22513    قد طال صبري في النوى إذ تركتني كئيبا  غريبا با...\n","Name: content, Length: 45275, dtype: object"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.corpus import stopwords\n","\n","def remove_stop_words(text):\n","    stop_words = set(stopwords.words(\"arabic\"))\n","    text = text.split()\n","    return \" \".join([word for word in text if word not in stop_words])\n","\n","import re\n","\n","def remove_emojis(text):\n","    emoji_pattern = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                           \"]+\", flags=re.UNICODE)\n","    return emoji_pattern.sub(r'', text)\n","\n","def remove_punctuation(text):\n","    return re.sub(r'[^\\w\\s]','',text)\n","\n","X_train.content.apply(remove_stop_words)\n","X_train.content.apply(remove_emojis)\n","X_train.content.apply(remove_punctuation)"]},{"cell_type":"markdown","id":"ef4ee65d","metadata":{"_cell_guid":"bc011b93-5ee3-47b1-b3e9-f66910d47ee1","_uuid":"f34fe2b0-69d1-4fc4-9154-2b23a28bfb66","papermill":{"duration":0.008002,"end_time":"2023-01-04T21:52:52.510866","exception":false,"start_time":"2023-01-04T21:52:52.502864","status":"completed"},"tags":[]},"source":["## Preprocessing dataset\n","Now we need to remove special characters## Tokenizing dataset\n","We need to write our tweets text as a feature-term dataframe (feature-term matrix). When using countvectorizer there is no need to preprocess the data, as it already removes stop words and speical characters.\n","\n","### Using CountVectorizer"]},{"cell_type":"code","execution_count":9,"id":"8e248f07","metadata":{"_cell_guid":"b5262f3a-9aa2-4b93-a6e7-8949ab7a9221","_uuid":"e3006a58-5728-4895-8ae6-36578619940b","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:52.529159Z","iopub.status.busy":"2023-01-04T21:52:52.528799Z","iopub.status.idle":"2023-01-04T21:52:53.219354Z","shell.execute_reply":"2023-01-04T21:52:53.218101Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.701991,"end_time":"2023-01-04T21:52:53.221361","exception":false,"start_time":"2023-01-04T21:52:52.51937","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(45275, 70856)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","count_vect = CountVectorizer()\n","X_train_counts = count_vect.fit_transform(X_train.content)\n","X_train_counts.shape"]},{"cell_type":"markdown","id":"a93a0be4","metadata":{"_cell_guid":"ca352397-dcdb-43c2-8e25-76fdd9f02ac2","_uuid":"3db34197-2475-4e45-a761-8e41e73a094b","papermill":{"duration":0.007899,"end_time":"2023-01-04T21:52:53.237803","exception":false,"start_time":"2023-01-04T21:52:53.229904","status":"completed"},"tags":[]},"source":["### Using TfidfTransformer"]},{"cell_type":"code","execution_count":10,"id":"1f8a1ffd","metadata":{"_cell_guid":"c12ed70f-ac79-4671-b21f-3c1259e30b0e","_uuid":"c70c0eda-68b1-4189-93d6-2030ab82e9e8","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:53.25627Z","iopub.status.busy":"2023-01-04T21:52:53.255916Z","iopub.status.idle":"2023-01-04T21:52:53.32066Z","shell.execute_reply":"2023-01-04T21:52:53.319725Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.076465,"end_time":"2023-01-04T21:52:53.322661","exception":false,"start_time":"2023-01-04T21:52:53.246196","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(45275, 70856)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_extraction.text import TfidfTransformer\n","tfidf_transformer = TfidfTransformer()\n","X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n","X_train_tfidf.shape"]},{"cell_type":"markdown","id":"03f3d2f1","metadata":{"_cell_guid":"e4fb18f7-951e-4373-ae73-345b9a60cf7f","_uuid":"15a5ad41-c2f5-4594-8384-0eb6d899cb5a","papermill":{"duration":0.008058,"end_time":"2023-01-04T21:52:53.339295","exception":false,"start_time":"2023-01-04T21:52:53.331237","status":"completed"},"tags":[]},"source":["## Training a classifier\n","### Using naïve Bayes"]},{"cell_type":"code","execution_count":11,"id":"db030f51","metadata":{"_cell_guid":"645996d7-320d-4b51-b534-b142fc545514","_uuid":"4dd3088e-6f46-44eb-a7b7-d875a8af6e7f","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:53.357811Z","iopub.status.busy":"2023-01-04T21:52:53.357428Z","iopub.status.idle":"2023-01-04T21:52:53.461975Z","shell.execute_reply":"2023-01-04T21:52:53.461317Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.116371,"end_time":"2023-01-04T21:52:53.464042","exception":false,"start_time":"2023-01-04T21:52:53.347671","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.naive_bayes import MultinomialNB\n","clf = MultinomialNB().fit(X_train_tfidf, X_train.sentiment)"]},{"cell_type":"markdown","id":"84fb1d04","metadata":{"_cell_guid":"6dc0f87e-66d4-4973-9f11-768bdaf82254","_uuid":"dde58efe-9130-4465-bfe6-d09719bc25ef","papermill":{"duration":0.008336,"end_time":"2023-01-04T21:52:53.481047","exception":false,"start_time":"2023-01-04T21:52:53.472711","status":"completed"},"tags":[]},"source":["We will now use our testing dataset (we combine the negative and positivie tweests into one pandas dataframe), then we will call transfrom without calling fit in order to make a prediction."]},{"cell_type":"code","execution_count":12,"id":"9c0d21bc","metadata":{"_cell_guid":"2bc882d9-64f3-4084-839e-c15f1bf5f8f6","_uuid":"fe2ffab8-6e91-49ff-b6fd-0b558c7a3035","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:53.501195Z","iopub.status.busy":"2023-01-04T21:52:53.499749Z","iopub.status.idle":"2023-01-04T21:52:53.66366Z","shell.execute_reply":"2023-01-04T21:52:53.662679Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.175742,"end_time":"2023-01-04T21:52:53.666008","exception":false,"start_time":"2023-01-04T21:52:53.490266","status":"completed"},"tags":[]},"outputs":[],"source":["X_test = pd.concat([test_tweets_positive,test_tweets_negative])\n","\n","X_test_counts = count_vect.transform(X_test.content)\n","X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n","\n","predicted = clf.predict(X_test_tfidf)\n","\n","# for tweet, sentiment in zip(X_test.content, predicted):\n","#     print('%r => %s' % (tweet, sentiment))"]},{"cell_type":"markdown","id":"f57c7b72","metadata":{"_cell_guid":"acb2606b-96b4-4d9f-9a71-cb155b57b87b","_uuid":"19c3b799-f686-4d8e-89a9-e1c5eb3b5c54","papermill":{"duration":0.007882,"end_time":"2023-01-04T21:52:53.682549","exception":false,"start_time":"2023-01-04T21:52:53.674667","status":"completed"},"tags":[]},"source":["## Calculating Accuracy Score\n","Now we need to get a percentage of the accuracy of our model, we have a list of predicted sentiment and a list of the actual sentiment. Whenever predicted sentiment is different of the actual one we will increment a counter, after going through all the tweets we will divide but the total number of tweets to get a percentage of the wrong predictions, to get the percentage of the right prediction all we need to do is subtract the calculated score from 100%."]},{"cell_type":"code","execution_count":13,"id":"390668e8","metadata":{"_cell_guid":"19c26993-64cb-49bb-b033-f7186a705740","_uuid":"afb6c115-b16d-4fb9-a060-ef68a1aa2bdf","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:53.700216Z","iopub.status.busy":"2023-01-04T21:52:53.699854Z","iopub.status.idle":"2023-01-04T21:52:53.711679Z","shell.execute_reply":"2023-01-04T21:52:53.710229Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.023174,"end_time":"2023-01-04T21:52:53.713834","exception":false,"start_time":"2023-01-04T21:52:53.69066","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["validity score: 78.4375%\n"]}],"source":["wrong_predictions = 0\n","validity_score = 0\n","for predicted_sentiment, actual_sentiment in zip(predicted, X_test.sentiment):\n","    if predicted_sentiment != actual_sentiment:\n","        wrong_predictions += 1\n","wrong_predictions_percentage = wrong_predictions / len(X_test.sentiment)\n","validity_score = 1 - wrong_predictions_percentage\n","print(\"validity score: \" + str(validity_score*100) + \"%\")"]},{"cell_type":"markdown","id":"7c84c70b","metadata":{"_cell_guid":"6b21ef9a-866c-4b88-8577-164f8a1e29ec","_uuid":"48435d85-c47f-4438-b829-1aee03325841","papermill":{"duration":0.008094,"end_time":"2023-01-04T21:52:53.730377","exception":false,"start_time":"2023-01-04T21:52:53.722283","status":"completed"},"tags":[]},"source":["For the time being we have a validity score of `78.4375%` therefore our prediction model is considred bad, we think it is because we're studying text in `arabic` CountVectorizer is unable to correctly preprocess text and tokenize it. We will try to use another vectorizer to see if the validity increases.\n","\n","### Building a pipeline\n","To simplify our training and prediction process we will build a new Pipeline"]},{"cell_type":"code","execution_count":14,"id":"6a6f440f","metadata":{"_cell_guid":"12907a38-023f-4149-81c8-a385f64f7186","_uuid":"6ccd3360-b20a-4c43-a609-ed1b718c5dd0","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:53.748521Z","iopub.status.busy":"2023-01-04T21:52:53.748169Z","iopub.status.idle":"2023-01-04T21:52:53.75576Z","shell.execute_reply":"2023-01-04T21:52:53.755046Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.018679,"end_time":"2023-01-04T21:52:53.757431","exception":false,"start_time":"2023-01-04T21:52:53.738752","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import SGDClassifier\n","\n","text_clf = Pipeline([\n","    ('vect', CountVectorizer()),\n","    ('tfidf', TfidfTransformer()),\n","    ('clf', SGDClassifier(loss='hinge', penalty=None,\n","                          alpha=1e-3, random_state=2,\n","                          max_iter=5, tol=None)),\n","])"]},{"cell_type":"markdown","id":"54ba5e24","metadata":{"_cell_guid":"2d6a1601-c2c5-46ab-93bf-4a1f7fcd57e6","_uuid":"0e26a857-855e-4042-bc5a-ff8758c44c09","papermill":{"duration":0.008226,"end_time":"2023-01-04T21:52:53.774338","exception":false,"start_time":"2023-01-04T21:52:53.766112","status":"completed"},"tags":[]},"source":["### Using SGDClassifierfrom sklearn.linear_model import SGDClassifier"]},{"cell_type":"code","execution_count":15,"id":"8cb6c323","metadata":{"_cell_guid":"38158940-ab9a-4410-9808-30357a62a7fb","_uuid":"279b3351-878e-45ac-b87e-0799205f4640","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:53.792844Z","iopub.status.busy":"2023-01-04T21:52:53.792312Z","iopub.status.idle":"2023-01-04T21:52:54.844498Z","shell.execute_reply":"2023-01-04T21:52:54.84347Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":1.063944,"end_time":"2023-01-04T21:52:54.846764","exception":false,"start_time":"2023-01-04T21:52:53.78282","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["0.7395833333333334"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["text_clf.fit(X_train.content, X_train.sentiment)\n","\n","predicted = text_clf.predict(X_test.content)\n","np.mean(predicted == X_test.sentiment)"]},{"cell_type":"markdown","id":"f7548eea","metadata":{"_cell_guid":"68a204ce-0b94-43ec-8546-423aac5564e5","_uuid":"6ddfb7ef-00f4-4870-b61b-411604437e54","papermill":{"duration":0.008162,"end_time":"2023-01-04T21:52:54.863905","exception":false,"start_time":"2023-01-04T21:52:54.855743","status":"completed"},"tags":[]},"source":["Now we will try to `Lemmetize` tweets, we believe this is the reason why our prediction model isn't producing better results.\n","We could try `Farasa` lemmetizer as it has a good reputation of outperforming other lemmetizers. But it uses an API and is not available as an imported library usable directly.\n","So for now we will just stick with ntlk's `ISRIStemmer`"]},{"cell_type":"code","execution_count":16,"id":"7ba61755","metadata":{"_cell_guid":"ea1968d1-15bf-4de2-8fe0-181a0e161618","_uuid":"e8ed1312-35a1-4711-a76f-01e1c5296650","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:54.882864Z","iopub.status.busy":"2023-01-04T21:52:54.882314Z","iopub.status.idle":"2023-01-04T21:52:54.886896Z","shell.execute_reply":"2023-01-04T21:52:54.885808Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.016391,"end_time":"2023-01-04T21:52:54.888794","exception":false,"start_time":"2023-01-04T21:52:54.872403","status":"completed"},"tags":[]},"outputs":[],"source":["from nltk.stem.isri import ISRIStemmer\n","st = ISRIStemmer()"]},{"cell_type":"markdown","id":"0d7ba666","metadata":{"_cell_guid":"9f278e4f-df5c-4a51-a45c-21447f11fdda","_uuid":"6f7b793d-49ca-48e1-9e0b-36a22d947f38","papermill":{"duration":0.008413,"end_time":"2023-01-04T21:52:54.906246","exception":false,"start_time":"2023-01-04T21:52:54.897833","status":"completed"},"tags":[]},"source":["A lemmetizer is also called a stemmer, NLTK has many stemmer solutions. We could try out each one of NLTK's stemmers and compare the results.\n","For now we will try out `ARLSTem Stemmer`\n","\n","* THE NEW PROCESS IS NOW:\n","1. tokenize\n","2. stem\n","3. remove stop words"]},{"cell_type":"code","execution_count":17,"id":"4ff75695","metadata":{"_cell_guid":"a53fabfb-3f1e-4226-886c-d5889bb8af5e","_uuid":"ca44c7c4-b9d4-459c-a9f7-83ced75b3e21","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:54.925333Z","iopub.status.busy":"2023-01-04T21:52:54.924792Z","iopub.status.idle":"2023-01-04T21:52:55.114374Z","shell.execute_reply":"2023-01-04T21:52:55.113224Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.201668,"end_time":"2023-01-04T21:52:55.116607","exception":false,"start_time":"2023-01-04T21:52:54.914939","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["0       #الهلال_الاهلي فوز هلالي مهم الحمد لله 💙 زوران...\n","1                                    صباحك خيرات ومسرات 🌸\n","2       #تامل قال الله ﷻ :- _*​﴿بواد غير ذي زرع ﴾*_ 💫💫...\n","3       😂😂 يا جدعان الرجاله اللي فوق ال دول خطر ع تويت...\n","4       رساله صباحيه : 💛 اللهم اسالك التوفيق في جميع ا...\n","                              ...                        \n","5763    نوم وانت مكسور ده احساس غبي اللي هو مش قادر تن...\n","5764    ستشهاد_الامام_كاظم_الغيظ السلام علي المعذب في ...\n","5765                             ان كنت اكل الصحن بكبره 😐\n","5766    قولوا لي ايش تشوفوا .. مع ملاحظة التلطف لانه ا...\n","5767    ✍ اذا اردت ان تعرف شيئا عني اسالني قبل ان تسال...\n","Name: content, Length: 11520, dtype: object"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.stem.arlstem import ARLSTem\n","\n","stemmer = ARLSTem()\n","X_test.content.apply(stemmer.stem)"]},{"cell_type":"markdown","id":"89437569","metadata":{"_cell_guid":"32fd7ae3-82bb-4873-8366-0fcd80965846","_uuid":"4415217a-9fe2-41e1-b88f-3dc74e334cf0","papermill":{"duration":0.00853,"end_time":"2023-01-04T21:52:55.134278","exception":false,"start_time":"2023-01-04T21:52:55.125748","status":"completed"},"tags":[]},"source":["# NLTK tokenization\n","    So we're still getting a very low validity score for our prediction model. It could be because we're using Arabic language and scikit learn is unable to correctly tokenize words, it could be that the words that have the same root arent' considred as the same token. I will try to preprocess the data first to turn each tweet text into tokens. \n","    A problem that could arise is not being able to detect the order of words. I'm not sure if NLtk will scramble the words or will they be in the same order for us to be able to use n-gram of words later on."]},{"cell_type":"code","execution_count":18,"id":"3519ba39","metadata":{"_cell_guid":"96aef751-938c-4c7d-a529-014ba033a901","_uuid":"4de24fee-81c6-40ea-941f-0b00169a6a6e","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:55.153748Z","iopub.status.busy":"2023-01-04T21:52:55.153365Z","iopub.status.idle":"2023-01-04T21:52:55.545182Z","shell.execute_reply":"2023-01-04T21:52:55.544096Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.404059,"end_time":"2023-01-04T21:52:55.547324","exception":false,"start_time":"2023-01-04T21:52:55.143265","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["0        [نحن, الذين, يتحول, كل, ما, نود, أن, نقوله, إل...\n","1        [وفي, النهاية, لن, يبقى, ٰ, معك, آحدإلا, من, ر...\n","2                                     [من, الخير, نفسه, 💛]\n","3        [#, زلزل_الملعب_نصرنا_بيلعب, كن, عالي, الهمه, ...\n","4        [الشيء, الوحيد, الذي, وصلوا, فيه, للعالمية, هو...\n","                               ...                        \n","22509    [كيف, ترى, أورانوس, لو, كان, يقع, مكان, القمر,...\n","22510                               [احسدك, على, الايم, 💔]\n","22511                       [لأول, مرة, ما, بنكون, سوا, 💔]\n","22512                             [بقله, ليش, يا, واطي, 🤔]\n","22513    [قد, طال, صبري, في, النوى, إذ, تركتني, كئيبا, ...\n","Name: content, Length: 45275, dtype: object"]},"metadata":{},"output_type":"display_data"}],"source":["from nltk.tokenize import wordpunct_tokenize,word_tokenize\n","\n","X_train = X_train.content.apply(wordpunct_tokenize)\n","display(X_train)"]},{"cell_type":"markdown","id":"014e0593","metadata":{"_cell_guid":"015c2d51-da19-4e95-a093-cd475b67125b","_uuid":"35301da8-3cd9-46cb-b859-52045c8d4cf0","papermill":{"duration":0.008363,"end_time":"2023-01-04T21:52:55.564742","exception":false,"start_time":"2023-01-04T21:52:55.556379","status":"completed"},"tags":[]},"source":["arabic isn't supported by `nltk` so we'll use some other third party library to tokenize our te"]},{"cell_type":"code","execution_count":19,"id":"52a6661d","metadata":{"_cell_guid":"76f933a7-228b-4b26-bdc6-8cbdf1fbd3fd","_uuid":"5d06e677-34eb-4331-9fa0-1234bd40fc62","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:55.5835Z","iopub.status.busy":"2023-01-04T21:52:55.583178Z","iopub.status.idle":"2023-01-04T21:52:55.587755Z","shell.execute_reply":"2023-01-04T21:52:55.586493Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.016195,"end_time":"2023-01-04T21:52:55.589632","exception":false,"start_time":"2023-01-04T21:52:55.573437","status":"completed"},"tags":[]},"outputs":[],"source":["# import tkseem as tk\n","\n","# tkseem_tokenizer = tk.WordTokenizer()\n","# X_train.content.apply(tkseem_tokenizer.tokenize)"]},{"cell_type":"markdown","id":"b3410145","metadata":{"_cell_guid":"f36c4f68-7a1b-483b-b4b3-45c7174ab3c2","_uuid":"20577384-4ae0-4530-9802-78167ed0ad6b","papermill":{"duration":0.008649,"end_time":"2023-01-04T21:52:55.607421","exception":false,"start_time":"2023-01-04T21:52:55.598772","status":"completed"},"tags":[]},"source":["### Parameter tuning using grid search"]},{"cell_type":"code","execution_count":20,"id":"c7a4e2cc","metadata":{"_cell_guid":"94b5d231-80d4-4a38-8955-53d142cc0be7","_uuid":"de14a436-17c2-4e5a-9eec-705f85f65406","collapsed":false,"execution":{"iopub.execute_input":"2023-01-04T21:52:55.626304Z","iopub.status.busy":"2023-01-04T21:52:55.625909Z","iopub.status.idle":"2023-01-04T21:52:55.630834Z","shell.execute_reply":"2023-01-04T21:52:55.629533Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.016924,"end_time":"2023-01-04T21:52:55.633054","exception":false,"start_time":"2023-01-04T21:52:55.61613","status":"completed"},"tags":[]},"outputs":[],"source":["# from sklearn.model_selection import GridSearchCV\n","\n","# parameters = {\n","#     'vect__ngram_range': [(1, 1), (1, 2)],\n","#     'tfidf__use_idf': (True, False),\n","#     'clf__alpha': (1e-2, 1e-3),\n","# }\n","\n","# gs_clf = GridSearchCV(text_clf, parameters, cv=5, n_jobs=-1)\n","# gs_clf = gs_clf.fit(X_train.content[:400], X_train.sentiment[:400])"]},{"cell_type":"markdown","id":"64489ea6","metadata":{"_cell_guid":"681e47fe-a024-4c79-b2ba-76c088564b66","_uuid":"29fb8111-45f0-480c-8a28-665dd790f1e3","papermill":{"duration":0.008541,"end_time":"2023-01-04T21:52:55.650865","exception":false,"start_time":"2023-01-04T21:52:55.642324","status":"completed"},"tags":[]},"source":["## Extracting features from tweets data\n","    After loading the data the next step is to try and extract features from our tweets corpus."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":29.307634,"end_time":"2023-01-04T21:52:56.480725","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-01-04T21:52:27.173091","version":"2.3.4"}},"nbformat":4,"nbformat_minor":5}